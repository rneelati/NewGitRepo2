<!DOCTYPE html>
<!-- saved from url=(0044)https://stackabuse.com/tag/machine-learning/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><link href="https://fonts.googleapis.com/" rel="preconnect"><link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin="true"><link href="https://cdnjs.cloudflare.com/" rel="preconnect"><link href="https://s3.stackabuse.com/" rel="preconnect"><link href="https://googletagservices.com/" rel="dns-prefetch"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="HandheldFriendly" content="True"><link rel="shortcut icon" href="https://stackabuse.com/assets/images/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="https://stackabuse.com/assets/images/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="https://stackabuse.com/assets/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="https://stackabuse.com/assets/images/favicon-16x16.png"><link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&amp;display=swap"><link rel="preload" as="style" href="https://fonts.googleapis.com/css?family=Nunito:400,400i,700,700i&amp;display=swap"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="alternate" type="application/rss+xml" title="Stack Abuse" href="https://stackabuse.com/rss/"><script async="" src="./machine learning - Stack Abuse_files/analytics.js.download"></script><script src="./machine learning - Stack Abuse_files/525232124909042" async=""></script><script async="" src="./machine learning - Stack Abuse_files/fbevents.js.download"></script><script defer="" src="./machine learning - Stack Abuse_files/autotrack.js.download"></script><script>
                        !function(f,b,e,v,n,t,s)
                        {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
                        n.callMethod.apply(n,arguments):n.queue.push(arguments)};
                        if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
                        n.queue=[];t=b.createElement(e);t.async=!0;
                        t.src=v;s=b.getElementsByTagName(e)[0];
                        s.parentNode.insertBefore(t,s)}(window, document,'script',
                        'https://connect.facebook.net/en_US/fbevents.js');
                        fbq('init', '525232124909042');
                        fbq('track', 'PageView');
                    </script><noscript>
                        <img height="1" width="1" style="display:none"
                          src="https://www.facebook.com/tr?id=525232124909042&ev=PageView&noscript=1"
                        />
                    </noscript><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>machine learning - Stack Abuse</title><link rel="canonical" href="https://stackabuse.com/tag/machine-learning/"><meta property="og:url" content="https://stackabuse.com/tag/machine-learning/"><meta name="twitter:url" content="https://stackabuse.com/tag/machine-learning/"><link rel="next" href="https://stackabuse.com/tag/machine-learning/page/2/"><meta property="og:site_name" content="Stack Abuse"><meta property="og:type" content="website"><meta property="og:title" content="machine learning - Stack Abuse"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="machine learning - Stack Abuse"><meta name="twitter:site" content="@StackAbuse"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "Series",
  "publisher": {
    "@type": "Organization",
    "name": "Stack Abuse",
    "logo": {
      "@type": "ImageObject",
      "url": "https://stackabuse.com/favicon.png"
    }
  },
  "url": "https://stackabuse.com/tag/machine-learning/",
  "name": "machine learning",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://stackabuse.com/"
  }
}</script><meta name="next-head-count" content="14"><link rel="preload" href="./machine learning - Stack Abuse_files/3c1f00a2cf1b3760dcc5.css" as="style"><link rel="stylesheet" href="./machine learning - Stack Abuse_files/3c1f00a2cf1b3760dcc5.css" data-n-g=""><noscript data-n-css=""></noscript><link rel="preload" href="./machine learning - Stack Abuse_files/webpack-be80a870933e223259e9.js.download" as="script"><link rel="preload" href="./machine learning - Stack Abuse_files/framework-0974038618461ef712e9.js.download" as="script"><link rel="preload" href="./machine learning - Stack Abuse_files/main-518b48f042719025b17a.js.download" as="script"><link rel="preload" href="./machine learning - Stack Abuse_files/_app-663f6941febb9816e224.js.download" as="script"><link rel="preload" href="./machine learning - Stack Abuse_files/3-9d8c849ccdc38cf8a43a.js.download" as="script"><link rel="preload" href="./machine learning - Stack Abuse_files/566-b745348a44c8b90e96f7.js.download" as="script"><link rel="preload" href="./machine learning - Stack Abuse_files/221-fd1b1d9fc7fde140cafc.js.download" as="script"><link rel="preload" href="./machine learning - Stack Abuse_files/455-3962e458e87e7d9e0442.js.download" as="script"><link rel="preload" href="./machine learning - Stack Abuse_files/[...slug]-cdcef7f320901bfdcbca.js.download" as="script"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/543-2e1c9b1100c9aa9ba3f6.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/3-9d8c849ccdc38cf8a43a.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/638-a94f01bea17e0c9ed083.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/677-fbfef5f954127ac1a02a.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/566-2b1d89f30e5af269bcf2.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/221-fd1b1d9fc7fde140cafc.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/456-c4e0b33b2f8e38f3f57f.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/988-736ed4d52c6ef55892cf.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/917-3229522ef7d6ecbe92c5.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/[slug]-e8f5b4417775ebd0d6be.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/455-3962e458e87e7d9e0442.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/[...slug]-cdcef7f320901bfdcbca.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/index-0fb1cfa76a5a39e8a613.js.download"><link as="script" rel="prefetch" href="./machine learning - Stack Abuse_files/[...slug]-2322209993883dd7542d.js.download"></head><body><div id="__next"><nav class="bg-white"><div class="max-w-7xl mx-auto px-2 sm:px-4 lg:px-8"><div class="flex justify-between h-16"><div class="flex items-center px-2 lg:px-0"><a href="https://stackabuse.com/"><div class="flex-shrink-0 flex items-center"><div style="background-color:#404040" class="logo rounded-md p-3"><div class="leading-5 font-bold" style="font-size:30px;color:#dddddd">Stack<!-- --> <span style="color:#f16334">Abuse</span></div></div></div></a><div class="hidden lg:block lg:ml-6"><div class="hidden lg:ml-6 lg:flex"><a class="inline-flex items-center px-1 pt-1 border-b-2 border-yellow-200 text-sm font-medium leading-10 text-gray-900 hover:text-gray-700 hover:border-gray-300 focus:outline-none focus:text-gray-700 focus:border-gray-300 transition duration-150 ease-in-out" href="https://stackabuse.com/tag/python/">Python</a><a class="ml-8 inline-flex items-center px-1 pt-1 border-b-2 border-green-400 text-sm font-medium leading-10 text-gray-500 hover:text-gray-700 hover:border-gray-300 focus:outline-none focus:text-gray-700 focus:border-gray-300 transition duration-150 ease-in-out" href="https://stackabuse.com/tag/javascript/">JavaScript</a><a class="ml-8 inline-flex items-center px-1 pt-1 border-b-2 border-red-500 text-sm font-medium leading-10 text-gray-500 hover:text-gray-700 hover:border-gray-300 focus:outline-none focus:text-gray-700 focus:border-gray-300 transition duration-150 ease-in-out" href="https://stackabuse.com/tag/java/">Java</a><button class="ml-8 inline-flex items-center px-1 pt-1 border-b-2 border-blue-500 text-sm font-medium leading-10 text-gray-500 hover:text-gray-700 hover:border-gray-300 focus:outline-none focus:text-gray-700 focus:border-gray-300 transition duration-150 ease-in-out">Write with Us</button></div></div></div><div class="hidden md:flex md:flex-1 md:items-center md:justify-center md:px-2 lg:ml-6 lg:justify-end"><div class="relative w-1/2 mt-4 text-gray-400 transition duration-200 focus-within:text-gray-600"><div class="absolute inset-y-0 left-0 flex items-center pointer-events-none" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" class="h-5 w-5 ml-2"><path fill-rule="evenodd" d="M8 4a4 4 0 100 8 4 4 0 000-8zM2 8a6 6 0 1110.89 3.476l4.817 4.817a1 1 0 01-1.414 1.414l-4.816-4.816A6 6 0 012 8z" clip-rule="evenodd"></path></svg></div><input type="text" id="search-field" class="block w-full h-full pl-10 pr-3 py-3 text-sm border-gray-300 text-gray-900 placeholder-gray-500 rounded transition duration-200 focus:outline-none focus:ring-0 focus:border-gray-400" placeholder="Search"></div></div><div class="flex items-center lg:hidden"><button class="inline-flex items-center justify-center p-2 rounded-md text-gray-400 hover:text-gray-500 hover:bg-gray-100 focus:outline-none focus:bg-gray-100 focus:text-gray-500 transition duration-150 ease-in-out" aria-label="Main menu" aria-expanded="false"><svg class="block h-6 w-6" stroke="currentColor" fill="none" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path></svg><svg class="hidden h-6 w-6" stroke="currentColor" fill="none" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path></svg></button></div></div></div><div class="hidden lg:hidden"><div class="pt-2 pb-3"><a class="mt-1 block pl-3 pr-4 py-2 border-l-4 border-transparent text-base font-medium text-gray-600 hover:text-gray-800 hover:bg-gray-50 hover:border-gray-300 focus:outline-none focus:text-gray-800 focus:bg-gray-50 focus:border-gray-300 transition duration-150 ease-in-out" href="https://stackabuse.com/tag/python/">Python</a><a class="mt-1 block pl-3 pr-4 py-2 border-l-4 border-transparent text-base font-medium text-gray-600 hover:text-gray-800 hover:bg-gray-50 hover:border-gray-300 focus:outline-none focus:text-gray-800 focus:bg-gray-50 focus:border-gray-300 transition duration-150 ease-in-out" href="https://stackabuse.com/tag/javascript/">JavaScript</a><a class="mt-1 block pl-3 pr-4 py-2 border-l-4 border-transparent text-base font-medium text-gray-600 hover:text-gray-800 hover:bg-gray-50 hover:border-gray-300 focus:outline-none focus:text-gray-800 focus:bg-gray-50 focus:border-gray-300 transition duration-150 ease-in-out" href="https://stackabuse.com/tag/java/">Java</a></div></div></nav><main class="bg-gray-50"><div class="mt-6 pb-6"><div class="mx-auto max-w-none lg:max-w-screen-xl px-4 sm:px-6"><div class="grid grid-cols-12 sm:gap-x-2 lg:gap-x-3 pt-6"><div class="col-span-12 lg:col-span-7"><h1 class="my-1 text-3xl tracking-tight leading-10 font-extrabold text-gray-900 sm:leading-none sm:text-6xl lg:text-4xl xl:text-5xl">machine learning</h1><div class="">Articles: <!-- -->94</div><div class="my-4"></div></div><div class="lg:col-span-1"></div></div><div class="mt-8 border-t border-gray-200"><h2 class="my-6 text-xl tracking-tight leading-10 font-extrabold text-gray-700 sm:leading-none sm:text-4xl lg:text-2xl xl:text-3xl">Recently published</h2><div class="grid gap-5 lg:grid-cols-3"><div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><a href="https://stackabuse.com/split-train-test-and-validation-sets-with-tensorflow-datasets-tfds/"><img class="h-48 w-full object-cover" src="./machine learning - Stack Abuse_files/logo-python.webp" alt=""></a></div><div class="flex-1 bg-white p-6 flex flex-col justify-between"><div class="flex-1"><p class="text-sm leading-5 font-medium text-primary">Article</p><a class="block hover:no-underline" href="https://stackabuse.com/split-train-test-and-validation-sets-with-tensorflow-datasets-tfds/"><h3 class="mt-2 text-xl leading-7 font-semibold text-gray-900 hover:underline">Split Train, Test and Validation Sets with Tensorflow Datasets - tfds</h3><p class="mt-3 text-base leading-6 text-gray-500 break-words">Tensorflow Datasets, also known as tfds is is a library that serves as a wrapper to a wide selection of datasets, with proprietary functions to load, split and prepare datasets for Machine and Deep Learning, primarily with Tensorflow.                                                                                                                                                                         Note: While the Tensorflow Datasets library is used to get data, it's...</p></a></div><div class="mt-6 flex items-center"><div class="flex-shrink-0"><a href="https://stackabuse.com/author/david/"><img class="h-10 w-10 rounded-full" src="./machine learning - Stack Abuse_files/865cd7d217ea11c9d9555c4f666e2d73.jpg" alt="David Landup"></a></div><div class="ml-3"><p class="text-sm leading-5 font-medium text-gray-900"><a class="hover:underline" href="https://stackabuse.com/author/david/">David Landup</a></p><div class="flex text-sm leading-5 text-gray-500"><time datetime="2022-01-28">Jan 28, 2022</time></div></div></div></div></div></div><div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><a href="https://stackabuse.com/image-classification-with-transfer-learning-in-keras-create-cutting-edge-cnn-models/"><div class="lazyload-wrapper "><img class="h-48 w-full object-cover" src="./machine learning - Stack Abuse_files/logo-python.webp" alt=""></div></a></div><div class="flex-1 bg-white p-6 flex flex-col justify-between"><div class="flex-1"><p class="text-sm leading-5 font-medium text-primary">Article</p><a class="block hover:no-underline" href="https://stackabuse.com/image-classification-with-transfer-learning-in-keras-create-cutting-edge-cnn-models/"><h3 class="mt-2 text-xl leading-7 font-semibold text-gray-900 hover:underline">Image Classification with Transfer Learning in Keras - Create Cutting Edge CNN Models</h3><p class="mt-3 text-base leading-6 text-gray-500 break-words">Deep Learning models are very versatile and powerful - they're routinely outperforming humans in narrow tasks, and their generalization power is increasing at a rapid rate. New models are being released and benchmarked against community-accepted datasets frequently, and keeping up with all of them is getting harder.  Most of these...</p></a></div><div class="mt-6 flex items-center"><div class="flex-shrink-0"><a href="https://stackabuse.com/author/david/"><div class="lazyload-wrapper "><img class="h-10 w-10 rounded-full" src="./machine learning - Stack Abuse_files/865cd7d217ea11c9d9555c4f666e2d73.jpg" alt="David Landup"></div></a></div><div class="ml-3"><p class="text-sm leading-5 font-medium text-gray-900"><a class="hover:underline" href="https://stackabuse.com/author/david/">David Landup</a></p><div class="flex text-sm leading-5 text-gray-500"><time datetime="2022-01-20">Jan 20, 2022</time></div></div></div></div></div></div><div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><a href="https://stackabuse.com/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations/"><div class="lazyload-wrapper "><img class="h-48 w-full object-cover" src="./machine learning - Stack Abuse_files/logo-python.webp" alt=""></div></a></div><div class="flex-1 bg-white p-6 flex flex-col justify-between"><div class="flex-1"><p class="text-sm leading-5 font-medium text-primary">Article</p><a class="block hover:no-underline" href="https://stackabuse.com/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations/"><h3 class="mt-2 text-xl leading-7 font-semibold text-gray-900 hover:underline">Keras Callbacks: Save and Visualize Prediction on Each Training Epoch</h3><p class="mt-3 text-base leading-6 text-gray-500 break-words">Keras is a high-level API, typically used with the Tensorflow library, and has lowered the barrier to entry for many and democratized the creation of Deep Learning models and systems.  When just starting out, a high-level API that abstracts most of the inner-workings helps people get the hang of the...</p></a></div><div class="mt-6 flex items-center"><div class="flex-shrink-0"><a href="https://stackabuse.com/author/david/"><div class="lazyload-wrapper "><img class="h-10 w-10 rounded-full" src="./machine learning - Stack Abuse_files/865cd7d217ea11c9d9555c4f666e2d73.jpg" alt="David Landup"></div></a></div><div class="ml-3"><p class="text-sm leading-5 font-medium text-gray-900"><a class="hover:underline" href="https://stackabuse.com/author/david/">David Landup</a></p><div class="flex text-sm leading-5 text-gray-500"><time datetime="2021-11-17">Nov 17, 2021</time></div></div></div></div></div></div><div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><a href="https://stackabuse.com/scikit-learns-traintestsplit-training-testing-and-validation-sets/"><div class="lazyload-wrapper "><img class="h-48 w-full object-cover" src="./machine learning - Stack Abuse_files/logo-python.webp" alt=""></div></a></div><div class="flex-1 bg-white p-6 flex flex-col justify-between"><div class="flex-1"><p class="text-sm leading-5 font-medium text-primary">Article</p><a class="block hover:no-underline" href="https://stackabuse.com/scikit-learns-traintestsplit-training-testing-and-validation-sets/"><h3 class="mt-2 text-xl leading-7 font-semibold text-gray-900 hover:underline">Scikit-Learn's train_test_split() - Training, Testing and Validation Sets</h3><p class="mt-3 text-base leading-6 text-gray-500 break-words">Scikit-Learn is one of the most widely-used Machine Learning library in Python. It's optimized and efficient - and its high-level API is simple and easy to use.  Scikit-Learn has a plethora of convenience tools and methods that make preprocessing, evaluating and other painstaking processes as easy as calling a single...</p></a></div><div class="mt-6 flex items-center"><div class="flex-shrink-0"><a href="https://stackabuse.com/author/david/"><div class="lazyload-wrapper "><div style="height:2.5rem" class="lazyload-placeholder"></div></div></a></div><div class="ml-3"><p class="text-sm leading-5 font-medium text-gray-900"><a class="hover:underline" href="https://stackabuse.com/author/david/">David Landup</a></p><div class="flex text-sm leading-5 text-gray-500"><time datetime="2021-10-12">Oct 12, 2021</time></div></div></div></div></div></div><div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><a href="https://stackabuse.com/machine-learning-overfitting-is-your-friend-not-your-foe/"><div class="lazyload-wrapper "><img class="h-48 w-full object-cover" src="./machine learning - Stack Abuse_files/logo-python.webp" alt=""></div></a></div><div class="flex-1 bg-white p-6 flex flex-col justify-between"><div class="flex-1"><p class="text-sm leading-5 font-medium text-primary">Article</p><a class="block hover:no-underline" href="https://stackabuse.com/machine-learning-overfitting-is-your-friend-not-your-foe/"><h3 class="mt-2 text-xl leading-7 font-semibold text-gray-900 hover:underline">Machine Learning: Overfitting Is Your Friend, Not Your Foe</h3><p class="mt-3 text-base leading-6 text-gray-500 break-words">Note: These are the musings of a man - flawed and prone to misjudgement. The point of writing this is to promote a discussion on the topic, not to be right or contrarian. If any glaring mistakes are present in the writing, please let me know.                                            Let me preface the...</p></a></div><div class="mt-6 flex items-center"><div class="flex-shrink-0"><a href="https://stackabuse.com/author/david/"><div class="lazyload-wrapper "><div style="height:2.5rem" class="lazyload-placeholder"></div></div></a></div><div class="ml-3"><p class="text-sm leading-5 font-medium text-gray-900"><a class="hover:underline" href="https://stackabuse.com/author/david/">David Landup</a></p><div class="flex text-sm leading-5 text-gray-500"><time datetime="2021-09-21">Sep 21, 2021</time></div></div></div></div></div></div><div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><a href="https://stackabuse.com/random-projection-theory-and-implementation-in-python-with-scikit-learn/"><div class="lazyload-wrapper "><img class="h-48 w-full object-cover" src="./machine learning - Stack Abuse_files/logo-python.webp" alt=""></div></a></div><div class="flex-1 bg-white p-6 flex flex-col justify-between"><div class="flex-1"><p class="text-sm leading-5 font-medium text-primary">Article</p><a class="block hover:no-underline" href="https://stackabuse.com/random-projection-theory-and-implementation-in-python-with-scikit-learn/"><h3 class="mt-2 text-xl leading-7 font-semibold text-gray-900 hover:underline">Random Projection: Theory and Implementation in Python with Scikit-Learn</h3><p class="mt-3 text-base leading-6 text-gray-500 break-words">This guide is an in-depth introduction to an unsupervised dimensionality reduction technique called Random Projections. A Random Projection can be used to reduce the complexity and size of data, making the data easier to process and visualize. It is also a preprocessing technique for input preparation to a classifier or...</p></a></div><div class="mt-6 flex items-center"><div class="flex-shrink-0"><a href="https://stackabuse.com/author/mehreen/"><div class="lazyload-wrapper "><div style="height:2.5rem" class="lazyload-placeholder"></div></div></a></div><div class="ml-3"><p class="text-sm leading-5 font-medium text-gray-900"><a class="hover:underline" href="https://stackabuse.com/author/mehreen/">Mehreen Saeed</a></p><div class="flex text-sm leading-5 text-gray-500"><time datetime="2021-08-31">Aug 31, 2021</time><span class="mx-1">·</span><span>16<!-- --> min read</span></div></div></div></div></div></div><div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><a href="https://stackabuse.com/self-organizing-maps-theory-and-implementation-in-python-with-numpy/"><div class="lazyload-wrapper "><div style="height:12rem" class="lazyload-placeholder"></div></div></a></div><div class="flex-1 bg-white p-6 flex flex-col justify-between"><div class="flex-1"><p class="text-sm leading-5 font-medium text-primary">Article</p><a class="block hover:no-underline" href="https://stackabuse.com/self-organizing-maps-theory-and-implementation-in-python-with-numpy/"><h3 class="mt-2 text-xl leading-7 font-semibold text-gray-900 hover:underline">Self-Organizing Maps: Theory and Implementation in Python with NumPy</h3><p class="mt-3 text-base leading-6 text-gray-500 break-words">In this guide, we'll be taking a look at an unsupervised learning model, known as a Self-Organizing Map (SOM), as well as its implementation in Python. We'll be using an RGB Color example to train the SOM and demonstrate its performance and typical usage.  Self-Organizing Maps: A General Introduction A...</p></a></div><div class="mt-6 flex items-center"><div class="flex-shrink-0"><a href="https://stackabuse.com/author/mehreen/"><div class="lazyload-wrapper "><div style="height:2.5rem" class="lazyload-placeholder"></div></div></a></div><div class="ml-3"><p class="text-sm leading-5 font-medium text-gray-900"><a class="hover:underline" href="https://stackabuse.com/author/mehreen/">Mehreen Saeed</a></p><div class="flex text-sm leading-5 text-gray-500"><time datetime="2021-08-25">Aug 25, 2021</time><span class="mx-1">·</span><span>11<!-- --> min read</span></div></div></div></div></div></div><div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><a href="https://stackabuse.com/guide-to-multidimensional-scaling-in-python-with-scikit-learn/"><div class="lazyload-wrapper "><div style="height:12rem" class="lazyload-placeholder"></div></div></a></div><div class="flex-1 bg-white p-6 flex flex-col justify-between"><div class="flex-1"><p class="text-sm leading-5 font-medium text-primary">Article</p><a class="block hover:no-underline" href="https://stackabuse.com/guide-to-multidimensional-scaling-in-python-with-scikit-learn/"><h3 class="mt-2 text-xl leading-7 font-semibold text-gray-900 hover:underline">Guide to Multidimensional Scaling in Python with Scikit-Learn</h3><p class="mt-3 text-base leading-6 text-gray-500 break-words">In this guide, we'll dive into a dimensionality reduction, data embedding and data visualization technique known as Multidimensional Scaling (MDS).  We'll be utilizing Scikit-Learn to perform Multidimensional Scaling, as it has a wonderfully simple and powerful API. Throughout the guide, we'll be using the Olivetti faces dataset from AT&amp;amp;...</p></a></div><div class="mt-6 flex items-center"><div class="flex-shrink-0"><a href="https://stackabuse.com/author/mehreen/"><div class="lazyload-wrapper "><div style="height:2.5rem" class="lazyload-placeholder"></div></div></a></div><div class="ml-3"><p class="text-sm leading-5 font-medium text-gray-900"><a class="hover:underline" href="https://stackabuse.com/author/mehreen/">Mehreen Saeed</a></p><div class="flex text-sm leading-5 text-gray-500"><time datetime="2021-08-24">Aug 24, 2021</time><span class="mx-1">·</span><span>8<!-- --> min read</span></div></div></div></div></div></div><div><div class="flex flex-col rounded-lg shadow-lg overflow-hidden"><div class="flex-shrink-0"><a href="https://stackabuse.com/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python/"><div class="lazyload-wrapper "><div style="height:12rem" class="lazyload-placeholder"></div></div></a></div><div class="flex-1 bg-white p-6 flex flex-col justify-between"><div class="flex-1"><p class="text-sm leading-5 font-medium text-primary">Article</p><a class="block hover:no-underline" href="https://stackabuse.com/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python/"><h3 class="mt-2 text-xl leading-7 font-semibold text-gray-900 hover:underline">Feature Scaling Data with Scikit-Learn for Machine Learning in Python</h3><p class="mt-3 text-base leading-6 text-gray-500 break-words">Preprocessing data is an often overlooked key step in Machine Learning. In fact - it's as important as the shiny model you want to fit with it.  Garbage in - garbage out.  You can have the best model crafted for any sort of problem - if you feed it garbage,...</p></a></div><div class="mt-6 flex items-center"><div class="flex-shrink-0"><a href="https://stackabuse.com/author/david/"><div class="lazyload-wrapper "><div style="height:2.5rem" class="lazyload-placeholder"></div></div></a></div><div class="ml-3"><p class="text-sm leading-5 font-medium text-gray-900"><a class="hover:underline" href="https://stackabuse.com/author/david/">David Landup</a></p><div class="flex text-sm leading-5 text-gray-500"><time datetime="2021-07-12">Jul 12, 2021</time></div></div></div></div></div></div></div><div class="mt-8 border-t border-gray-200 px-4 flex items-center justify-between sm:px-0"><div class="-mt-px w-0 flex-1 flex justify-end"><a class="border-t-2 border-transparent pt-4 pl-1 inline-flex items-center text-sm font-medium text-gray-500 hover:text-gray-700 hover:border-gray-300" href="https://stackabuse.com/tag/machine-learning/page/2/">Next<svg class="ml-3 h-5 w-5 text-gray-400" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true"><path fill-rule="evenodd" d="M12.293 5.293a1 1 0 011.414 0l4 4a1 1 0 010 1.414l-4 4a1 1 0 01-1.414-1.414L14.586 11H3a1 1 0 110-2h11.586l-2.293-2.293a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></a></div></div></div></div></div></main><div class="bg-white border-t border-gray-300"><div class="max-w-screen-xl mx-auto pt-12 pb-6 px-4 sm:px-6 md:flex md:items-center md:justify-between lg:px-8"><div class="flex justify-center md:order-2"><a href="https://twitter.com/stackabuse" target="_blank" rel="noreferrer noopener" class="text-gray-500 hover:text-twitter"><span class="sr-only">Twitter</span><svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24"><path d="M8.29 20.251c7.547 0 11.675-6.253 11.675-11.675 0-.178 0-.355-.012-.53A8.348 8.348 0 0022 5.92a8.19 8.19 0 01-2.357.646 4.118 4.118 0 001.804-2.27 8.224 8.224 0 01-2.605.996 4.107 4.107 0 00-6.993 3.743 11.65 11.65 0 01-8.457-4.287 4.106 4.106 0 001.27 5.477A4.072 4.072 0 012.8 9.713v.052a4.105 4.105 0 003.292 4.022 4.095 4.095 0 01-1.853.07 4.108 4.108 0 003.834 2.85A8.233 8.233 0 012 18.407a11.616 11.616 0 006.29 1.84"></path></svg></a><a href="https://github.com/stackabuse" target="_blank" rel="noreferrer noopener" class="ml-6 text-gray-500 hover:text-github"><span class="sr-only">GitHub</span><svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd"></path></svg></a><a href="https://www.facebook.com/stackabuse" target="_blank" rel="noreferrer noopener" class="ml-6 text-gray-500 hover:text-facebook"><span class="sr-only">Facebook</span><svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24"><path fill-rule="evenodd" d="M22 12c0-5.523-4.477-10-10-10S2 6.477 2 12c0 4.991 3.657 9.128 8.438 9.878v-6.987h-2.54V12h2.54V9.797c0-2.506 1.492-3.89 3.777-3.89 1.094 0 2.238.195 2.238.195v2.46h-1.26c-1.243 0-1.63.771-1.63 1.562V12h2.773l-.443 2.89h-2.33v6.988C18.343 21.128 22 16.991 22 12z" clip-rule="evenodd"></path></svg></a></div><div class="mt-8 md:mt-0 md:order-1"><p class="text-center text-base leading-6 text-gray-500">© 2013-<!-- -->2022<!-- --> Stack Abuse. All rights reserved.</p><div class="text-sm"><a class="pr-4 text-gray-500 border-r-2 border-grey-400 hover:no-underline hover:text-gray-600" href="https://stackabuse.com/disclosure/">Disclosure</a><a class="px-4 text-gray-500 border-r-2 border-grey-400 hover:no-underline hover:text-gray-600" href="https://stackabuse.com/privacy-policy/">Privacy</a><a class="pl-4 text-gray-500 hover:no-underline hover:text-gray-600" href="https://stackabuse.com/terms-of-service/">Terms</a></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"tag":{"id":75,"name":"machine learning","slug":"machine-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507305534000,"updated_at":1507305534000,"contents":[{"id":1197,"old_id":null,"uuid":"7f65cc51-babc-44a9-b365-9a0d14557695","title":"Split Train, Test and Validation Sets with Tensorflow Datasets - tfds","slug":"split-train-test-and-validation-sets-with-tensorflow-datasets-tfds","body_md":"### Introduction\n\n**_Tensorflow Datasets_**, also known as `tfds` is is a library that serves as a wrapper to a wide selection of datasets, with proprietary functions to load, split and prepare datasets for _Machine and Deep Learning_, primarily with _Tensorflow_.\n\n:::note\n**Note:** While the _Tensorflow Datasets_ library is used to *get* data, it's not used to *preprocess* data. That job is delegated to the *Tensorflow Data* (`tf.data`) library.\n:::\n\nAll of the datasets acquired through Tensorflow Datasets are wrapped into `tf.data.Dataset` objects - so you can programmatically obtain and prepare a wide variety of datasets easily! One of the first steps you'll be taking after loading and getting to know a dataset is a *train/test/validation* split.\n\n\u003e In this guide, we'll take a look at what training, testing and validation sets are before learning how to load in and **perform a train/test/validation split with Tensorflow Datasets.**\n\n### Training and Testing Sets\n\nWhen working on supervised learning tasks - you'll want to obtain a set of features and a set of labels for those features, either as separate entities or within a single `Dataset`. Just training the network on all of the data is fine and dandy - but you can't test its accuracy on that same data, since evaluating the model like that would be rewarding *memorization* instead of *generalization*.\n\nInstead - we *train* the models on one part of the data, holding off a part of it to *test* the model once it's done training. The ratio between these two is commonly 80/20, and that's a fairly sensible default. Depending on the size of the dataset, you might opt for different ratios, such as 60/40 or even 90/10. If there are *many* samples in the testing set, there's no need to have a large percentage of samples dedicated to it. For instance, if 1% of the dataset represents 1.000.000 samples - you probably don't need more than that for testing!\n\nFor some models and architectures - you won't have any test set at all! For instance, when training _Generative Adversarial Networks (GANs)_ that generate images - testing the model isn't as easy as comparing the real and predicted labels! In most generative models (music, text, video), at least as of now, a human is typically required to judge the outputs, in which cases, a *test* set is totally redundant.\n\n\u003e The test set should be held out from the model until the testing stage, and it should only ever be used for **inference** - not **training**. It's common practice to define a test set and \"forget it\" until the end stages where you validate the model's accuracy.\n\n### Validation Sets\n\nA **_validation set_** is an extremely important, and sometimes overlooked set. Validation sets are oftentimes described as taken \"out of\" test sets, since it's convenient to imagine, but really - they're separate sets. There's no set rule for split ratios, but it's common to have a validation set of similar size to the test set, or slightly smaller - anything along the lines of _75/15/10_, _70/15/15_, and _70/20/10_.\n\nA validation set is used *during training*, to *approximately* validate the model on each epoch. This helps to update the model by giving \"hints\" as to whether it's performing well or not. Additionally, you don't have to wait for an entire set of epochs to finish to get a more accurate glimpse at the model's actual performance.\n\n::: note\n**Note:** The validation set isn't used *for* training, and the model doesn't train on the validation set at any given point. It's used to validate the performance in a given epoch. Since it does affect the training process, the model *indirectly* trains on the validation set and thus, it can't be *fully* trusted for testing, but is a good approximation/proxy for updating beliefs during training.\n:::\n\nThis is analogous to knowing when you're wrong, but not knowing what the right answer is. Eventually, by updating your beliefs after realizing you're not right, you'll get closer to the truth without explicitly being told what it is. A validation set *indirectly* trains your knowledge.\n\nUsing a validation set - you can easily interpret when a model has begun to overfit significantly in real-time, and based on the disparity between the validation and training accuracies, you could opt to trigger responses - such as automatically stopping training, updating the learning rate, etc.\n\n### Split Train, Test and Validation Sets using Tensorflow Datasets\n\nThe `load()` function of the `tfds` module loads in a dataset, given its name. If it's not already downloaded on the local machine - it'll automatically download the dataset with a progress bar:\n\n```python\nimport tensorflow_datasets as tfds\n\n# Load dataset\ndataset, info = tfds.load(\"cifar10\", as_supervised=True, with_info=True)\n\n# Extract informative features\nclass_names = info.features[\"label\"].names\nn_classes = info.features[\"label\"].num_classes\n\nprint(class_names) # ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nprint(n_classes) # 10\n```\n\nOne of the optional arguments you can pass into the `load()` function is the `split` argument. \n\nThe new *Split API* allows you to define which *splits* of the dataset you want to split out. By default, it only supports a `'train'` and `'test'` split - these are the \"official\" splits. There's no `valid` split! \n\nThey correspond to the `tfds.Split.TRAIN` and `tfds.Split.TEST` enums, which used to be exposed through the API in an earlier version. It's curious to note that `tfds.Split.VALIDATION` does exist, but doesn't have a string represented alias in the new API.\n\n\u003e It's worth noting that the strings used to name these aren't really relevant, as long as you achieve the right proportions.\n\nYou can really slice a `Dataset` into any arbitrary number of sets, though, we typically do three - `train_set`, `test_set`, `valid_set`:\n\n```python\ntest_set, valid_set, train_set = tfds.load(\"cifar10\", \n                                           split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n                                           as_supervised=True)\n\nprint(\"Train set size: \", len(train_set)) # Train set size:  37500\nprint(\"Test set size: \", len(test_set))   # Test set size:  5000\nprint(\"Valid set size: \", len(valid_set)) # Valid set size:  7500\n```\n\nWe've taken out the *first* 10% of the dataset, and extracted it into the `test_set`. The slice between 10% and 25% is assigned to the `valid_set` and everything beyond 25% is the `train_set`. This is validated through the sizes of the sets themselves as well.\n\n:::note\n**Note:** It's worth noting that we've used the `train` split, even though we split the dataset into other sets as well. Again, the only two accepted strings are `train` and `test`, but these don't *really* mean anything other than to let you know which parts are which.\n:::\n\nInstead of percentages, you can use absolute values or a mix of percentage and absolute values:\n\n```python\n# Absolute value split\ntest_set, valid_set, train_set = tfds.load(\"cifar10\", \n                                           split=[\"train[:2500]\", \"train[2500:5000]\", \"train[5000:]\"],\n                                           as_supervised=True)\n\nprint(\"Train set size: \", len(train_set)) # Train set size:  45000\nprint(\"Test set size: \", len(test_set))   # Test set size:  2500\nprint(\"Valid set size: \", len(valid_set)) # Valid set size:  2500\n\n\n# Mixed notation split\n# 5000 - 50% (25000) left unassigned\ntest_set, valid_set, train_set = tfds.load(\"cifar10\", \n                                           split=[\"train[:2500]\", # First 2500 are assigned to `test_set`\n                                           \"train[2500:5000]\",    # 2500-5000 are assigned to `valid_set`\n                                           \"train[50%:]\"],        # 50% - 100% (25000) assigned to `train_set`\n                                           as_supervised=True)\n```\n\nYou can additionally do a *union* of sets, which is less commonly used, as sets are interleaved then:\n\n```python\ntrain_and_test, half_of_train_and_test = tfds.load(\"cifar10\", \n                                split=['train+test', 'train[:50%]+test'],\n                                as_supervised=True)\n                                \nprint(\"Train+test: \", len(train_and_test))               # Train+test:  60000\nprint(\"Train[:50%]+test: \", len(half_of_train_and_test)) # Train[:50%]+test:  35000\n```\n\nThese two sets are now heavily interleaved.\n\n### Even Splits for N Sets\n\nAgain, you can create any arbitrary number of splits, just by adding more splits to the split list:\n\n```python\nsplit=[\"train[:10%]\", \"train[10%:20%]\", \"train[20%:30%]\", \"train[30%:40%]\", ...]\n```\n\nHowever, if you're creating many splits, especially if they're even - the strings you'll be passing in are very predictable. This can be automated by creating a list of strings, with a given equal interval (such as 10%) instead. For exactly this purpose, the `tfds.even_splits()` function generates a list of strings, given a prefix string and the desired number of splits:\n\n```python\nimport tensorflow_datasets as tfds\n\ns1, s2, s3, s4, s5 = tfds.even_splits('train', n=5)\n# Each of these elements is just a string\nsplit_list = [s1, s2, s3, s4, s5]\nprint(f\"Type: {type(s1)}, contents: '{s1}'\")\n# Type: \u003cclass 'str'\u003e, contents: 'train[0%:20%]'\n\nfor split in split_list:\n    test_set = tfds.load(\"cifar10\", \n                                split=split,\n                                as_supervised=True)\n    print(f\"Test set length for Split {split}: \", len(test_set))\n```\n\nThis results in:\n\n```plaintext\nTest set length for Split train[0%:20%]:  10000\nTest set length for Split train[20%:40%]:  10000\nTest set length for Split train[40%:60%]:  10000\nTest set length for Split train[60%:80%]:  10000\nTest set length for Split train[80%:100%]:  10000\n```\n\nAlternatively, you can pass in the entire `split_list` as the `split` argument itself, to construct several split datasets outside of a loop:\n\n```python\nts1, ts2, ts3, ts4, ts5 = tfds.load(\"cifar10\", \n                                split=split_list,\n                                as_supervised=True)\n```\n\n### Conclusion\n\nIn this guide, we've taken a look at what the training and testing sets are as well as the importance of validation sets. Finally, we've explored the new Splits API of the Tensorflow Datasets library, and performed a train/test/validation split.\n\n\n\n\n","body_html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTensorflow Datasets\u003c/em\u003e\u003c/strong\u003e, also known as \u003ccode\u003etfds\u003c/code\u003e is is a library that serves as a wrapper to a wide selection of datasets, with proprietary functions to load, split and prepare datasets for \u003cem\u003eMachine and Deep Learning\u003c/em\u003e, primarily with \u003cem\u003eTensorflow\u003c/em\u003e.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e While the \u003cem\u003eTensorflow Datasets\u003c/em\u003e library is used to \u003cem\u003eget\u003c/em\u003e data, it's not used to \u003cem\u003epreprocess\u003c/em\u003e data. That job is delegated to the \u003cem\u003eTensorflow Data\u003c/em\u003e (\u003ccode\u003etf.data\u003c/code\u003e) library.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eAll of the datasets acquired through Tensorflow Datasets are wrapped into \u003ccode\u003etf.data.Dataset\u003c/code\u003e objects - so you can programmatically obtain and prepare a wide variety of datasets easily! One of the first steps you'll be taking after loading and getting to know a dataset is a \u003cem\u003etrain/test/validation\u003c/em\u003e split.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn this guide, we'll take a look at what training, testing and validation sets are before learning how to load in and \u003cstrong\u003eperform a train/test/validation split with Tensorflow Datasets.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"trainingandtestingsets\"\u003eTraining and Testing Sets\u003c/h3\u003e\n\u003cp\u003eWhen working on supervised learning tasks - you'll want to obtain a set of features and a set of labels for those features, either as separate entities or within a single \u003ccode\u003eDataset\u003c/code\u003e. Just training the network on all of the data is fine and dandy - but you can't test its accuracy on that same data, since evaluating the model like that would be rewarding \u003cem\u003ememorization\u003c/em\u003e instead of \u003cem\u003egeneralization\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eInstead - we \u003cem\u003etrain\u003c/em\u003e the models on one part of the data, holding off a part of it to \u003cem\u003etest\u003c/em\u003e the model once it's done training. The ratio between these two is commonly 80/20, and that's a fairly sensible default. Depending on the size of the dataset, you might opt for different ratios, such as 60/40 or even 90/10. If there are \u003cem\u003emany\u003c/em\u003e samples in the testing set, there's no need to have a large percentage of samples dedicated to it. For instance, if 1% of the dataset represents 1.000.000 samples - you probably don't need more than that for testing!\u003c/p\u003e\n\u003cp\u003eFor some models and architectures - you won't have any test set at all! For instance, when training \u003cem\u003eGenerative Adversarial Networks (GANs)\u003c/em\u003e that generate images - testing the model isn't as easy as comparing the real and predicted labels! In most generative models (music, text, video), at least as of now, a human is typically required to judge the outputs, in which cases, a \u003cem\u003etest\u003c/em\u003e set is totally redundant.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe test set should be held out from the model until the testing stage, and it should only ever be used for \u003cstrong\u003einference\u003c/strong\u003e - not \u003cstrong\u003etraining\u003c/strong\u003e. It's common practice to define a test set and \u0026quot;forget it\u0026quot; until the end stages where you validate the model's accuracy.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"validationsets\"\u003eValidation Sets\u003c/h3\u003e\n\u003cp\u003eA \u003cstrong\u003e\u003cem\u003evalidation set\u003c/em\u003e\u003c/strong\u003e is an extremely important, and sometimes overlooked set. Validation sets are oftentimes described as taken \u0026quot;out of\u0026quot; test sets, since it's convenient to imagine, but really - they're separate sets. There's no set rule for split ratios, but it's common to have a validation set of similar size to the test set, or slightly smaller - anything along the lines of \u003cem\u003e75/15/10\u003c/em\u003e, \u003cem\u003e70/15/15\u003c/em\u003e, and \u003cem\u003e70/20/10\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eA validation set is used \u003cem\u003eduring training\u003c/em\u003e, to \u003cem\u003eapproximately\u003c/em\u003e validate the model on each epoch. This helps to update the model by giving \u0026quot;hints\u0026quot; as to whether it's performing well or not. Additionally, you don't have to wait for an entire set of epochs to finish to get a more accurate glimpse at the model's actual performance.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e The validation set isn't used \u003cem\u003efor\u003c/em\u003e training, and the model doesn't train on the validation set at any given point. It's used to validate the performance in a given epoch. Since it does affect the training process, the model \u003cem\u003eindirectly\u003c/em\u003e trains on the validation set and thus, it can't be \u003cem\u003efully\u003c/em\u003e trusted for testing, but is a good approximation/proxy for updating beliefs during training.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eThis is analogous to knowing when you're wrong, but not knowing what the right answer is. Eventually, by updating your beliefs after realizing you're not right, you'll get closer to the truth without explicitly being told what it is. A validation set \u003cem\u003eindirectly\u003c/em\u003e trains your knowledge.\u003c/p\u003e\n\u003cp\u003eUsing a validation set - you can easily interpret when a model has begun to overfit significantly in real-time, and based on the disparity between the validation and training accuracies, you could opt to trigger responses - such as automatically stopping training, updating the learning rate, etc.\u003c/p\u003e\n\u003ch3 id=\"splittraintestandvalidationsetsusingtensorflowdatasets\"\u003eSplit Train, Test and Validation Sets using Tensorflow Datasets\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eload()\u003c/code\u003e function of the \u003ccode\u003etfds\u003c/code\u003e module loads in a dataset, given its name. If it's not already downloaded on the local machine - it'll automatically download the dataset with a progress bar:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow_datasets \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tfds\n\n\u003cspan class=\"hljs-comment\"\u003e# Load dataset\u003c/span\u003e\ndataset, info = tfds.load(\u003cspan class=\"hljs-string\"\u003e\u0026quot;cifar10\u0026quot;\u003c/span\u003e, as_supervised=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, with_info=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# Extract informative features\u003c/span\u003e\nclass_names = info.features[\u003cspan class=\"hljs-string\"\u003e\u0026quot;label\u0026quot;\u003c/span\u003e].names\nn_classes = info.features[\u003cspan class=\"hljs-string\"\u003e\u0026quot;label\u0026quot;\u003c/span\u003e].num_classes\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(class_names) \u003cspan class=\"hljs-comment\"\u003e# [\u0026#x27;airplane\u0026#x27;, \u0026#x27;automobile\u0026#x27;, \u0026#x27;bird\u0026#x27;, \u0026#x27;cat\u0026#x27;, \u0026#x27;deer\u0026#x27;, \u0026#x27;dog\u0026#x27;, \u0026#x27;frog\u0026#x27;, \u0026#x27;horse\u0026#x27;, \u0026#x27;ship\u0026#x27;, \u0026#x27;truck\u0026#x27;]\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(n_classes) \u003cspan class=\"hljs-comment\"\u003e# 10\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOne of the optional arguments you can pass into the \u003ccode\u003eload()\u003c/code\u003e function is the \u003ccode\u003esplit\u003c/code\u003e argument.\u003c/p\u003e\n\u003cp\u003eThe new \u003cem\u003eSplit API\u003c/em\u003e allows you to define which \u003cem\u003esplits\u003c/em\u003e of the dataset you want to split out. By default, it only supports a \u003ccode\u003e'train'\u003c/code\u003e and \u003ccode\u003e'test'\u003c/code\u003e split - these are the \u0026quot;official\u0026quot; splits. There's no \u003ccode\u003evalid\u003c/code\u003e split!\u003c/p\u003e\n\u003cp\u003eThey correspond to the \u003ccode\u003etfds.Split.TRAIN\u003c/code\u003e and \u003ccode\u003etfds.Split.TEST\u003c/code\u003e enums, which used to be exposed through the API in an earlier version. It's curious to note that \u003ccode\u003etfds.Split.VALIDATION\u003c/code\u003e does exist, but doesn't have a string represented alias in the new API.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIt's worth noting that the strings used to name these aren't really relevant, as long as you achieve the right proportions.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eYou can really slice a \u003ccode\u003eDataset\u003c/code\u003e into any arbitrary number of sets, though, we typically do three - \u003ccode\u003etrain_set\u003c/code\u003e, \u003ccode\u003etest_set\u003c/code\u003e, \u003ccode\u003evalid_set\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003etest_set, valid_set, train_set = tfds.load(\u003cspan class=\"hljs-string\"\u003e\u0026quot;cifar10\u0026quot;\u003c/span\u003e, \n                                           split=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;train[:10%]\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[10%:25%]\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[25%:]\u0026quot;\u003c/span\u003e],\n                                           as_supervised=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Train set size: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(train_set)) \u003cspan class=\"hljs-comment\"\u003e# Train set size:  37500\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Test set size: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(test_set))   \u003cspan class=\"hljs-comment\"\u003e# Test set size:  5000\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Valid set size: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(valid_set)) \u003cspan class=\"hljs-comment\"\u003e# Valid set size:  7500\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe've taken out the \u003cem\u003efirst\u003c/em\u003e 10% of the dataset, and extracted it into the \u003ccode\u003etest_set\u003c/code\u003e. The slice between 10% and 25% is assigned to the \u003ccode\u003evalid_set\u003c/code\u003e and everything beyond 25% is the \u003ccode\u003etrain_set\u003c/code\u003e. This is validated through the sizes of the sets themselves as well.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e It's worth noting that we've used the \u003ccode\u003etrain\u003c/code\u003e split, even though we split the dataset into other sets as well. Again, the only two accepted strings are \u003ccode\u003etrain\u003c/code\u003e and \u003ccode\u003etest\u003c/code\u003e, but these don't \u003cem\u003ereally\u003c/em\u003e mean anything other than to let you know which parts are which.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eInstead of percentages, you can use absolute values or a mix of percentage and absolute values:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Absolute value split\u003c/span\u003e\ntest_set, valid_set, train_set = tfds.load(\u003cspan class=\"hljs-string\"\u003e\u0026quot;cifar10\u0026quot;\u003c/span\u003e, \n                                           split=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;train[:2500]\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[2500:5000]\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[5000:]\u0026quot;\u003c/span\u003e],\n                                           as_supervised=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Train set size: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(train_set)) \u003cspan class=\"hljs-comment\"\u003e# Train set size:  45000\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Test set size: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(test_set))   \u003cspan class=\"hljs-comment\"\u003e# Test set size:  2500\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Valid set size: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(valid_set)) \u003cspan class=\"hljs-comment\"\u003e# Valid set size:  2500\u003c/span\u003e\n\n\n\u003cspan class=\"hljs-comment\"\u003e# Mixed notation split\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# 5000 - 50% (25000) left unassigned\u003c/span\u003e\ntest_set, valid_set, train_set = tfds.load(\u003cspan class=\"hljs-string\"\u003e\u0026quot;cifar10\u0026quot;\u003c/span\u003e, \n                                           split=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;train[:2500]\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-comment\"\u003e# First 2500 are assigned to `test_set`\u003c/span\u003e\n                                           \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[2500:5000]\u0026quot;\u003c/span\u003e,    \u003cspan class=\"hljs-comment\"\u003e# 2500-5000 are assigned to `valid_set`\u003c/span\u003e\n                                           \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[50%:]\u0026quot;\u003c/span\u003e],        \u003cspan class=\"hljs-comment\"\u003e# 50% - 100% (25000) assigned to `train_set`\u003c/span\u003e\n                                           as_supervised=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can additionally do a \u003cem\u003eunion\u003c/em\u003e of sets, which is less commonly used, as sets are interleaved then:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003etrain_and_test, half_of_train_and_test = tfds.load(\u003cspan class=\"hljs-string\"\u003e\u0026quot;cifar10\u0026quot;\u003c/span\u003e, \n                                split=[\u003cspan class=\"hljs-string\"\u003e\u0026#x27;train+test\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;train[:50%]+test\u0026#x27;\u003c/span\u003e],\n                                as_supervised=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n                                \n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Train+test: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(train_and_test))               \u003cspan class=\"hljs-comment\"\u003e# Train+test:  60000\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Train[:50%]+test: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(half_of_train_and_test)) \u003cspan class=\"hljs-comment\"\u003e# Train[:50%]+test:  35000\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese two sets are now heavily interleaved.\u003c/p\u003e\n\u003ch3 id=\"evensplitsfornsets\"\u003eEven Splits for N Sets\u003c/h3\u003e\n\u003cp\u003eAgain, you can create any arbitrary number of splits, just by adding more splits to the split list:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003esplit=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;train[:10%]\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[10%:20%]\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[20%:30%]\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[30%:40%]\u0026quot;\u003c/span\u003e, ...]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHowever, if you're creating many splits, especially if they're even - the strings you'll be passing in are very predictable. This can be automated by creating a list of strings, with a given equal interval (such as 10%) instead. For exactly this purpose, the \u003ccode\u003etfds.even_splits()\u003c/code\u003e function generates a list of strings, given a prefix string and the desired number of splits:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow_datasets \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tfds\n\ns1, s2, s3, s4, s5 = tfds.even_splits(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;train\u0026#x27;\u003c/span\u003e, n=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# Each of these elements is just a string\u003c/span\u003e\nsplit_list = [s1, s2, s3, s4, s5]\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\u0026quot;Type: \u003cspan class=\"hljs-subst\"\u003e{\u003cspan class=\"hljs-built_in\"\u003etype\u003c/span\u003e(s1)}\u003c/span\u003e, contents: \u0026#x27;\u003cspan class=\"hljs-subst\"\u003e{s1}\u003c/span\u003e\u0026#x27;\u0026quot;\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# Type: \u0026lt;class \u0026#x27;str\u0026#x27;\u0026gt;, contents: \u0026#x27;train[0%:20%]\u0026#x27;\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e split \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e split_list:\n    test_set = tfds.load(\u003cspan class=\"hljs-string\"\u003e\u0026quot;cifar10\u0026quot;\u003c/span\u003e, \n                                split=split,\n                                as_supervised=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\u0026quot;Test set length for Split \u003cspan class=\"hljs-subst\"\u003e{split}\u003c/span\u003e: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(test_set))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eTest set length for Split train[0%:20%]:  10000\nTest set length for Split train[20%:40%]:  10000\nTest set length for Split train[40%:60%]:  10000\nTest set length for Split train[60%:80%]:  10000\nTest set length for Split train[80%:100%]:  10000\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlternatively, you can pass in the entire \u003ccode\u003esplit_list\u003c/code\u003e as the \u003ccode\u003esplit\u003c/code\u003e argument itself, to construct several split datasets outside of a loop:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003ets1, ts2, ts3, ts4, ts5 = tfds.load(\u003cspan class=\"hljs-string\"\u003e\u0026quot;cifar10\u0026quot;\u003c/span\u003e, \n                                split=split_list,\n                                as_supervised=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eIn this guide, we've taken a look at what the training and testing sets are as well as the importance of validation sets. Finally, we've explored the new Splits API of the Tensorflow Datasets library, and performed a train/test/validation split.\u003c/p\u003e\n","parent_id":null,"type":"article","status":"published","visibility":"public","img_feature":null,"is_featured":false,"locale":"en","custom_excerpt":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"comment_id":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In this tutorial, use the Splits API of Tensorflow Datasets (tfds) and learn how to perform a train, test and validation set split, as well as even splits, through practical Python examples.","read_time_min":null,"published_by":null,"published_at":1643369400000,"created_by":16,"updated_by":null,"created_at":1643213841146,"updated_at":1643229465435,"contributors":[{"id":16,"name":"David Landup","slug":"david","email":"thealduinmaster@gmail.com","password_hash":"$2a$10$W/oMJdUBSTeG3trWAHa1xO0pQruxuLgD/6hS7VuxPafcmAxeBXmVi","role_id":2,"img_profile":"//s3.stackabuse.com/media/users/865cd7d217ea11c9d9555c4f666e2d73.jpg","img_cover":null,"bio_md":"Entrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs. \n\nGreat passion for accessible education and promotion of reason, science, humanism, and progress.","bio_html":"\u003cp\u003eEntrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs.\u003c/p\u003e\n\u003cp\u003eGreat passion for accessible education and promotion of reason, science, humanism, and progress.\u003c/p\u003e\n","website":"https://www.upwork.com/freelancers/~017664e499a2766871","location":"Serbia","facebook":"","twitter":"","github":null,"status":"active","locale":null,"last_seen_at":1622229427000,"created_by":null,"updated_by":null,"created_at":1534532687000,"updated_at":1640861394795,"role":"editor","secret_token":"2a2be92558fae38f89cfbd0c6a4ba90c","is_email_confirmed":false,"_pivot_content_id":1197,"_pivot_user_id":16,"_pivot_role":"author","_pivot_sort_order":0}],"tags":[{"id":9,"name":"python","slug":"python","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1431358631000,"updated_at":1431358631000,"_pivot_content_id":1197,"_pivot_tag_id":9,"_pivot_sort_order":0},{"id":75,"name":"machine learning","slug":"machine-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507305534000,"updated_at":1507305534000,"_pivot_content_id":1197,"_pivot_tag_id":75,"_pivot_sort_order":3},{"id":78,"name":"tensorflow","slug":"tensorflow","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1508009047000,"updated_at":1508009047000,"_pivot_content_id":1197,"_pivot_tag_id":78,"_pivot_sort_order":1},{"id":106,"name":"data science","slug":"data-science","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1536865458000,"updated_at":1536865458000,"_pivot_content_id":1197,"_pivot_tag_id":106,"_pivot_sort_order":2},{"id":181,"name":"deep learning","slug":"deep-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1595356360000,"updated_at":1595356360000,"_pivot_content_id":1197,"_pivot_tag_id":181,"_pivot_sort_order":4}],"_pivot_tag_id":75,"_pivot_content_id":1197},{"id":1186,"old_id":null,"uuid":"f7bf6911-545f-4223-915b-5922654fb4df","title":"Image Classification with Transfer Learning in Keras - Create Cutting Edge CNN Models","slug":"image-classification-with-transfer-learning-in-keras-create-cutting-edge-cnn-models","body_md":"### Introduction\n\n_Deep Learning models_ are very versatile and powerful - they're routinely outperforming humans in narrow tasks, and their generalization power is increasing at a rapid rate. New models are being released and benchmarked against community-accepted datasets frequently, and keeping up with all of them is getting harder.\n\n\u003e Most of these models are open source, and you can implement them yourself as well.\n\nThis means that the average enthusiast can load in and play around with the cutting edge models in their home, on very average machines, not only to gain a deeper understanding and appreciation of the craft, but also to contribute to the scientific discourse and publish their own improvements whenever they're made.\n\n\u003e In this guide, you'll learn how to use pre-trained, cutting edge Deep Learning models for Image Classification and repurpose them for your own specific application. This way, you're leveraging their high performance, ingenious architectures **and** someone else's training time - while applying these models to your own domain.\n\n:::note\nAll of the code written in the guide is also available on \u003ca target=\"_blank\" href=\"https://github.com/StackAbuse/image-classification-transfer-learning-with-keras/blob/main/SA-Transfer%20Learning%20with%20Keras.ipynb\"\u003e_GitHub_\u003c/a\u003e.\n:::\n\n### Transfer Learning for Computer Vision and Convolutional Neural Networks (CNNs)\n\n_Knowledge_ and _knowledge representations_ are very universal. A computer vision model trained on one dataset learns to recognize patterns that might be very prevalent in many other datasets. \n\nNotably, in **_\"Deep Learning for the Life Sciences\"_**, by Bharath Ramsundar, Peter Eastman, Patrick Walters and Vijay Pande, it's noted that:\n\n\u003e \"There have been multiple studies looking into the use of recommendation system algorithms for use in molecular binding prediction. Machine learning architectures used in one field tend to carry over to other fields, so it’s important to retain the flexibility needed for innovative work.\"\n\nFor instance, straight and curved lines, which are typically learned at a lower level of a CNN hierarchy are bound to be present in practically all datasets. Some high-level features, such as the ones that distinguish a bee from an ant are going to be represented and learned much higher in the hierarchy:\n\n![feature hierarchies for convolutional neural networks](https://s3.stackabuse.com/media/articles/image-classification-with-transfer-learning-in-keras-create-cutting-edge-models-1.png)\n\nThe \"fine line\" between these is what you can reuse! Depending on the level of similarity between your dataset and the one a model's been pre-trained on, you may be able to reuse a small or large portion of it.\n\n\u003e A model that classifies human-made structures (trained on a dataset such as the Places365) and a model that classifies animals (trained on a dataset such as ImageNet) are bound to have **some** shared patterns, although, not a lot.\n\nYou might want to train a model to distinguish, say, *buses* and *cars* for a self-driving car's vision system. You may also reasonably choose to use a very performant architecture that has proven to work well on datasets similar to yours. Then, the long process of training begins, and you end up having a performant model of your own!\n\nHowever, if another model is likely to have similar representations on lower *and* higher levels of abstraction, there's no need to re-train a model from scratch. You may decide to use some of the already pre-trained weights, which are just as applicable to your own application of the model as they were applicable to the creator of the original architecture. You'd be **transferring** some of the knowledge from an already existing model to a new one, and this is known as **_Transfer Learning_**.\n\nThe closer the dataset of a pre-trained model is to your own, the more you can transfer. The more you can transfer, the more of your own time and computation you can save. It's worth remembering that training neural networks does have a carbon footprint, so you're not only saving time!\n\nTypically, **Transfer Learning** is done by loading a pre-trained model, and **freezing** its layers. In many cases, you can just cut off the **classification layer** (the final layers, or, *head*) and just re-train the classification layer, while keeping all of the other abstraction layers intact. In other cases, you may decide to re-train several layers in the hierarchy instead, and this is typically done when the datasets contain sufficiently different data points that re-training multiple layers is warranted. You may also decide to re-train the entirety of the model to fine-tune all of the layers.\n\nThese two approaches can be summarized as:\n\n- Using the Convolutional Network as a Feature Extractor\n- Fine-Tuning the Convolutional Network\n\nIn the former, you use the underlying entropic capacity of the model as a fixed feature extractor, and just train a dense network on top to discern between these features. In the latter, you fine-tune the entire (or portion of the) convolutional network, if it doesn't already have representative feature maps for some other more specific dataset, while also relying on the already trained feature maps.\n\nHere's a visual representation of how Transfer Learning works:\n\n![how does transfer learning work?](https://s3.stackabuse.com/media/articles/image-classification-with-transfer-learning-in-keras-create-cutting-edge-models-2.png)\n\n### Established and Cutting Edge Image Classification Models\n\nMany models exist out there, and for well-known datasets, you're likely to find hundreds of well-performing models published in online repositories and papers. A good holistic view of models trained on the ImageNet dataset can be seen at \u003ca rel=\"nofollow noopener noreferrer\" target=\"_blank\" href=\"https://paperswithcode.com/sota/image-classification-on-imagenet\"\u003ePapersWithCode\u003c/a\u003e.\n\nSome of the well-known published architectures that have subsequently been ported into many Deep Learning frameworks include:\n\n- **_EfficientNet_**\n- **_SENet_**\n- **_Xception_**\n- **_ResNet_**\n- **_VGGNet_**\n- **_AlexNet_**\n- **_LeNet-5_**\n\nThe list of models on PapersWithCode is constantly being updated, and you shouldn't hang up on the position of these models there. Many of them are outperformed by various other models as of writing, and many of the new models are actually based on the ones outlined in the list above. It's worth noting that **Transfer Learning** actually played an important role in the newer, higher accuracy models!\n\nThe downside is - a lot of the newest models aren't ported as pre-trained models within frameworks such as Tensorflow and PyTorch. It's not like you'll be losing out on *a lot* of the performance, so going with any of the well-established ones isn't really bad at all.\n\n### Transfer Learning with Keras - Adapting Existing Models\n\nWith Keras, the pre-trained models are available under the `tensorflow.keras.applications` module. Each model has its own sub-module and class. When loading a model in, you can set a couple of optional arguments to control how the models are being loaded in.\n\nFor instance, the `weights` argument, if present, defines the pre-trained weights. If omitted, only the architecture (untrained network) will be loaded in. If you supply the name of a dataset - a pre-trained network will be returned for that dataset.\n\nAdditionally, since you'll most likely be removing the top layer(s) for Transfer Learning, the `include_top` argument is used to define whether the top layer(s) should be present or not!\n\n```python\nimport tensorflow.keras.applications as models\n\n# 98 MB\nresnet = models.resnet50.ResNet50(weights='imagenet', include_top=False)\n# 528MB\nvgg16 = models.vgg16.VGG16(weights='imagenet', include_top=False)\n# 23MB\nnnm = models.NASNetMobile(weights='imagenet', include_top=False)\n# etc...\n```\n\n:::note\n**Note:** If you've never loaded pre-trained models before, they'll be downloaded over an internet connection. This may take anywhere between a few seconds and a couple of minutes, depending on your internet speed and the size of the models. The size of pre-trained models spans from as little as _14MB_ (typically lower for *Mobile* models) to as high as _549MB_.\n:::\n\n**_EfficientNet_** is a family of networks that are quite performant, scalable and, well, efficient. They were made with reducing learnable parameters in mind, so they only have 4M parameters to train. While 4M is still a large number, consider that VGG16, for instance, has 20M. On a home setup, this also helps with training times significantly!\n\nLet's load in one of the members of the EfficientNet family - EfficientNet-B0:\n\n```python\neffnet = keras.applications.EfficientNetB0(weights='imagenet', include_top=False)\neffnet.summary()\n```\n\nThis results in:\n\n```plaintext\nModel: \"efficientnetb0\"\n________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n========================================================================================\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n________________________________________________________________________________________\nrescaling (Rescaling)           (None, 224, 224, 3)  0           input_1[0][0]                    \n________________________________________________________________________________________\n...\n...\nblock7a_project_conv (Conv2D)   (None, 7, 7, 320)    368640      block7a_se_excite[0][0]          \n________________________________________________________________________________________\nblock7a_project_bn (BatchNormal (None, 7, 7, 320)    1280        block7a_project_conv[0][0]                    \n========================================================================================\nTotal params: 3,634,851\nTrainable params: 3,592,828\nNon-trainable params: 42,023\n________________________________________________________________________________________\n```\n\nOn the other hand, if we were to load in EfficientNet-B0 with the top included, we'd also have a few new layers at the end, that were trained to classify the data for ImageNet. This is the top of the model that we'll be training ourselves for our own application:\n\n```python\neffnet = keras.applications.EfficientNetB0(weights='imagenet', include_top=True)\neffnet.summary()\n```\n\nThis would include the `Flatten` and `Dense` layers, which then prop up the parameter size significantly:\n\n```plaintext\nModel: \"efficientnetb0\"\n_________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n=========================================================================================\ninput_11 (InputLayer)           [(None, 224, 224, 3) 0                                            \n_________________________________________________________________________________________\n...\n...\n_________________________________________________________________________________________\ntop_conv (Conv2D)               (None, 7, 7, 1280)   409600      block7a_project_bn[0][0]         \n_________________________________________________________________________________________\ntop_bn (BatchNormalization)     (None, 7, 7, 1280)   5120        top_conv[0][0]                   \n_________________________________________________________________________________________\ntop_activation (Activation)     (None, 7, 7, 1280)   0           top_bn[0][0]                     \n_________________________________________________________________________________________\navg_pool (GlobalAveragePooling2 (None, 1280)         0           top_activation[0][0]             \n_________________________________________________________________________________________\ntop_dropout (Dropout)           (None, 1280)         0           avg_pool[0][0]                   \n_________________________________________________________________________________________\npredictions (Dense)             (None, 1000)         1281000     top_dropout[0][0]                \n=========================================================================================\nTotal params: 5,330,571\nTrainable params: 5,288,548\nNon-trainable params: 42,023\n_________________________________________________________________________________________\n```\n\nAgain, we won't be using the top layers, as we'll be adding our own top to the EfficientNet model and re-training only the ones we add on top. It is worth noting what the architecture is already built with! They seem to be using a `Conv2D` layer, followed by a `BatchNormalization`, `GlobalAveragePooling2D` and `Dropout` before the final `Dense` classification layer. While we don't have to strictly follow this approach (and other approaches may prove to be better for another dataset), it's reasonable to remember how the original top looked like.\n\n:::note\n**Note:** Data preprocessing plays a crucial role in model training, and most models will have different preprocessing pipelines. You don't have to perform guesswork here! Where applicable, a model comes with its own `preprocess_input()` function.\n::: \n\nThe `preprocess_input()` function applies the same preprocessing steps to the input as they were applied during training. You can import the function from the respective module of the model, if a model resides in its own module. For instance, VGG16 has its own `preprocess_input` function:\n\n```python\nfrom keras.applications.vgg16 import preprocess_input\n```\n\nThat being said, loading in a model, preprocessing input for it and predicting a result in Keras is as easy as:\n\n```python\nimport tensorflow.keras.applications as models\nfrom keras.applications.vgg16 import preprocess_input\n\nvgg16 = models.vgg16.VGG16(weights='imagenet', include_top=True)\n\nimg = # get data\nimg = preprocess_input(img)\npred = vgg16.predict(img)\n```\n\n:::note\n**Note:** Not all models have a dedicated `preprocess_input()` function, because the preprocessing is *done within the model itself*. For instance, EfficientNet that we'll be using doesn't have its own dedicated preprocessing function, as the `Rescaling` layer takes care of that.\n:::\n\nThat's it! Now, since the `pred` array doesn't really contain human-readable data, you can also import the `decode_predictions()` function alongside the `preprocess_input()` function from a module. Alternatively, you can import the generic `decode_predictions()` function that also applies to models that don't have their dedicated modules:\n\n```python\nfrom keras.applications.model_name import preprocess_input, decode_predictions\n# OR\nfrom keras.applications.imagenet_utils import decode_predictions\n# ...\nprint(decode_predictions(pred))\n```\n\nTying this together, let's get an image of a black bear via `urllib`, save that file into a target size suitable for EfficientNet (the input layer expects a shape of `(None, 224, 224, 3)`) and classify it with the pre-trained model:\n\n```python\nfrom tensorflow import keras\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom tensorflow.keras.preprocessing import image\n\nimport urllib.request\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Public domain image\nurl = 'https://upload.wikimedia.org/wikipedia/commons/0/02/Black_bear_large.jpg'\nurllib.request.urlretrieve(url, 'bear.jpg')\n\n# Load image and resize (doesn't keep aspect ratio)\nimg = image.load_img('bear.jpg', target_size=(224, 224))\n# Turn to array of shape (224, 224, 3)\nimg = image.img_to_array(img)\n# Expand array into (1, 224, 224, 3)\nimg = np.expand_dims(img, 0)\n# Preprocess for models that have specific preprocess_input() function\n# img_preprocessed = preprocess_input(img)\n\n# Load model and run prediction\neffnet = keras.applications.EfficientNetB0(weights='imagenet', include_top=True)\npred = effnet.predict(img)\nprint(decode_predictions(pred))\n```\n\nThis results in:\n\n```plaintext\n[[\n('n02133161', 'American_black_bear', 0.6024658),\n('n02132136', 'brown_bear', 0.1457715),\n('n02134418', 'sloth_bear', 0.09819221),\n('n02510455', 'giant_panda', 0.0069221947),\n('n02509815', 'lesser_panda', 0.005077324)\n]]\n```\n\nIt's fairly certain that the image is an image of an American Black Bear, which is right! When preprocessed with a preprocessing function, the image may change significantly. For instance, VGG16's preprocessing function would change the color of the bear's fur:\n\n![preprocessing image for VGG16 CNN](https://s3.stackabuse.com/media/articles/image-classification-with-transfer-learning-in-keras-create-cutting-edge-models-3.PNG)\n\nIt looks a lot more brown now! If we were to feed this image into EfficientNet, it'd think it's a *brown bear*:\n\n```plaintext\n[[\n('n02132136', 'brown_bear', 0.7152758), \n('n02133161', 'American_black_bear', 0.15667434), \n('n02134418', 'sloth_bear', 0.012813852), \n('n02134084', 'ice_bear', 0.0067828503), ('n02117135', 'hyena', 0.0050422684)\n]]\n```\n\nAwesome! The model works. Now, let's add a new top to it and re-train the top to perform classification for something outside of the ImageNet set.\n\n#### Adding a New Top to a pre-trained Model\n\nWhen performing transfer learning, you'll be loading models without tops, or remove them manually:\n\n```python\n# Load without top\n# When adding new layers, we also need to define the input_shape\n# so that  the new Dense layers have a fixed input_shape as well\neffnet_base = keras.applications.EfficientNetB0(weights='imagenet', \n                                          include_top=False, \n                                          input_shape=((224, 224, 3)))\n\n# Or load the full model\nfull_effnet = keras.applications.EfficientNetB0(weights='imagenet', \n                                            include_top=True, \n                                            input_shape=((224, 224, 3)))\n                                            \n# And then remove X layers from the top\ntrimmed_effnet = keras.Model(inputs=full_effnet.input, outputs=full_effnet.layers[-3].output)\n```\n\nWe'll be going with the first option since it's more convenient. Depending on whether you'd like to fine-tune the convolutional blocks or not - you'll either freeze or won't freeze them. Say we want to use the underlying pre-trained feature maps and freeze the layers so that we only re-train the new classification layers at the top:\n\n```python\neffnet_base.trainable = False\n```\n\nYou don't need to iterate through the model and set each layer to be `trainable` or not, though you also can. If you'd like to turn off the first `n` layers, and allow some higher-level feature maps to be fine-tuned, but leave the lower-level ones untouched, you can:\n\n```python\nfor layer in effnet_base.layers[:-2]:\n    layer.trainable = False\n```\n\nHere, we've set all layers in the base model to be untrainable, except for the last two. If we check the model, there are only ~2.5K trainable parameters now:\n\n```python\neffnet_base.summary()\n```\n```plaintext\n# ...                \n=========================================================================================\nTotal params: 4,049,571\nTrainable params: 2,560\nNon-trainable params: 4,047,011\n_________________________________________________________________________________________\n```\n\nNow, let's define a `Sequential` model that'll be put on top of this `effnet_base`. Fortunately, chaining models in Keras is as easy as making a new model and putting it on top of another one! You can leverage the Functional API and just chain a few new layers on top of a model.\n\nLet's add a `Conv2D` layer, a `BatchNormalization` layer, a `GlobalAveragePooling2D` layer, some `Dropout` and a couple of fully connected layers after a `Flatten`:\n\n```python\nconv2d = keras.layers.Conv2D(7, 7)(effnet_base.output, training=False)\nbn = keras.layers.BatchNormalization()(conv2d)\ngap = keras.layers.GlobalAveragePooling2D()(bn)\ndo = keras.layers.Dropout(0.2)(gap)\nflatten = keras.layers.Flatten()(gap)\nfc1 = keras.layers.Dense(512, activation='relu')(flatten)\noutput = keras.layers.Dense(100, activation='softmax')(fc1)\n\nnew_model = keras.Model(inputs=effnet_base.input, outputs=output)\n```\n\n:::note\n**Note:** When adding the layers of the EfficientNet, we set the `training` to `False`. This puts the network in *inference mode* instead of *training mode* and it's a different parameter than the `trainable` we've set to `False` earlier. This is a crucial step if you wish to unfreeze layers later on! *BatchNormalization* computes moving statistics. When unfrozen, it'll start applying updates to parameters again, and will \"undo\" the training done before fine-tuning. Since TF 2.0, setting the model's `trainable` as `False` also turns `training` to `False` but only for `BatchNormalization` layers, so this step is unnecessary for versions after TF 2.0.\n:::\n\nAlternatively, you can use the Sequential API and call the `add()` method multiple times:\n\n```python\nnew_model = keras.Sequential()\nnew_model.add(effnet_base) # Add entire model\nnew_model.add(keras.layers.Conv2D(7,7))\nnew_model.add(keras.layers.BatchNormalization())\nnew_model.add(keras.layers.GlobalAveragePooling2D())\nnew_model.add(keras.layers.Dropout(0.3))\nnew_model.add(keras.layers.Flatten())\nnew_model.add(keras.layers.Dense(512, activation='relu'))\nnew_model.add(keras.layers.Dense(100, activation='softmax'))\n```\n\nThis adds the entire model as a layer itself, so it's treated as one entity:\n\n```plaintext\nLayer: 0, Trainable: False # Entire EfficientNet model\nLayer: 1, Trainable: True\nLayer: 2, Trainable: True\n...\n```\n\nOn the other hand, you can extract all of the layers and add them instead as separate entities, by adding the `output` of `effnet_base`:\n\n```python\nnew_model = keras.Sequential()\nnew_model.add(effnet_base.output) # Add unwrapped layers\nnew_model.add(keras.layers.Conv2D(7,7))\nnew_model.add(keras.layers.BatchNormalization())\nnew_model.add(keras.layers.GlobalAveragePooling2D())\nnew_model.add(keras.layers.Dropout(0.3))\nnew_model.add(keras.layers.Dense(100, activation='softmax'))\n```\n\nIn any of these cases - we've added 10 output classes, since we'll be using the CIFAR10 dataset later on, which has 10 classes! Let's take a look at the trainable layers in the network:\n\n```python\nfor index, layer in enumerate(new_model.layers):\n    print(\"Layer: {}, Trainable: {}\".format(index, layer.trainable))\n```\n\nThis results in:\n\n```plaintext\nLayer: 0, Trainable: False\nLayer: 1, Trainable: False\nLayer: 2, Trainable: False\n...\nLayer: 235, Trainable: False\nLayer: 236, Trainable: False\nLayer: 237, Trainable: True\nLayer: 238, Trainable: True\nLayer: 239, Trainable: True\nLayer: 240, Trainable: True\nLayer: 241, Trainable: True\n```\n\nAwesome! Let's load in the dataset, preprocess it and re-train the classification layers on it.\n\n#### Loading and Preprocessing Data\n\nWe'll be working with the \u003ca rel=\"nofollow noopener noreferrer\" target=\"_blank\" href=\"https://www.tensorflow.org/datasets/catalog/cifar10\"\u003e_CIFAR10 dataset_\u003c/a\u003e. This is a dataset that's not too hard to classify since it only has 10 classes, and we'll be leveraging a well-received architecture to help us in that endeavor. \n\nIts \"older brother\", CIFAR100 is a genuinely hard one to work with. It has 50.000 images, with 100 labels, meaning each class has **only 500 samples**. This is extremely hard to get right on so few labels, and almost all well-performing models on the dataset use heavy data augmentation.\n\n\u003e Data augmentation is an art and science in and of itself, and is out of scope for this guide - so we'll only be diversifying the dataset with a couple of random transformations.\n\nFor brevity's sake, we'll stick to CIFAR10, to emulate the dataset you'll be working with yourself!\n\n:::note\n**Note:** Keras' `datasets` module contains a few datasets, but these are mainly meant for benchmarking and learning. We can use `tensorflow_datasets` to get access to a much larger corpora of datasets! Alternatively, you can use any other source, such as Kaggle or academic repositories.\n:::\n\nWe'll be using `tensorflow_datasets` to download the CIFAR10 dataset, get the labels and the number of classes:\n\n```python\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\ndataset, info = tfds.load(\"cifar10\", as_supervised=True, with_info=True)\n# Save class_names and n_classes for later\nclass_names = info.features[\"label\"].names\nn_classes = info.features[\"label\"].num_classes\n\nprint(class_names) # ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nprint(n_classes) # 10\n```\n\nYou can get to know the dataset like this, but we won't be diving into that right now. Let's split it into a `train_set`, `valid_set` and `test_set` instead:\n\n```python\ntest_set, valid_set, train_set = tfds.load(\"cifar10\", \n                                           split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n                                           as_supervised=True)\n\nprint(\"Train set size: \", len(train_set)) # Train set size:  37500\nprint(\"Test set size: \", len(test_set)) # Test set size:  5000\nprint(\"Valid set size: \", len(valid_set)) # Valid set size:  7500\n```\n\n:::note\n**Note:** The `split` argument expects `train` and `test` keywords, and there's no `valid` keyword that can be used to extract a validation set. Because of this, we need to perform the slightly awkward and clunky split as we have - with a _10/15/75 split_.\n:::\n\nNow, the CIFAR10 images are significantly different from the ImageNet images! Namely, CIFAR10 images are just 32x32, while our EfficientNet model expects 224x224 images. We'll want to resize the images in any case. We might also want to apply some transformation functions on duplicate images to artificially expand the sample size per class if the dataset doesn't have enough of them. In the case of CIFAR10, this isn't an issue, as there are enough images per class, but with CIFAR100 - it's a different story. It's worth noting that, when upscaling images that are this small, even humans have a significant difficulty discerning what's on some of the images.\n\nFor instance, here are a few non-resized images:\n\n![cifar100 image examples](https://s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-4.png)\n\nCan you tell what's on these with confidence? Consider the lifelong amount of context you have for these images, as well, which the model doesn't have. It's worth keeping this in mind when you train it and observe the accuracy.\n\nLet's define a preprocessing function for each image and its associated label:\n\n```python\ndef preprocess_image(image, label):\n    # Resize to EfficientNet size\n    resized_image = tf.image.resize(image, [224, 224])\n    # Random flips and rotations (fully optional, and doesn't impact performance much no augmentation takes place)\n    # If we run this function multiple times, it'll net different results\n    img = tf.image.random_flip_left_right(resized_image)\n    img = tf.image.random_flip_up_down(img)\n    img = tf.image.rot90(img)\n    # Preprocess image with model-specific function if it has one\n    # img = preprocess_input(img)\n    return img, label\n```\n\nAnd finally, we'll want to apply this function to each image in the sets! We haven't performed augmentation by expanding the sets here, though you could. For brevity's sake, we'll avoid performing data augmentation.\n\nThis is easily done via the `map()` function. Since the input into the network also expects batches (`(None, 224, 224, 3)` instead of `(224, 224, 3)`) - we'll also `batch()` the datasets after mapping:\n\n```python\ntrain_set = train_set.map(preprocess_image).batch(32).prefetch(1)\ntest_set = test_set.map(preprocess_image).batch(32).prefetch(1)\nvalid_set = valid_set.map(preprocess_image).batch(32).prefetch(1)\n```\n\n:::note\n**Note:** The `prefetch()` function is optional but helps with efficiency. As the model is training on a single batch, the `prefetch()` function pre-fetches the next batch so it's not waited upon when the training step is finished.\n:::\n\nFinally, we can train the model!\n\n#### Training a Model\n\nWith the data loaded, preprocessed and split into adequate sets - we can train the model on it. The optimizer as well as its hyperparameters, loss function and metrics generally depend on your specific task.\n\nSince we're doing sparse classification, a `sparse_categorical_crossentropy` loss should work well, and the `Adam` optimizer is a reasonable default loss function. Let's compile the model, and train it on a few epochs. It's worth remembering that most of the layers in the network are frozen! We're only training the new classifier on top of the extracted feature maps.\n\nOnly once we train the top layers, we may decide to *unfreeze* the feature extraction layers, and let them fine-tune a bit more. This step is optional, and in many cases, you won't unfreeze them (mainly when working with really large networks). A good rule of thumb is to try and compare the datasets and guesstimate which levels of the hierarchy you can re-use without re-training.\n\nIf they're *really* different, you probably chose a network pre-trained on the wrong dataset. It wouldn't be efficient to use feature extraction of Places365 (man-made objects) for classifying animals. However, it would make sense to use a network trained on ImageNet (which has various objects, animals, plants and humans) and then use it for a different dataset with relatively similar categories, such as CIFAR10.\n\n:::note\n**Note:** Depending on the architecture you're using, unfreezing the layers might be a bad idea, due to their size. There's a *good chance* that your local machine will run out of memory when trying to tackle a 20M parameter model and loading a training step into the RAM/VRAM. When possible, try to find an architecture pre-trained on a dataset that's sufficiently similar to yours that you don't have to change the feature extractors. If you have to, it's not impossible but does make the process much slower. We'll cover that later.\n:::\n\nLet's train the new network (really, only the top of it) for 10 epochs:\n\n```python\noptimizer = keras.optimizers.Adam(learning_rate=2e-5)\n\nnew_model.compile(loss=\"sparse_categorical_crossentropy\", \n                  optimizer=optimizer, \n                  metrics=[\"accuracy\"])\n\nhistory = new_model.fit(train_set, \n                        epochs=10,\n                        validation_data=valid_set)\n```\n\n:::note\n**Note:** This may take some time and is ideally done on a GPU. Depending on how large the model is, and the dataset being fed into it. If you don't have access to a GPU, it's advised to run this code on any of the cloud providers that give you access to a free GPU, such as _Google Collab_, _Kaggle Notebooks_, etc. Each epoch can take anywhere from 60 seconds on stronger GPUs to 10 minutes, on weaker ones.\n:::\n\nThis is the point in which you sit back and go grab a coffee (or tea)! After 10 epochs, the train and validation accuracy are looking good:\n\n```plaintext\nEpoch 1/10\n1172/1172 [==============================] - 92s 76ms/step - loss: 1.6582 - accuracy: 0.6373 - val_loss: 1.1582 - val_accuracy: 0.7935\n...\nEpoch 10/10\n1172/1172 [==============================] - 89s 76ms/step - loss: 0.0911 - accuracy: 0.9781 - val_loss: 0.3847 - val_accuracy: 0.8792\n```\n\n~98% on the training set and ~88% on the validation set - it clearly overfit, but not too badly. Let's test it out and plot the learning curves.\n\n#### Testing a Model\n\nLet's first test this model out, before trying to unfreeze all of the layers and seeing if we can fine-tune it then:\n\n```python\nnew_model.evaluate(test_set)\n# 157/157 [==============================] - 10s 61ms/step - loss: 0.3778 - accuracy: 0.8798\n# [0.3778476417064667, 0.879800021648407]\n```\n\n88% on the testing set, and extremely close to the accuracy on the validation set! Looks like our model is generalizing well, but there's still room for improvement. Let's take a look at the learning curves.\n\nThe training curves are to be expected - they're pretty short since we only trained for 10 epochs, but they've quickly plateaued, so we probably wouldn't have gotten much better performance with more epochs. While oscillations do occur and the accuracy could very well rise in Epoch 11 - it's not too likely, so we'll miss out on the chance:\n\n![transfer learning training curves](https://s3.stackabuse.com/media/articles/image-classification-with-transfer-learning-in-keras-create-cutting-edge-models-5.png)\n\nCan we fine-tune this network further? We've replaced and re-trained the top layers concerned with classification of feature maps. Let's try unfreezing the convolutional layers and fine-tuning them as well!\n\n#### Unfreezing Layers - Fine-Tuning a Network Trained with Transfer Learning\n\nOnce you've finished re-training the top layers, you can close the deal and be happy with your model. For instance, suppose you got a 95% accuracy - you seriously don't need to go further. However, why not?\n\nIf you can squeeze out an additional 1% in accuracy, it might not sound like a lot, but consider the other end of the trade. If your model has a 95% accuracy on 100 samples, it misclassified 5 samples. If you up that to 96% accuracy, it misclassified 4 samples.\n\n\u003e The **1% of accuracy** translates to a **25% decrease in false classifications**.\n\nWhatever you can further squeeze out of your model can actually make a significant difference on the number of incorrect classifications. We have a pretty satisfactory 88% accuracy with our model, but we can most probably squeeze more out of it if we just slightly re-train the feature extractors. Again, the images in CIFAR10 are *much* smaller than ImageNet images, and it's almost as if someone with great eyesight suddenly gained a huge prescription and only saw the world through blurry eyes. The feature maps *have to* be at least somewhat different!\n\nLet's save the model into a file so we don't lose the progress, and unfreeze/fine-tune a loaded copy, so we don't accidentally mess up the weights on the original one:\n\n```python\nnew_model.save('effnet_transfer_learning.h5')\nloaded_model = keras.models.load_model('effnet_transfer_learning.h5')\n```\n\nNow, we can fiddle around and change the `loaded_model` without impacting `new_model`! To start out, we'll want to change the `loaded_model` from inference mode back to training mode - i.e. *unfreeze the layers* so that they're trainable again. \n\n:::note\n**Note:** Again, if a network uses `BatchNormalization` (and most do), you'll want to keep it frozen before fine-tuning a network. Since we're not freezing the entire base network anymore, we'll just freeze the `BatchNormalization` layers instead and allow other layers to be altered.\n:::\n\nLet's turn off the `BatchNormalization` layers so our training doesn't go down the drain:\n\n```python\nfor layer in loaded_model.layers:\n    if isinstance(layer, keras.layers.BatchNormalization):\n        layer.trainable = False\n    else:\n        layer.trainable = True\n\nfor index, layer in enumerate(loaded_model.layers):\n    print(\"Layer: {}, Trainable: {}\".format(index, layer.trainable))\n```\n\nLet's check if that worked:\n\n```plaintext\nLayer: 0, Trainable: True\nLayer: 1, Trainable: True\nLayer: 2, Trainable: True\nLayer: 3, Trainable: True\nLayer: 4, Trainable: True\nLayer: 5, Trainable: False\nLayer: 6, Trainable: True\nLayer: 7, Trainable: True\nLayer: 8, Trainable: False\n...\n```\n\nAwesome! Before we can do anything with the model, to \"solidify\" the trainability, we have to recompile it. This time around, we'll be using a smaller `learning_rate`, since we don't want to alter the network much at all, and just want to fine-tune some of the feature extracting capabilities and the new classification layer on top:\n\n```python\noptimizer = keras.optimizers.Adam(learning_rate=1e-6, decay=(1e-6/50))\n\n# Recompile after turning to trainable\nloaded_model.compile(loss=\"sparse_categorical_crossentropy\", \n                  optimizer=optimizer, \n                  metrics=[\"accuracy\"])\n\nhistory = loaded_model.fit(train_set, \n                        epochs=50,\n                        validation_data=valid_set)\n```\n\nAgain, this may take some time - so sip on another hot beverage of your choice while this runs in the background. Once it finishes, it should reach up to 92% in validation accuracy and 92.6% on the test set:\n\n```plaintext\nEpoch 1/10\n1172/1172 [==============================] - 389s 328ms/step - loss: 0.0552 - accuracy: 0.9863 - val_loss: 0.3493 - val_accuracy: 0.8941\n...\nEpoch 50/50\n1172/1172 [==============================] - 375s 320ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.3454 - val_accuracy: 0.9188\n```\n\nAgain, this isn't a huge jump from the perspective of accuracy itself, but it is a much more significant reduction in the proportion of misclassifications. Let's evaluate it and visualize some of the predictions:\n\n```python\nloaded_model.evaluate(test_set)\n\n# 157/157 [==============================] - 10s 61ms/step - loss: 0.3144 - accuracy: 0.9262\n# [0.3143560290336609, 0.9261999726295471]\n\n\nfig = plt.figure(figsize=(15, 10))\n\ni = 1\nfor entry in test_set.take(25):\n    # Predict, get the raw Numpy prediction probabilities\n    # Reshape entry to the model's expected input shape\n    pred = np.argmax(loaded_model.predict(entry[0].numpy()[0].reshape(1, 224, 224, 3)))\n\n    # Get sample image as numpy array\n    sample_image = entry[0].numpy()[0]\n    # Get associated label\n    sample_label = class_names[entry[1].numpy()[0]]\n    # Get human label based on the prediction\n    prediction_label = class_names[pred]\n    ax = fig.add_subplot(5, 5, i)\n    \n    # Plot image and sample_label alongside prediction_label\n    ax.imshow(np.array(sample_image, np.int32))\n    ax.set_title(\"Actual: %s\\nPred: %s\" % (sample_label, prediction_label))\n    i = i+1\n\nplt.tight_layout()\nplt.show()\n```\n\n![transfer learning efficientnet-b0 model predictions](https://s3.stackabuse.com/media/articles/image-classification-with-transfer-learning-in-keras-create-cutting-edge-models-6.png)\n\nHere, the only misclassification we can see in the first 25 images is a truck being misclassified as a horse. This is likely due to the context - it's in a *forest* and is *brown* and *elongated*. This also fits the description of a horse to a degree, so it's not too surprising that in a blurry, small (224x224) image, the truck was misclassified. \n\nAnother thing that definitely didn't help is that it *looks* like the gate of the truck is open, which could look like the neck of a horse as it feeds on grass.\n\n### Conclusion\n\nTransfer Learning is the process of transferring already learned knowledge representations from one model to another, when applicable.\n\nThis concludes this guide to Transfer Learning for Image Classification with Keras and Tensorflow. We've started out with taking a look at what Transfer Learning is and how knowledge representations can be shared between models and architectures.\n\nThen, we've taken a look at some of the most popular and cutting edge models for Image Classification released publically, and piggy-backed on one of them - EfficientNet - to help us in classifying some of our own data. We've taken a look at how to load and examine pre-trained models, how to work with their layers, predict with them and decode the results, as well as how to define your own layers and intertwine them with the existing architecture.\n\nFinally, we've loaded and preprocessed a dataset, and trained our new classification top layers on it, before unfreezing the layers and fine-tuning it further through several additional epochs.","body_html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eDeep Learning models\u003c/em\u003e are very versatile and powerful - they're routinely outperforming humans in narrow tasks, and their generalization power is increasing at a rapid rate. New models are being released and benchmarked against community-accepted datasets frequently, and keeping up with all of them is getting harder.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMost of these models are open source, and you can implement them yourself as well.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis means that the average enthusiast can load in and play around with the cutting edge models in their home, on very average machines, not only to gain a deeper understanding and appreciation of the craft, but also to contribute to the scientific discourse and publish their own improvements whenever they're made.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn this guide, you'll learn how to use pre-trained, cutting edge Deep Learning models for Image Classification and repurpose them for your own specific application. This way, you're leveraging their high performance, ingenious architectures \u003cstrong\u003eand\u003c/strong\u003e someone else's training time - while applying these models to your own domain.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003eAll of the code written in the guide is also available on \u003ca target=\"_blank\" href=\"https://github.com/StackAbuse/image-classification-transfer-learning-with-keras/blob/main/SA-Transfer%20Learning%20with%20Keras.ipynb\"\u003e\u003cem\u003eGitHub\u003c/em\u003e\u003c/a\u003e.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003ch3 id=\"transferlearningforcomputervisionandconvolutionalneuralnetworkscnns\"\u003eTransfer Learning for Computer Vision and Convolutional Neural Networks (CNNs)\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eKnowledge\u003c/em\u003e and \u003cem\u003eknowledge representations\u003c/em\u003e are very universal. A computer vision model trained on one dataset learns to recognize patterns that might be very prevalent in many other datasets.\u003c/p\u003e\n\u003cp\u003eNotably, in \u003cstrong\u003e\u003cem\u003e\u0026quot;Deep Learning for the Life Sciences\u0026quot;\u003c/em\u003e\u003c/strong\u003e, by Bharath Ramsundar, Peter Eastman, Patrick Walters and Vijay Pande, it's noted that:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026quot;There have been multiple studies looking into the use of recommendation system algorithms for use in molecular binding prediction. Machine learning architectures used in one field tend to carry over to other fields, so it’s important to retain the flexibility needed for innovative work.\u0026quot;\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFor instance, straight and curved lines, which are typically learned at a lower level of a CNN hierarchy are bound to be present in practically all datasets. Some high-level features, such as the ones that distinguish a bee from an ant are going to be represented and learned much higher in the hierarchy:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/image-classification-with-transfer-learning-in-keras-create-cutting-edge-models-1.png\" alt=\"feature hierarchies for convolutional neural networks\"\u003e\u003c/p\u003e\n\u003cp\u003eThe \u0026quot;fine line\u0026quot; between these is what you can reuse! Depending on the level of similarity between your dataset and the one a model's been pre-trained on, you may be able to reuse a small or large portion of it.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA model that classifies human-made structures (trained on a dataset such as the Places365) and a model that classifies animals (trained on a dataset such as ImageNet) are bound to have \u003cstrong\u003esome\u003c/strong\u003e shared patterns, although, not a lot.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eYou might want to train a model to distinguish, say, \u003cem\u003ebuses\u003c/em\u003e and \u003cem\u003ecars\u003c/em\u003e for a self-driving car's vision system. You may also reasonably choose to use a very performant architecture that has proven to work well on datasets similar to yours. Then, the long process of training begins, and you end up having a performant model of your own!\u003c/p\u003e\n\u003cp\u003eHowever, if another model is likely to have similar representations on lower \u003cem\u003eand\u003c/em\u003e higher levels of abstraction, there's no need to re-train a model from scratch. You may decide to use some of the already pre-trained weights, which are just as applicable to your own application of the model as they were applicable to the creator of the original architecture. You'd be \u003cstrong\u003etransferring\u003c/strong\u003e some of the knowledge from an already existing model to a new one, and this is known as \u003cstrong\u003e\u003cem\u003eTransfer Learning\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThe closer the dataset of a pre-trained model is to your own, the more you can transfer. The more you can transfer, the more of your own time and computation you can save. It's worth remembering that training neural networks does have a carbon footprint, so you're not only saving time!\u003c/p\u003e\n\u003cp\u003eTypically, \u003cstrong\u003eTransfer Learning\u003c/strong\u003e is done by loading a pre-trained model, and \u003cstrong\u003efreezing\u003c/strong\u003e its layers. In many cases, you can just cut off the \u003cstrong\u003eclassification layer\u003c/strong\u003e (the final layers, or, \u003cem\u003ehead\u003c/em\u003e) and just re-train the classification layer, while keeping all of the other abstraction layers intact. In other cases, you may decide to re-train several layers in the hierarchy instead, and this is typically done when the datasets contain sufficiently different data points that re-training multiple layers is warranted. You may also decide to re-train the entirety of the model to fine-tune all of the layers.\u003c/p\u003e\n\u003cp\u003eThese two approaches can be summarized as:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUsing the Convolutional Network as a Feature Extractor\u003c/li\u003e\n\u003cli\u003eFine-Tuning the Convolutional Network\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn the former, you use the underlying entropic capacity of the model as a fixed feature extractor, and just train a dense network on top to discern between these features. In the latter, you fine-tune the entire (or portion of the) convolutional network, if it doesn't already have representative feature maps for some other more specific dataset, while also relying on the already trained feature maps.\u003c/p\u003e\n\u003cp\u003eHere's a visual representation of how Transfer Learning works:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/image-classification-with-transfer-learning-in-keras-create-cutting-edge-models-2.png\" alt=\"how does transfer learning work?\"\u003e\u003c/p\u003e\n\u003ch3 id=\"establishedandcuttingedgeimageclassificationmodels\"\u003eEstablished and Cutting Edge Image Classification Models\u003c/h3\u003e\n\u003cp\u003eMany models exist out there, and for well-known datasets, you're likely to find hundreds of well-performing models published in online repositories and papers. A good holistic view of models trained on the ImageNet dataset can be seen at \u003ca rel=\"nofollow noopener noreferrer\" target=\"_blank\" href=\"https://paperswithcode.com/sota/image-classification-on-imagenet\"\u003ePapersWithCode\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSome of the well-known published architectures that have subsequently been ported into many Deep Learning frameworks include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eEfficientNet\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eSENet\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eXception\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eResNet\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eVGGNet\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eAlexNet\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eLeNet-5\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe list of models on PapersWithCode is constantly being updated, and you shouldn't hang up on the position of these models there. Many of them are outperformed by various other models as of writing, and many of the new models are actually based on the ones outlined in the list above. It's worth noting that \u003cstrong\u003eTransfer Learning\u003c/strong\u003e actually played an important role in the newer, higher accuracy models!\u003c/p\u003e\n\u003cp\u003eThe downside is - a lot of the newest models aren't ported as pre-trained models within frameworks such as Tensorflow and PyTorch. It's not like you'll be losing out on \u003cem\u003ea lot\u003c/em\u003e of the performance, so going with any of the well-established ones isn't really bad at all.\u003c/p\u003e\n\u003ch3 id=\"transferlearningwithkerasadaptingexistingmodels\"\u003eTransfer Learning with Keras - Adapting Existing Models\u003c/h3\u003e\n\u003cp\u003eWith Keras, the pre-trained models are available under the \u003ccode\u003etensorflow.keras.applications\u003c/code\u003e module. Each model has its own sub-module and class. When loading a model in, you can set a couple of optional arguments to control how the models are being loaded in.\u003c/p\u003e\n\u003cp\u003eFor instance, the \u003ccode\u003eweights\u003c/code\u003e argument, if present, defines the pre-trained weights. If omitted, only the architecture (untrained network) will be loaded in. If you supply the name of a dataset - a pre-trained network will be returned for that dataset.\u003c/p\u003e\n\u003cp\u003eAdditionally, since you'll most likely be removing the top layer(s) for Transfer Learning, the \u003ccode\u003einclude_top\u003c/code\u003e argument is used to define whether the top layer(s) should be present or not!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow.keras.applications \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e models\n\n\u003cspan class=\"hljs-comment\"\u003e# 98 MB\u003c/span\u003e\nresnet = models.resnet50.ResNet50(weights=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;imagenet\u0026#x27;\u003c/span\u003e, include_top=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# 528MB\u003c/span\u003e\nvgg16 = models.vgg16.VGG16(weights=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;imagenet\u0026#x27;\u003c/span\u003e, include_top=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# 23MB\u003c/span\u003e\nnnm = models.NASNetMobile(weights=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;imagenet\u0026#x27;\u003c/span\u003e, include_top=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# etc...\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e If you've never loaded pre-trained models before, they'll be downloaded over an internet connection. This may take anywhere between a few seconds and a couple of minutes, depending on your internet speed and the size of the models. The size of pre-trained models spans from as little as \u003cem\u003e14MB\u003c/em\u003e (typically lower for \u003cem\u003eMobile\u003c/em\u003e models) to as high as \u003cem\u003e549MB\u003c/em\u003e.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003e\u003cstrong\u003e\u003cem\u003eEfficientNet\u003c/em\u003e\u003c/strong\u003e is a family of networks that are quite performant, scalable and, well, efficient. They were made with reducing learnable parameters in mind, so they only have 4M parameters to train. While 4M is still a large number, consider that VGG16, for instance, has 20M. On a home setup, this also helps with training times significantly!\u003c/p\u003e\n\u003cp\u003eLet's load in one of the members of the EfficientNet family - EfficientNet-B0:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eeffnet = keras.applications.EfficientNetB0(weights=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;imagenet\u0026#x27;\u003c/span\u003e, include_top=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\neffnet.summary()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eModel: \u0026quot;efficientnetb0\u0026quot;\n________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n========================================================================================\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n________________________________________________________________________________________\nrescaling (Rescaling)           (None, 224, 224, 3)  0           input_1[0][0]                    \n________________________________________________________________________________________\n...\n...\nblock7a_project_conv (Conv2D)   (None, 7, 7, 320)    368640      block7a_se_excite[0][0]          \n________________________________________________________________________________________\nblock7a_project_bn (BatchNormal (None, 7, 7, 320)    1280        block7a_project_conv[0][0]                    \n========================================================================================\nTotal params: 3,634,851\nTrainable params: 3,592,828\nNon-trainable params: 42,023\n________________________________________________________________________________________\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOn the other hand, if we were to load in EfficientNet-B0 with the top included, we'd also have a few new layers at the end, that were trained to classify the data for ImageNet. This is the top of the model that we'll be training ourselves for our own application:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eeffnet = keras.applications.EfficientNetB0(weights=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;imagenet\u0026#x27;\u003c/span\u003e, include_top=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\neffnet.summary()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis would include the \u003ccode\u003eFlatten\u003c/code\u003e and \u003ccode\u003eDense\u003c/code\u003e layers, which then prop up the parameter size significantly:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eModel: \u0026quot;efficientnetb0\u0026quot;\n_________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n=========================================================================================\ninput_11 (InputLayer)           [(None, 224, 224, 3) 0                                            \n_________________________________________________________________________________________\n...\n...\n_________________________________________________________________________________________\ntop_conv (Conv2D)               (None, 7, 7, 1280)   409600      block7a_project_bn[0][0]         \n_________________________________________________________________________________________\ntop_bn (BatchNormalization)     (None, 7, 7, 1280)   5120        top_conv[0][0]                   \n_________________________________________________________________________________________\ntop_activation (Activation)     (None, 7, 7, 1280)   0           top_bn[0][0]                     \n_________________________________________________________________________________________\navg_pool (GlobalAveragePooling2 (None, 1280)         0           top_activation[0][0]             \n_________________________________________________________________________________________\ntop_dropout (Dropout)           (None, 1280)         0           avg_pool[0][0]                   \n_________________________________________________________________________________________\npredictions (Dense)             (None, 1000)         1281000     top_dropout[0][0]                \n=========================================================================================\nTotal params: 5,330,571\nTrainable params: 5,288,548\nNon-trainable params: 42,023\n_________________________________________________________________________________________\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAgain, we won't be using the top layers, as we'll be adding our own top to the EfficientNet model and re-training only the ones we add on top. It is worth noting what the architecture is already built with! They seem to be using a \u003ccode\u003eConv2D\u003c/code\u003e layer, followed by a \u003ccode\u003eBatchNormalization\u003c/code\u003e, \u003ccode\u003eGlobalAveragePooling2D\u003c/code\u003e and \u003ccode\u003eDropout\u003c/code\u003e before the final \u003ccode\u003eDense\u003c/code\u003e classification layer. While we don't have to strictly follow this approach (and other approaches may prove to be better for another dataset), it's reasonable to remember how the original top looked like.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e Data preprocessing plays a crucial role in model training, and most models will have different preprocessing pipelines. You don't have to perform guesswork here! Where applicable, a model comes with its own \u003ccode\u003epreprocess_input()\u003c/code\u003e function.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eThe \u003ccode\u003epreprocess_input()\u003c/code\u003e function applies the same preprocessing steps to the input as they were applied during training. You can import the function from the respective module of the model, if a model resides in its own module. For instance, VGG16 has its own \u003ccode\u003epreprocess_input\u003c/code\u003e function:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e keras.applications.vgg16 \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e preprocess_input\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThat being said, loading in a model, preprocessing input for it and predicting a result in Keras is as easy as:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow.keras.applications \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e models\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e keras.applications.vgg16 \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e preprocess_input\n\nvgg16 = models.vgg16.VGG16(weights=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;imagenet\u0026#x27;\u003c/span\u003e, include_top=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nimg = \u003cspan class=\"hljs-comment\"\u003e# get data\u003c/span\u003e\nimg = preprocess_input(img)\npred = vgg16.predict(img)\n\u003c/code\u003e\u003c/pre\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e Not all models have a dedicated \u003ccode\u003epreprocess_input()\u003c/code\u003e function, because the preprocessing is \u003cem\u003edone within the model itself\u003c/em\u003e. For instance, EfficientNet that we'll be using doesn't have its own dedicated preprocessing function, as the \u003ccode\u003eRescaling\u003c/code\u003e layer takes care of that.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eThat's it! Now, since the \u003ccode\u003epred\u003c/code\u003e array doesn't really contain human-readable data, you can also import the \u003ccode\u003edecode_predictions()\u003c/code\u003e function alongside the \u003ccode\u003epreprocess_input()\u003c/code\u003e function from a module. Alternatively, you can import the generic \u003ccode\u003edecode_predictions()\u003c/code\u003e function that also applies to models that don't have their dedicated modules:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e keras.applications.model_name \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e preprocess_input, decode_predictions\n\u003cspan class=\"hljs-comment\"\u003e# OR\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e keras.applications.imagenet_utils \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e decode_predictions\n\u003cspan class=\"hljs-comment\"\u003e# ...\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(decode_predictions(pred))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTying this together, let's get an image of a black bear via \u003ccode\u003eurllib\u003c/code\u003e, save that file into a target size suitable for EfficientNet (the input layer expects a shape of \u003ccode\u003e(None, 224, 224, 3)\u003c/code\u003e) and classify it with the pre-trained model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e keras\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e keras.applications.vgg16 \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e preprocess_input, decode_predictions\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tensorflow.keras.preprocessing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e image\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e urllib.request\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\n\u003cspan class=\"hljs-comment\"\u003e# Public domain image\u003c/span\u003e\nurl = \u003cspan class=\"hljs-string\"\u003e\u0026#x27;https://upload.wikimedia.org/wikipedia/commons/0/02/Black_bear_large.jpg\u0026#x27;\u003c/span\u003e\nurllib.request.urlretrieve(url, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;bear.jpg\u0026#x27;\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# Load image and resize (doesn\u0026#x27;t keep aspect ratio)\u003c/span\u003e\nimg = image.load_img(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;bear.jpg\u0026#x27;\u003c/span\u003e, target_size=(\u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e))\n\u003cspan class=\"hljs-comment\"\u003e# Turn to array of shape (224, 224, 3)\u003c/span\u003e\nimg = image.img_to_array(img)\n\u003cspan class=\"hljs-comment\"\u003e# Expand array into (1, 224, 224, 3)\u003c/span\u003e\nimg = np.expand_dims(img, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# Preprocess for models that have specific preprocess_input() function\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# img_preprocessed = preprocess_input(img)\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# Load model and run prediction\u003c/span\u003e\neffnet = keras.applications.EfficientNetB0(weights=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;imagenet\u0026#x27;\u003c/span\u003e, include_top=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\npred = effnet.predict(img)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(decode_predictions(pred))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e[[\n(\u0026#x27;n02133161\u0026#x27;, \u0026#x27;American_black_bear\u0026#x27;, 0.6024658),\n(\u0026#x27;n02132136\u0026#x27;, \u0026#x27;brown_bear\u0026#x27;, 0.1457715),\n(\u0026#x27;n02134418\u0026#x27;, \u0026#x27;sloth_bear\u0026#x27;, 0.09819221),\n(\u0026#x27;n02510455\u0026#x27;, \u0026#x27;giant_panda\u0026#x27;, 0.0069221947),\n(\u0026#x27;n02509815\u0026#x27;, \u0026#x27;lesser_panda\u0026#x27;, 0.005077324)\n]]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt's fairly certain that the image is an image of an American Black Bear, which is right! When preprocessed with a preprocessing function, the image may change significantly. For instance, VGG16's preprocessing function would change the color of the bear's fur:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/image-classification-with-transfer-learning-in-keras-create-cutting-edge-models-3.PNG\" alt=\"preprocessing image for VGG16 CNN\"\u003e\u003c/p\u003e\n\u003cp\u003eIt looks a lot more brown now! If we were to feed this image into EfficientNet, it'd think it's a \u003cem\u003ebrown bear\u003c/em\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e[[\n(\u0026#x27;n02132136\u0026#x27;, \u0026#x27;brown_bear\u0026#x27;, 0.7152758), \n(\u0026#x27;n02133161\u0026#x27;, \u0026#x27;American_black_bear\u0026#x27;, 0.15667434), \n(\u0026#x27;n02134418\u0026#x27;, \u0026#x27;sloth_bear\u0026#x27;, 0.012813852), \n(\u0026#x27;n02134084\u0026#x27;, \u0026#x27;ice_bear\u0026#x27;, 0.0067828503), (\u0026#x27;n02117135\u0026#x27;, \u0026#x27;hyena\u0026#x27;, 0.0050422684)\n]]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAwesome! The model works. Now, let's add a new top to it and re-train the top to perform classification for something outside of the ImageNet set.\u003c/p\u003e\n\u003ch4 id=\"addinganewtoptoapretrainedmodel\"\u003eAdding a New Top to a pre-trained Model\u003c/h4\u003e\n\u003cp\u003eWhen performing transfer learning, you'll be loading models without tops, or remove them manually:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Load without top\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# When adding new layers, we also need to define the input_shape\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# so that  the new Dense layers have a fixed input_shape as well\u003c/span\u003e\neffnet_base = keras.applications.EfficientNetB0(weights=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;imagenet\u0026#x27;\u003c/span\u003e, \n                                          include_top=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e, \n                                          input_shape=((\u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)))\n\n\u003cspan class=\"hljs-comment\"\u003e# Or load the full model\u003c/span\u003e\nfull_effnet = keras.applications.EfficientNetB0(weights=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;imagenet\u0026#x27;\u003c/span\u003e, \n                                            include_top=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, \n                                            input_shape=((\u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)))\n                                            \n\u003cspan class=\"hljs-comment\"\u003e# And then remove X layers from the top\u003c/span\u003e\ntrimmed_effnet = keras.Model(inputs=full_effnet.\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e, outputs=full_effnet.layers[-\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e].output)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe'll be going with the first option since it's more convenient. Depending on whether you'd like to fine-tune the convolutional blocks or not - you'll either freeze or won't freeze them. Say we want to use the underlying pre-trained feature maps and freeze the layers so that we only re-train the new classification layers at the top:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eeffnet_base.trainable = \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou don't need to iterate through the model and set each layer to be \u003ccode\u003etrainable\u003c/code\u003e or not, though you also can. If you'd like to turn off the first \u003ccode\u003en\u003c/code\u003e layers, and allow some higher-level feature maps to be fine-tuned, but leave the lower-level ones untouched, you can:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e layer \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e effnet_base.layers[:-\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]:\n    layer.trainable = \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, we've set all layers in the base model to be untrainable, except for the last two. If we check the model, there are only ~2.5K trainable parameters now:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eeffnet_base.summary()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e# ...                \n=========================================================================================\nTotal params: 4,049,571\nTrainable params: 2,560\nNon-trainable params: 4,047,011\n_________________________________________________________________________________________\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, let's define a \u003ccode\u003eSequential\u003c/code\u003e model that'll be put on top of this \u003ccode\u003eeffnet_base\u003c/code\u003e. Fortunately, chaining models in Keras is as easy as making a new model and putting it on top of another one! You can leverage the Functional API and just chain a few new layers on top of a model.\u003c/p\u003e\n\u003cp\u003eLet's add a \u003ccode\u003eConv2D\u003c/code\u003e layer, a \u003ccode\u003eBatchNormalization\u003c/code\u003e layer, a \u003ccode\u003eGlobalAveragePooling2D\u003c/code\u003e layer, some \u003ccode\u003eDropout\u003c/code\u003e and a couple of fully connected layers after a \u003ccode\u003eFlatten\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003econv2d = keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e)(effnet_base.output, training=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\nbn = keras.layers.BatchNormalization()(conv2d)\ngap = keras.layers.GlobalAveragePooling2D()(bn)\ndo = keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e)(gap)\nflatten = keras.layers.Flatten()(gap)\nfc1 = keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e512\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e)(flatten)\noutput = keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e)(fc1)\n\nnew_model = keras.Model(inputs=effnet_base.\u003cspan class=\"hljs-built_in\"\u003einput\u003c/span\u003e, outputs=output)\n\u003c/code\u003e\u003c/pre\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e When adding the layers of the EfficientNet, we set the \u003ccode\u003etraining\u003c/code\u003e to \u003ccode\u003eFalse\u003c/code\u003e. This puts the network in \u003cem\u003einference mode\u003c/em\u003e instead of \u003cem\u003etraining mode\u003c/em\u003e and it's a different parameter than the \u003ccode\u003etrainable\u003c/code\u003e we've set to \u003ccode\u003eFalse\u003c/code\u003e earlier. This is a crucial step if you wish to unfreeze layers later on! \u003cem\u003eBatchNormalization\u003c/em\u003e computes moving statistics. When unfrozen, it'll start applying updates to parameters again, and will \u0026quot;undo\u0026quot; the training done before fine-tuning. Since TF 2.0, setting the model's \u003ccode\u003etrainable\u003c/code\u003e as \u003ccode\u003eFalse\u003c/code\u003e also turns \u003ccode\u003etraining\u003c/code\u003e to \u003ccode\u003eFalse\u003c/code\u003e but only for \u003ccode\u003eBatchNormalization\u003c/code\u003e layers, so this step is unnecessary for versions after TF 2.0.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eAlternatively, you can use the Sequential API and call the \u003ccode\u003eadd()\u003c/code\u003e method multiple times:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003enew_model = keras.Sequential()\nnew_model.add(effnet_base) \u003cspan class=\"hljs-comment\"\u003e# Add entire model\u003c/span\u003e\nnew_model.add(keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e))\nnew_model.add(keras.layers.BatchNormalization())\nnew_model.add(keras.layers.GlobalAveragePooling2D())\nnew_model.add(keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e))\nnew_model.add(keras.layers.Flatten())\nnew_model.add(keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e512\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e))\nnew_model.add(keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis adds the entire model as a layer itself, so it's treated as one entity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eLayer: 0, Trainable: False # Entire EfficientNet model\nLayer: 1, Trainable: True\nLayer: 2, Trainable: True\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOn the other hand, you can extract all of the layers and add them instead as separate entities, by adding the \u003ccode\u003eoutput\u003c/code\u003e of \u003ccode\u003eeffnet_base\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003enew_model = keras.Sequential()\nnew_model.add(effnet_base.output) \u003cspan class=\"hljs-comment\"\u003e# Add unwrapped layers\u003c/span\u003e\nnew_model.add(keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e))\nnew_model.add(keras.layers.BatchNormalization())\nnew_model.add(keras.layers.GlobalAveragePooling2D())\nnew_model.add(keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e))\nnew_model.add(keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn any of these cases - we've added 10 output classes, since we'll be using the CIFAR10 dataset later on, which has 10 classes! Let's take a look at the trainable layers in the network:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e index, layer \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eenumerate\u003c/span\u003e(new_model.layers):\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Layer: {}, Trainable: {}\u0026quot;\u003c/span\u003e.\u003cspan class=\"hljs-built_in\"\u003eformat\u003c/span\u003e(index, layer.trainable))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eLayer: 0, Trainable: False\nLayer: 1, Trainable: False\nLayer: 2, Trainable: False\n...\nLayer: 235, Trainable: False\nLayer: 236, Trainable: False\nLayer: 237, Trainable: True\nLayer: 238, Trainable: True\nLayer: 239, Trainable: True\nLayer: 240, Trainable: True\nLayer: 241, Trainable: True\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAwesome! Let's load in the dataset, preprocess it and re-train the classification layers on it.\u003c/p\u003e\n\u003ch4 id=\"loadingandpreprocessingdata\"\u003eLoading and Preprocessing Data\u003c/h4\u003e\n\u003cp\u003eWe'll be working with the \u003ca rel=\"nofollow noopener noreferrer\" target=\"_blank\" href=\"https://www.tensorflow.org/datasets/catalog/cifar10\"\u003e\u003cem\u003eCIFAR10 dataset\u003c/em\u003e\u003c/a\u003e. This is a dataset that's not too hard to classify since it only has 10 classes, and we'll be leveraging a well-received architecture to help us in that endeavor.\u003c/p\u003e\n\u003cp\u003eIts \u0026quot;older brother\u0026quot;, CIFAR100 is a genuinely hard one to work with. It has 50.000 images, with 100 labels, meaning each class has \u003cstrong\u003eonly 500 samples\u003c/strong\u003e. This is extremely hard to get right on so few labels, and almost all well-performing models on the dataset use heavy data augmentation.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eData augmentation is an art and science in and of itself, and is out of scope for this guide - so we'll only be diversifying the dataset with a couple of random transformations.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFor brevity's sake, we'll stick to CIFAR10, to emulate the dataset you'll be working with yourself!\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e Keras' \u003ccode\u003edatasets\u003c/code\u003e module contains a few datasets, but these are mainly meant for benchmarking and learning. We can use \u003ccode\u003etensorflow_datasets\u003c/code\u003e to get access to a much larger corpora of datasets! Alternatively, you can use any other source, such as Kaggle or academic repositories.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eWe'll be using \u003ccode\u003etensorflow_datasets\u003c/code\u003e to download the CIFAR10 dataset, get the labels and the number of classes:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow_datasets \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tfds\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tf\n\ndataset, info = tfds.load(\u003cspan class=\"hljs-string\"\u003e\u0026quot;cifar10\u0026quot;\u003c/span\u003e, as_supervised=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, with_info=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# Save class_names and n_classes for later\u003c/span\u003e\nclass_names = info.features[\u003cspan class=\"hljs-string\"\u003e\u0026quot;label\u0026quot;\u003c/span\u003e].names\nn_classes = info.features[\u003cspan class=\"hljs-string\"\u003e\u0026quot;label\u0026quot;\u003c/span\u003e].num_classes\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(class_names) \u003cspan class=\"hljs-comment\"\u003e# [\u0026#x27;airplane\u0026#x27;, \u0026#x27;automobile\u0026#x27;, \u0026#x27;bird\u0026#x27;, \u0026#x27;cat\u0026#x27;, \u0026#x27;deer\u0026#x27;, \u0026#x27;dog\u0026#x27;, \u0026#x27;frog\u0026#x27;, \u0026#x27;horse\u0026#x27;, \u0026#x27;ship\u0026#x27;, \u0026#x27;truck\u0026#x27;]\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(n_classes) \u003cspan class=\"hljs-comment\"\u003e# 10\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can get to know the dataset like this, but we won't be diving into that right now. Let's split it into a \u003ccode\u003etrain_set\u003c/code\u003e, \u003ccode\u003evalid_set\u003c/code\u003e and \u003ccode\u003etest_set\u003c/code\u003e instead:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003etest_set, valid_set, train_set = tfds.load(\u003cspan class=\"hljs-string\"\u003e\u0026quot;cifar10\u0026quot;\u003c/span\u003e, \n                                           split=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;train[:10%]\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[10%:25%]\u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026quot;train[25%:]\u0026quot;\u003c/span\u003e],\n                                           as_supervised=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Train set size: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(train_set)) \u003cspan class=\"hljs-comment\"\u003e# Train set size:  37500\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Test set size: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(test_set)) \u003cspan class=\"hljs-comment\"\u003e# Test set size:  5000\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Valid set size: \u0026quot;\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(valid_set)) \u003cspan class=\"hljs-comment\"\u003e# Valid set size:  7500\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e The \u003ccode\u003esplit\u003c/code\u003e argument expects \u003ccode\u003etrain\u003c/code\u003e and \u003ccode\u003etest\u003c/code\u003e keywords, and there's no \u003ccode\u003evalid\u003c/code\u003e keyword that can be used to extract a validation set. Because of this, we need to perform the slightly awkward and clunky split as we have - with a \u003cem\u003e10/15/75 split\u003c/em\u003e.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eNow, the CIFAR10 images are significantly different from the ImageNet images! Namely, CIFAR10 images are just 32x32, while our EfficientNet model expects 224x224 images. We'll want to resize the images in any case. We might also want to apply some transformation functions on duplicate images to artificially expand the sample size per class if the dataset doesn't have enough of them. In the case of CIFAR10, this isn't an issue, as there are enough images per class, but with CIFAR100 - it's a different story. It's worth noting that, when upscaling images that are this small, even humans have a significant difficulty discerning what's on some of the images.\u003c/p\u003e\n\u003cp\u003eFor instance, here are a few non-resized images:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-4.png\" alt=\"cifar100 image examples\"\u003e\u003c/p\u003e\n\u003cp\u003eCan you tell what's on these with confidence? Consider the lifelong amount of context you have for these images, as well, which the model doesn't have. It's worth keeping this in mind when you train it and observe the accuracy.\u003c/p\u003e\n\u003cp\u003eLet's define a preprocessing function for each image and its associated label:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003epreprocess_image\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eimage, label\u003c/span\u003e):\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# Resize to EfficientNet size\u003c/span\u003e\n    resized_image = tf.image.resize(image, [\u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e])\n    \u003cspan class=\"hljs-comment\"\u003e# Random flips and rotations (fully optional, and doesn\u0026#x27;t impact performance much no augmentation takes place)\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# If we run this function multiple times, it\u0026#x27;ll net different results\u003c/span\u003e\n    img = tf.image.random_flip_left_right(resized_image)\n    img = tf.image.random_flip_up_down(img)\n    img = tf.image.rot90(img)\n    \u003cspan class=\"hljs-comment\"\u003e# Preprocess image with model-specific function if it has one\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# img = preprocess_input(img)\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e img, label\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd finally, we'll want to apply this function to each image in the sets! We haven't performed augmentation by expanding the sets here, though you could. For brevity's sake, we'll avoid performing data augmentation.\u003c/p\u003e\n\u003cp\u003eThis is easily done via the \u003ccode\u003emap()\u003c/code\u003e function. Since the input into the network also expects batches (\u003ccode\u003e(None, 224, 224, 3)\u003c/code\u003e instead of \u003ccode\u003e(224, 224, 3)\u003c/code\u003e) - we'll also \u003ccode\u003ebatch()\u003c/code\u003e the datasets after mapping:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003etrain_set = train_set.\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(preprocess_image).batch(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e).prefetch(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\ntest_set = test_set.\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(preprocess_image).batch(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e).prefetch(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\nvalid_set = valid_set.\u003cspan class=\"hljs-built_in\"\u003emap\u003c/span\u003e(preprocess_image).batch(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e).prefetch(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e The \u003ccode\u003eprefetch()\u003c/code\u003e function is optional but helps with efficiency. As the model is training on a single batch, the \u003ccode\u003eprefetch()\u003c/code\u003e function pre-fetches the next batch so it's not waited upon when the training step is finished.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eFinally, we can train the model!\u003c/p\u003e\n\u003ch4 id=\"trainingamodel\"\u003eTraining a Model\u003c/h4\u003e\n\u003cp\u003eWith the data loaded, preprocessed and split into adequate sets - we can train the model on it. The optimizer as well as its hyperparameters, loss function and metrics generally depend on your specific task.\u003c/p\u003e\n\u003cp\u003eSince we're doing sparse classification, a \u003ccode\u003esparse_categorical_crossentropy\u003c/code\u003e loss should work well, and the \u003ccode\u003eAdam\u003c/code\u003e optimizer is a reasonable default loss function. Let's compile the model, and train it on a few epochs. It's worth remembering that most of the layers in the network are frozen! We're only training the new classifier on top of the extracted feature maps.\u003c/p\u003e\n\u003cp\u003eOnly once we train the top layers, we may decide to \u003cem\u003eunfreeze\u003c/em\u003e the feature extraction layers, and let them fine-tune a bit more. This step is optional, and in many cases, you won't unfreeze them (mainly when working with really large networks). A good rule of thumb is to try and compare the datasets and guesstimate which levels of the hierarchy you can re-use without re-training.\u003c/p\u003e\n\u003cp\u003eIf they're \u003cem\u003ereally\u003c/em\u003e different, you probably chose a network pre-trained on the wrong dataset. It wouldn't be efficient to use feature extraction of Places365 (man-made objects) for classifying animals. However, it would make sense to use a network trained on ImageNet (which has various objects, animals, plants and humans) and then use it for a different dataset with relatively similar categories, such as CIFAR10.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e Depending on the architecture you're using, unfreezing the layers might be a bad idea, due to their size. There's a \u003cem\u003egood chance\u003c/em\u003e that your local machine will run out of memory when trying to tackle a 20M parameter model and loading a training step into the RAM/VRAM. When possible, try to find an architecture pre-trained on a dataset that's sufficiently similar to yours that you don't have to change the feature extractors. If you have to, it's not impossible but does make the process much slower. We'll cover that later.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eLet's train the new network (really, only the top of it) for 10 epochs:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eoptimizer = keras.optimizers.Adam(learning_rate=\u003cspan class=\"hljs-number\"\u003e2e-5\u003c/span\u003e)\n\nnew_model.\u003cspan class=\"hljs-built_in\"\u003ecompile\u003c/span\u003e(loss=\u003cspan class=\"hljs-string\"\u003e\u0026quot;sparse_categorical_crossentropy\u0026quot;\u003c/span\u003e, \n                  optimizer=optimizer, \n                  metrics=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;accuracy\u0026quot;\u003c/span\u003e])\n\nhistory = new_model.fit(train_set, \n                        epochs=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e,\n                        validation_data=valid_set)\n\u003c/code\u003e\u003c/pre\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e This may take some time and is ideally done on a GPU. Depending on how large the model is, and the dataset being fed into it. If you don't have access to a GPU, it's advised to run this code on any of the cloud providers that give you access to a free GPU, such as \u003cem\u003eGoogle Collab\u003c/em\u003e, \u003cem\u003eKaggle Notebooks\u003c/em\u003e, etc. Each epoch can take anywhere from 60 seconds on stronger GPUs to 10 minutes, on weaker ones.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eThis is the point in which you sit back and go grab a coffee (or tea)! After 10 epochs, the train and validation accuracy are looking good:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch 1/10\n1172/1172 [==============================] - 92s 76ms/step - loss: 1.6582 - accuracy: 0.6373 - val_loss: 1.1582 - val_accuracy: 0.7935\n...\nEpoch 10/10\n1172/1172 [==============================] - 89s 76ms/step - loss: 0.0911 - accuracy: 0.9781 - val_loss: 0.3847 - val_accuracy: 0.8792\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e~98% on the training set and ~88% on the validation set - it clearly overfit, but not too badly. Let's test it out and plot the learning curves.\u003c/p\u003e\n\u003ch4 id=\"testingamodel\"\u003eTesting a Model\u003c/h4\u003e\n\u003cp\u003eLet's first test this model out, before trying to unfreeze all of the layers and seeing if we can fine-tune it then:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003enew_model.evaluate(test_set)\n\u003cspan class=\"hljs-comment\"\u003e# 157/157 [==============================] - 10s 61ms/step - loss: 0.3778 - accuracy: 0.8798\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# [0.3778476417064667, 0.879800021648407]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e88% on the testing set, and extremely close to the accuracy on the validation set! Looks like our model is generalizing well, but there's still room for improvement. Let's take a look at the learning curves.\u003c/p\u003e\n\u003cp\u003eThe training curves are to be expected - they're pretty short since we only trained for 10 epochs, but they've quickly plateaued, so we probably wouldn't have gotten much better performance with more epochs. While oscillations do occur and the accuracy could very well rise in Epoch 11 - it's not too likely, so we'll miss out on the chance:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/image-classification-with-transfer-learning-in-keras-create-cutting-edge-models-5.png\" alt=\"transfer learning training curves\"\u003e\u003c/p\u003e\n\u003cp\u003eCan we fine-tune this network further? We've replaced and re-trained the top layers concerned with classification of feature maps. Let's try unfreezing the convolutional layers and fine-tuning them as well!\u003c/p\u003e\n\u003ch4 id=\"unfreezinglayersfinetuninganetworktrainedwithtransferlearning\"\u003eUnfreezing Layers - Fine-Tuning a Network Trained with Transfer Learning\u003c/h4\u003e\n\u003cp\u003eOnce you've finished re-training the top layers, you can close the deal and be happy with your model. For instance, suppose you got a 95% accuracy - you seriously don't need to go further. However, why not?\u003c/p\u003e\n\u003cp\u003eIf you can squeeze out an additional 1% in accuracy, it might not sound like a lot, but consider the other end of the trade. If your model has a 95% accuracy on 100 samples, it misclassified 5 samples. If you up that to 96% accuracy, it misclassified 4 samples.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe \u003cstrong\u003e1% of accuracy\u003c/strong\u003e translates to a \u003cstrong\u003e25% decrease in false classifications\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhatever you can further squeeze out of your model can actually make a significant difference on the number of incorrect classifications. We have a pretty satisfactory 88% accuracy with our model, but we can most probably squeeze more out of it if we just slightly re-train the feature extractors. Again, the images in CIFAR10 are \u003cem\u003emuch\u003c/em\u003e smaller than ImageNet images, and it's almost as if someone with great eyesight suddenly gained a huge prescription and only saw the world through blurry eyes. The feature maps \u003cem\u003ehave to\u003c/em\u003e be at least somewhat different!\u003c/p\u003e\n\u003cp\u003eLet's save the model into a file so we don't lose the progress, and unfreeze/fine-tune a loaded copy, so we don't accidentally mess up the weights on the original one:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003enew_model.save(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;effnet_transfer_learning.h5\u0026#x27;\u003c/span\u003e)\nloaded_model = keras.models.load_model(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;effnet_transfer_learning.h5\u0026#x27;\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, we can fiddle around and change the \u003ccode\u003eloaded_model\u003c/code\u003e without impacting \u003ccode\u003enew_model\u003c/code\u003e! To start out, we'll want to change the \u003ccode\u003eloaded_model\u003c/code\u003e from inference mode back to training mode - i.e. \u003cem\u003eunfreeze the layers\u003c/em\u003e so that they're trainable again.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e Again, if a network uses \u003ccode\u003eBatchNormalization\u003c/code\u003e (and most do), you'll want to keep it frozen before fine-tuning a network. Since we're not freezing the entire base network anymore, we'll just freeze the \u003ccode\u003eBatchNormalization\u003c/code\u003e layers instead and allow other layers to be altered.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eLet's turn off the \u003ccode\u003eBatchNormalization\u003c/code\u003e layers so our training doesn't go down the drain:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e layer \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e loaded_model.layers:\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eisinstance\u003c/span\u003e(layer, keras.layers.BatchNormalization):\n        layer.trainable = \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n        layer.trainable = \u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e index, layer \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eenumerate\u003c/span\u003e(loaded_model.layers):\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Layer: {}, Trainable: {}\u0026quot;\u003c/span\u003e.\u003cspan class=\"hljs-built_in\"\u003eformat\u003c/span\u003e(index, layer.trainable))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet's check if that worked:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eLayer: 0, Trainable: True\nLayer: 1, Trainable: True\nLayer: 2, Trainable: True\nLayer: 3, Trainable: True\nLayer: 4, Trainable: True\nLayer: 5, Trainable: False\nLayer: 6, Trainable: True\nLayer: 7, Trainable: True\nLayer: 8, Trainable: False\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAwesome! Before we can do anything with the model, to \u0026quot;solidify\u0026quot; the trainability, we have to recompile it. This time around, we'll be using a smaller \u003ccode\u003elearning_rate\u003c/code\u003e, since we don't want to alter the network much at all, and just want to fine-tune some of the feature extracting capabilities and the new classification layer on top:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eoptimizer = keras.optimizers.Adam(learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-6\u003c/span\u003e, decay=(\u003cspan class=\"hljs-number\"\u003e1e-6\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e))\n\n\u003cspan class=\"hljs-comment\"\u003e# Recompile after turning to trainable\u003c/span\u003e\nloaded_model.\u003cspan class=\"hljs-built_in\"\u003ecompile\u003c/span\u003e(loss=\u003cspan class=\"hljs-string\"\u003e\u0026quot;sparse_categorical_crossentropy\u0026quot;\u003c/span\u003e, \n                  optimizer=optimizer, \n                  metrics=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;accuracy\u0026quot;\u003c/span\u003e])\n\nhistory = loaded_model.fit(train_set, \n                        epochs=\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e,\n                        validation_data=valid_set)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAgain, this may take some time - so sip on another hot beverage of your choice while this runs in the background. Once it finishes, it should reach up to 92% in validation accuracy and 92.6% on the test set:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch 1/10\n1172/1172 [==============================] - 389s 328ms/step - loss: 0.0552 - accuracy: 0.9863 - val_loss: 0.3493 - val_accuracy: 0.8941\n...\nEpoch 50/50\n1172/1172 [==============================] - 375s 320ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.3454 - val_accuracy: 0.9188\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAgain, this isn't a huge jump from the perspective of accuracy itself, but it is a much more significant reduction in the proportion of misclassifications. Let's evaluate it and visualize some of the predictions:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eloaded_model.evaluate(test_set)\n\n\u003cspan class=\"hljs-comment\"\u003e# 157/157 [==============================] - 10s 61ms/step - loss: 0.3144 - accuracy: 0.9262\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# [0.3143560290336609, 0.9261999726295471]\u003c/span\u003e\n\n\nfig = plt.figure(figsize=(\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e))\n\ni = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e entry \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e test_set.take(\u003cspan class=\"hljs-number\"\u003e25\u003c/span\u003e):\n    \u003cspan class=\"hljs-comment\"\u003e# Predict, get the raw Numpy prediction probabilities\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# Reshape entry to the model\u0026#x27;s expected input shape\u003c/span\u003e\n    pred = np.argmax(loaded_model.predict(entry[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].numpy()[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].reshape(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e224\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)))\n\n    \u003cspan class=\"hljs-comment\"\u003e# Get sample image as numpy array\u003c/span\u003e\n    sample_image = entry[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].numpy()[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n    \u003cspan class=\"hljs-comment\"\u003e# Get associated label\u003c/span\u003e\n    sample_label = class_names[entry[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e].numpy()[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]]\n    \u003cspan class=\"hljs-comment\"\u003e# Get human label based on the prediction\u003c/span\u003e\n    prediction_label = class_names[pred]\n    ax = fig.add_subplot(\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, i)\n    \n    \u003cspan class=\"hljs-comment\"\u003e# Plot image and sample_label alongside prediction_label\u003c/span\u003e\n    ax.imshow(np.array(sample_image, np.int32))\n    ax.set_title(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Actual: %s\\nPred: %s\u0026quot;\u003c/span\u003e % (sample_label, prediction_label))\n    i = i+\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n\nplt.tight_layout()\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/image-classification-with-transfer-learning-in-keras-create-cutting-edge-models-6.png\" alt=\"transfer learning efficientnet-b0 model predictions\"\u003e\u003c/p\u003e\n\u003cp\u003eHere, the only misclassification we can see in the first 25 images is a truck being misclassified as a horse. This is likely due to the context - it's in a \u003cem\u003eforest\u003c/em\u003e and is \u003cem\u003ebrown\u003c/em\u003e and \u003cem\u003eelongated\u003c/em\u003e. This also fits the description of a horse to a degree, so it's not too surprising that in a blurry, small (224x224) image, the truck was misclassified.\u003c/p\u003e\n\u003cp\u003eAnother thing that definitely didn't help is that it \u003cem\u003elooks\u003c/em\u003e like the gate of the truck is open, which could look like the neck of a horse as it feeds on grass.\u003c/p\u003e\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eTransfer Learning is the process of transferring already learned knowledge representations from one model to another, when applicable.\u003c/p\u003e\n\u003cp\u003eThis concludes this guide to Transfer Learning for Image Classification with Keras and Tensorflow. We've started out with taking a look at what Transfer Learning is and how knowledge representations can be shared between models and architectures.\u003c/p\u003e\n\u003cp\u003eThen, we've taken a look at some of the most popular and cutting edge models for Image Classification released publically, and piggy-backed on one of them - EfficientNet - to help us in classifying some of our own data. We've taken a look at how to load and examine pre-trained models, how to work with their layers, predict with them and decode the results, as well as how to define your own layers and intertwine them with the existing architecture.\u003c/p\u003e\n\u003cp\u003eFinally, we've loaded and preprocessed a dataset, and trained our new classification top layers on it, before unfreezing the layers and fine-tuning it further through several additional epochs.\u003c/p\u003e\n","parent_id":null,"type":"article","status":"published","visibility":"public","img_feature":null,"is_featured":false,"locale":"en","custom_excerpt":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"comment_id":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In this detailed step-by-step guide, learn what Transfer Learning is and learn how to create a cutting-edge Image Classification model for CIFAR10, with EfficientNet-B0 in Keras and Tensorflow.","read_time_min":null,"published_by":16,"published_at":1642678200000,"created_by":16,"updated_by":null,"created_at":1642449014426,"updated_at":1643120063147,"contributors":[{"id":16,"name":"David Landup","slug":"david","email":"thealduinmaster@gmail.com","password_hash":"$2a$10$W/oMJdUBSTeG3trWAHa1xO0pQruxuLgD/6hS7VuxPafcmAxeBXmVi","role_id":2,"img_profile":"//s3.stackabuse.com/media/users/865cd7d217ea11c9d9555c4f666e2d73.jpg","img_cover":null,"bio_md":"Entrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs. \n\nGreat passion for accessible education and promotion of reason, science, humanism, and progress.","bio_html":"\u003cp\u003eEntrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs.\u003c/p\u003e\n\u003cp\u003eGreat passion for accessible education and promotion of reason, science, humanism, and progress.\u003c/p\u003e\n","website":"https://www.upwork.com/freelancers/~017664e499a2766871","location":"Serbia","facebook":"","twitter":"","github":null,"status":"active","locale":null,"last_seen_at":1622229427000,"created_by":null,"updated_by":null,"created_at":1534532687000,"updated_at":1640861394795,"role":"editor","secret_token":"2a2be92558fae38f89cfbd0c6a4ba90c","is_email_confirmed":false,"_pivot_content_id":1186,"_pivot_user_id":16,"_pivot_role":"author","_pivot_sort_order":0},{"id":110,"name":"Jovana Ninkovic","slug":"jovana","email":"jovana@stackabuse.com","password_hash":"$2a$10$B6/KR02kZDgRohJoEv8T5.C8XfQQwh9j32n3v2pqOFd5V.N59fg1W","role_id":null,"img_profile":null,"img_cover":null,"bio_md":null,"bio_html":null,"website":null,"location":null,"facebook":null,"twitter":null,"github":null,"status":"active","locale":"en","last_seen_at":null,"created_by":1,"updated_by":1,"created_at":1641835336180,"updated_at":1641906261321,"role":"editor","secret_token":"712ecccc20919e061bc6be14f8cdf776","is_email_confirmed":false,"_pivot_content_id":1186,"_pivot_user_id":110,"_pivot_role":"illustrator","_pivot_sort_order":1}],"tags":[{"id":9,"name":"python","slug":"python","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1431358631000,"updated_at":1431358631000,"_pivot_content_id":1186,"_pivot_tag_id":9,"_pivot_sort_order":0},{"id":57,"name":"artificial intelligence","slug":"artificial-intelligence","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1442517465000,"updated_at":1442517465000,"_pivot_content_id":1186,"_pivot_tag_id":57,"_pivot_sort_order":5},{"id":75,"name":"machine learning","slug":"machine-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507305534000,"updated_at":1507305534000,"_pivot_content_id":1186,"_pivot_tag_id":75,"_pivot_sort_order":4},{"id":78,"name":"tensorflow","slug":"tensorflow","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1508009047000,"updated_at":1508009047000,"_pivot_content_id":1186,"_pivot_tag_id":78,"_pivot_sort_order":2},{"id":131,"name":"keras","slug":"keras","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1558780443000,"updated_at":1558780443000,"_pivot_content_id":1186,"_pivot_tag_id":131,"_pivot_sort_order":1},{"id":181,"name":"deep learning","slug":"deep-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1595356360000,"updated_at":1595356360000,"_pivot_content_id":1186,"_pivot_tag_id":181,"_pivot_sort_order":3}],"_pivot_tag_id":75,"_pivot_content_id":1186},{"id":1143,"old_id":null,"uuid":"d2c3bef2-430e-46fe-a4cf-a4b839086346","title":"Keras Callbacks: Save and Visualize Prediction on Each Training Epoch","slug":"custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations","body_md":"### Introduction\n\nKeras is a high-level API, typically used with the Tensorflow library, and has lowered the barrier to entry for many and democratized the creation of Deep Learning models and systems.\n\nWhen just starting out, a high-level API that abstracts most of the inner-workings helps people get the hang of the basics, and build a starting intuition. Down the line though, practitioners naturally want to build a stronger intuition of what happens under the hood both to gain actionable insight and gain a deeper understanding of *how* their model learns.\n\n\u003e In a lot of cases, it's useful to take a look at the learning process of a Deep Neural Network, testing how it predicts values on each learning epoch, and save the values.\n\nThese saved values can be used to visualize the predictions, using libraries like Matplotlib or Seaborn, or can be saved in a log for further analysis in smart systems, or simply analyzed by a human. We typically extract the *learning curves* of a model to gain a better understanding of how it performs through time - but learning curves reflect the *mean loss* through time, and you don't get to *see* how the model performs until it's done training.\n\nKeras has a wonderful feature - **callbacks** which are snippets of code that are called during training, and can be used to customize the training process. Typically, you use callbacks to save the model if it performs well, stop the training if it's overfitting, or otherwise react to or affect the steps in the learning process.\n\nThis makes **callbacks** the natural choice for running predictions on each batch or epoch, and saving the results, and in this guide - we'll take a look at how to run a prediction on the test set, visualize the results, and save them as images, on each training epoch in Keras.\n\n:::note\n**Note:** We'll be building a simple Deep Learning model using Keras in the proceeding sections, but won't put much focus on the implementation or the dataset. This isn't meant to be a guide to building regression models, but a model *is* needed to properly showcase how the callback works.\n:::\n\n\u003e If you're interested in reading more about *how* to build these models and how to get them **highly accurate** instead of just **accurate** - read our extensive and detailed \u003ca target=\"_blank\" href=\"https://stackabuse.com/hands-on-house-price-prediction-deep-learning-in-python-with-keras/\"\u003eHands-On House Price Prediction - Deep Learning in Python with Keras\u003c/a\u003e!\n\n### Building and Evaluating a Deep Learning Model with Keras\n\nLet's build a simple Keras model for illustrational purposes. We'll speed through this section with minimal focus and attention - this isn't a guide on building regression models. We'll be working with the \u003ca target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\"\u003eCalifornia Housing Dataset\u003c/a\u003e, obtained through Scikit-Learn's `datasets` module, which is a dataset meant for *regression*.\n\nLet's go ahead and import the libraries and static methods we'll be using:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n```\n\nNow, let's load in the dataset, \u003ca target=\"_blank\" href=\"https://stackabuse.com/scikit-learns-traintestsplit-training-testing-and-validation-sets/\"\u003esplit it into a training and testing set\u003c/a\u003e (we'll split out a validation set later), and visualize the locations of the houses to check if the data's been loaded correctly:\n\n```python\nX, y = fetch_california_housing(as_frame=True, return_X_y=True)\nx_train, x_test, y_train, y_test = train_test_split(x, y)\n\nplt.figure(figsize=(12, 8))\nsns.scatterplot(data=x, x='Longitude', y='Latitude', size=y, alpha=0.5, hue=y, palette='magma')\nplt.show()\n```\n\n![california dataset visualization](https://stackabuse.s3.amazonaws.com/media/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations-1.png)\n\nLooks like California! Since the data is loaded correctly, we can define a simple sequential Keras model:\n\n```python\ncheckpoint = keras.callbacks.ModelCheckpoint(\"california.h5\", save_best_only=True)\n\nmodel = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', kernel_initializer='normal', kernel_regularizer=\"l2\", input_shape=[x_train.shape[1]]),\n    keras.layers.Dropout(0.2),\n    keras.layers.BatchNormalization(),\n    \n    keras.layers.Dense(64, activation='relu', kernel_initializer='normal', kernel_regularizer=\"l2\"),\n    keras.layers.Dropout(0.2),\n    keras.layers.BatchNormalization(),\n  \n    keras.layers.Dense(1)\n])\n\nmodel.compile(loss='mae',\n              optimizer=keras.optimizers.RMSprop(learning_rate=1e-2, decay=0.1),\n              metrics=['mae'])\n              \nhistory = model.fit(\n    x_train, y_train,\n    epochs=150,\n    batch_size=64,\n    validation_split=0.2,\n    callbacks=[checkpoint]\n)\n```\n\nHere, we've got a simple MLP, with a bit of Dropout and Batch Normalization to battle overfitting, optimized with the _RMSprop_ optimizer and a _Mean Absolute Error_ loss. We've fitted the model for 150 epochs, with a validation split of `0.2`, and a `ModelCheckpoint` callback to save the weights in a file. Running this results in:\n\n```plaintext\n...\nEpoch 150/150\n387/387 [==============================] - 3s 7ms/step - loss: 0.6279 - mae: 0.5976 - val_loss: 0.6346 - val_mae: 0.6042\n```\n\nWe could visualize the learning curves to gain some basic insight into how the training went, but it doesn't tell us the whole story - these are just aggregate means over the training and validation sets during training:\n\n```python\nmodel_history = pd.DataFrame(history.history)\nmodel_history['epoch'] = history.epoch\n\nfig, ax = plt.subplots(1, figsize=(8,6))\nnum_epochs = model_history.shape[0]\n\nax.plot(np.arange(0, num_epochs), model_history[\"mae\"], \n        label=\"Training MAE\")\nax.plot(np.arange(0, num_epochs), model_history[\"val_mae\"], \n        label=\"Validation MAE\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\nThis results in:\n\n![deep learning model learning curves](https://stackabuse.s3.amazonaws.com/media/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations-2.png)\n\nAnd we can evaluate our model with:\n\n```python\nmodel.evaluate(x_test, y_test)\n```\n\n```plaintext\n162/162 [==============================] - 0s 2ms/step - loss: 0.5695 - mae: 0.5451 - mape: 32.2959\n```\n\nAs the target variable is measured in multiples of *$100.000*, which means our network misses the price by up to about *$54.000*, which is a *Mean Absolute Percentage Error* of ~32%. Most traditional Machine Learning methods such as Random Forest Regression, even after more extensive data pre-processing for this dataset achieve around *$52.000*, with tuned hyperparameters - so this is actually a pretty decent result, although, it could be improved with more preprocessing, better tuning and different architectures.\n\nThe point here wasn't to build a particularly accurate model, but we did choose a dataset using which the model wouldn't converge very quickly, so we can observe its dance around the target variables.\n\nA more illustrative way to evaluate how the model's working ditches the aggregate *Mean Absolute Error* and *Mean Absolute Percentage Error* fully, and we can plot a \u003ca target=\"_blank\" href=\"https://stackabuse.com/matplotlib-scatterplot-tutorial-and-examples/\"\u003escatter plot\u003c/a\u003e of the *predicted prices* against the *actual prices*. If they're equal - the plotted markers will follow a straight trajectory diagonally. For reference and scope - we can also plot a diagonal line and evaluate how close each marker is to the line:\n\n```python\ntest_predictions = model.predict(x_test)\ntest_labels = y_test\n\nfig, ax = plt.subplots(figsize=(8,4))\nplt.scatter(test_labels, test_predictions, alpha=0.6, \n            color='#FF0000', lw=1, ec='black')\nlims = [0, 5]\n\nplt.plot(lims, lims, lw=1, color='#0000FF')\nplt.ticklabel_format(useOffset=False, style='plain')\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.xlim(lims)\nplt.ylim(lims)\n\nplt.tight_layout()\nplt.show()\n```\n\nRunning this code results in:\n\n![deep learning regression performance evaluation](https://stackabuse.s3.amazonaws.com/media/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations-3.png)\n\nThe network overprices cheaper houses and underprices more expensive ones - and the estimates have a pretty generous scope (with some predictions on the right being totally out of scope - though, this happens because we haven't cleaned the dataset and many house prices *are* capped at that value when imported). \n\nThis isn't the insight you get from the learning curves, and a network that had the opposite effect - underpricing cheaper houses and overpricing expensive ones might have the same MAE and MAPE but behave totally differently.\n\nWhat we're also interested in is *how* the model got here and how these predictions changed through time and the learning process. This is just the end point of the training process, and there was a fair bit of training involved to get here.\n\nLet's go ahead and write a *custom callback* to add to the list of callbacks in the training process, that will run a prediction on the test set on each epoch, visualize the predictions and save them as an image.\n\n### Custom Prediction Keras Callback with Plots\n\nJust like we've used the `ModelCheckpoint` callback to check whether a model is in its best-performing state on each epoch, and save it into a `.h5` file and persist it - we can write a *custom callback* that'll run predictions, visualize them, and save the images on our disk.\n\nCreating a custom callback boils down to extending the `Callback` class and overriding any of the methods it provides - the ones you *don't* override, retain their default behavior:\n\n```python\nclass PerformancePlotCallback(keras.callbacks.Callback):\n       \n    def on_train_end(self, logs=None):\n      ...\n    def on_epoch_begin(self, epoch, logs=None):\n      ...\n    def on_epoch_end(self, epoch, logs=None):\n      ...\n    def on_test_begin(self, logs=None):\n      ...\n    def on_test_end(self, logs=None):\n      ...\n    # Etc.\n```\n\nDepending on *when* you'd like to predict using your in-the-training model, you'll choose the appropriate method. A good measure of how it's progressing is an *epoch*, so on the end of each training epoch, we'll test the model on our test set.\n\nWe need a way to provide the test set to the callback, since this is external data. The easiest way to do that is to define a *constructor* that accepts the test set and evaluates the *current model* on it, giving you a consistent result:\n\n```python\nclass PerformancePlotCallback(keras.callbacks.Callback):\n    def __init__(self, x_test, y_test):\n        self.x_test = x_test\n        self.y_test = y_test\n        \n    def on_epoch_end(self, epoch, logs=None):\n        print('Evaluating Model...')\n        print('Model Evaluation: ', self.model.evaluate(self.x_test))   \n```\n\nThis simple callback accepts the test set of houses and relevant target variables and evaluates itself on each epoch, printing the result to the console, right alongside the usual Keras output. \n\nIf we were to instantiate and add this callback to the model, and `fit()` it again, we'd see a different result from before:\n\n```python\nperformance_simple = PerformancePlotCallback(x_test, y_test)\n\n# Model definition and compilation...\n\nhistory = model.fit(\n    x_train, y_train,\n    epochs=150,\n    validation_split=0.2,\n    callbacks=[performance_simple]\n)\n```\n\nThis results in:\n\n```plaintext\nEpoch 1/150\n387/387 [==============================] - 3s 7ms/step - loss: 1.0785 - mae: 1.0140 - val_loss: 0.9455 - val_mae: 0.8927\nEvaluating Model...\n162/162 [==============================] - 0s 1ms/step - loss: 0.0528 - mae: 0.0000e+00\nModel Evaluation:  [0.05277165770530701, 0.0]\nEpoch 2/150\n387/387 [==============================] - 3s 7ms/step - loss: 0.9048 - mae: 0.8553 - val_loss: 0.8547 - val_mae: 0.8077\nEvaluating Model...\n162/162 [==============================] - 0s 1ms/step - loss: 0.0471 - mae: 0.0000e+00\nModel Evaluation:  [0.04705655574798584, 0.0]\n...\n```\n\nAwesome! The model is evaluating itself on each epoch, on the data we've passed into the callback. Now, let's modify the callback so it visualizes the predictions instead of printing them to the already cluttered output.\n\nTo simplify things, we'll get the callback to save the images to a folder, so that we can stitch them together into a video or a Gif later on. We'll also include a `model_name` in the constructor to help us differentiate models when generating the images and their filenames:\n\n```python\nclass PerformancePlotCallback(keras.callbacks.Callback):\n    def __init__(self, x_test, y_test, model_name):\n        self.x_test = x_test\n        self.y_test = y_test\n        self.model_name = model_name\n        \n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict(self.x_test)\n        fig, ax = plt.subplots(figsize=(8,4))\n        plt.scatter(y_test, y_pred, alpha=0.6, \n            color='#FF0000', lw=1, ec='black')\n        \n        lims = [0, 5]\n\n        plt.plot(lims, lims, lw=1, color='#0000FF')\n        plt.ticklabel_format(useOffset=False, style='plain')\n        plt.xticks(fontsize=18)\n        plt.yticks(fontsize=18)\n        plt.xlim(lims)\n        plt.ylim(lims)\n\n        plt.tight_layout()\n        plt.title(f'Prediction Visualization Keras Callback - Epoch: {epoch}')\n        plt.savefig('model_train_images/'+self.model_name+\"_\"+str(epoch))\n        plt.close()\n```\n\nHere, we create a Matplotlib figure on each epoch, and plot a \u003ca target=\"_blank\" href=\"https://stackabuse.com/matplotlib-scatterplot-tutorial-and-examples/\"\u003escatter plot\u003c/a\u003e of the predicted prices against the actual prices. Additionally, we've added a diagonal reference line - the closer our scatter plot markers are to the diagonal line, the more accurate our model's predictions were.\n\nThe plot is then saved via \u003ca target=\"_blank\" href=\"https://stackabuse.com/save-plot-as-image-with-matplotlib/\"\u003e`plt.savefig()`\u003c/a\u003e with the model's name and the epoch number, alongside an informative title that lets you know which epoch the model is in during training.\n\nNow, let's use this custom callback again, providing a model name in addition to the `x_test` and `y_test` sets:\n\n```python\ncheckpoint = keras.callbacks.ModelCheckpoint(\"california.h5\", save_best_only=True)\nperformance = PerformancePlotCallback(x_test, y_test, \"california_model\")\n\n# Model definition and compilation...\n              \nhistory = model.fit(\n    x_train, y_train,\n    epochs=150,\n    validation_split=0.2,\n    callbacks=[checkpoint, performance]\n)\n```\n\nThe `PerformancePlotCallback` goes into full swing, and in the designated folder generates an image of the performance on each epoch. The `model_train_images` folder is now filled with 150 plots:\n\n![saving predictions on each epoch keras](https://stackabuse.s3.amazonaws.com/media/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations-4.png)\n\nYou can now use your favorite tool to stitch the images together into a video or a Gif file, or simply peruse them manually. Here's a Gif of the model we've built training on this data:\n\n![animated learning process on each epoch with keras](https://stackabuse.s3.amazonaws.com/media/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations-5.gif)\n\n### Conclusion\n\nIn this guide, we've built a simple model to predict the price of a house in the California Housing Dataset with okay-ish accuracy. We've then taken a look at how to write a custom Keras callback to test a Deep Learning model's performance and visualize it during training, on each epoch. \n\nWe've proceeded to save these images to the disk and created a  Gif from them, giving us a different perspective on the training process than the one we get from analyzing the learning curves of a model.","body_html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eKeras is a high-level API, typically used with the Tensorflow library, and has lowered the barrier to entry for many and democratized the creation of Deep Learning models and systems.\u003c/p\u003e\n\u003cp\u003eWhen just starting out, a high-level API that abstracts most of the inner-workings helps people get the hang of the basics, and build a starting intuition. Down the line though, practitioners naturally want to build a stronger intuition of what happens under the hood both to gain actionable insight and gain a deeper understanding of \u003cem\u003ehow\u003c/em\u003e their model learns.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn a lot of cases, it's useful to take a look at the learning process of a Deep Neural Network, testing how it predicts values on each learning epoch, and save the values.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThese saved values can be used to visualize the predictions, using libraries like Matplotlib or Seaborn, or can be saved in a log for further analysis in smart systems, or simply analyzed by a human. We typically extract the \u003cem\u003elearning curves\u003c/em\u003e of a model to gain a better understanding of how it performs through time - but learning curves reflect the \u003cem\u003emean loss\u003c/em\u003e through time, and you don't get to \u003cem\u003esee\u003c/em\u003e how the model performs until it's done training.\u003c/p\u003e\n\u003cp\u003eKeras has a wonderful feature - \u003cstrong\u003ecallbacks\u003c/strong\u003e which are snippets of code that are called during training, and can be used to customize the training process. Typically, you use callbacks to save the model if it performs well, stop the training if it's overfitting, or otherwise react to or affect the steps in the learning process.\u003c/p\u003e\n\u003cp\u003eThis makes \u003cstrong\u003ecallbacks\u003c/strong\u003e the natural choice for running predictions on each batch or epoch, and saving the results, and in this guide - we'll take a look at how to run a prediction on the test set, visualize the results, and save them as images, on each training epoch in Keras.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e We'll be building a simple Deep Learning model using Keras in the proceeding sections, but won't put much focus on the implementation or the dataset. This isn't meant to be a guide to building regression models, but a model \u003cem\u003eis\u003c/em\u003e needed to properly showcase how the callback works.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cblockquote\u003e\n\u003cp\u003eIf you're interested in reading more about \u003cem\u003ehow\u003c/em\u003e to build these models and how to get them \u003cstrong\u003ehighly accurate\u003c/strong\u003e instead of just \u003cstrong\u003eaccurate\u003c/strong\u003e - read our extensive and detailed \u003ca target=\"_blank\" href=\"https://stackabuse.com/hands-on-house-price-prediction-deep-learning-in-python-with-keras/\"\u003eHands-On House Price Prediction - Deep Learning in Python with Keras\u003c/a\u003e!\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"buildingandevaluatingadeeplearningmodelwithkeras\"\u003eBuilding and Evaluating a Deep Learning Model with Keras\u003c/h3\u003e\n\u003cp\u003eLet's build a simple Keras model for illustrational purposes. We'll speed through this section with minimal focus and attention - this isn't a guide on building regression models. We'll be working with the \u003ca target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\"\u003eCalifornia Housing Dataset\u003c/a\u003e, obtained through Scikit-Learn's \u003ccode\u003edatasets\u003c/code\u003e module, which is a dataset meant for \u003cem\u003eregression\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eLet's go ahead and import the libraries and static methods we'll be using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e seaborn \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e sns\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e tf\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e keras\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e fetch_california_housing\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.model_selection \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e train_test_split\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow, let's load in the dataset, \u003ca target=\"_blank\" href=\"https://stackabuse.com/scikit-learns-traintestsplit-training-testing-and-validation-sets/\"\u003esplit it into a training and testing set\u003c/a\u003e (we'll split out a validation set later), and visualize the locations of the houses to check if the data's been loaded correctly:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eX, y = fetch_california_housing(as_frame=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nx_train, x_test, y_train, y_test = train_test_split(x, y)\n\nplt.figure(figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e))\nsns.scatterplot(data=x, x=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Longitude\u0026#x27;\u003c/span\u003e, y=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Latitude\u0026#x27;\u003c/span\u003e, size=y, alpha=\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e, hue=y, palette=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;magma\u0026#x27;\u003c/span\u003e)\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://stackabuse.s3.amazonaws.com/media/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations-1.png\" alt=\"california dataset visualization\"\u003e\u003c/p\u003e\n\u003cp\u003eLooks like California! Since the data is loaded correctly, we can define a simple sequential Keras model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003echeckpoint = keras.callbacks.ModelCheckpoint(\u003cspan class=\"hljs-string\"\u003e\u0026quot;california.h5\u0026quot;\u003c/span\u003e, save_best_only=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nmodel = keras.Sequential([\n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, kernel_initializer=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;normal\u0026#x27;\u003c/span\u003e, kernel_regularizer=\u003cspan class=\"hljs-string\"\u003e\u0026quot;l2\u0026quot;\u003c/span\u003e, input_shape=[x_train.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]]),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    \n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, kernel_initializer=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;normal\u0026#x27;\u003c/span\u003e, kernel_regularizer=\u003cspan class=\"hljs-string\"\u003e\u0026quot;l2\u0026quot;\u003c/span\u003e),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n  \n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n])\n\nmodel.\u003cspan class=\"hljs-built_in\"\u003ecompile\u003c/span\u003e(loss=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;mae\u0026#x27;\u003c/span\u003e,\n              optimizer=keras.optimizers.RMSprop(learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-2\u003c/span\u003e, decay=\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e),\n              metrics=[\u003cspan class=\"hljs-string\"\u003e\u0026#x27;mae\u0026#x27;\u003c/span\u003e])\n              \nhistory = model.fit(\n    x_train, y_train,\n    epochs=\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e,\n    batch_size=\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e,\n    validation_split=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e,\n    callbacks=[checkpoint]\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, we've got a simple MLP, with a bit of Dropout and Batch Normalization to battle overfitting, optimized with the \u003cem\u003eRMSprop\u003c/em\u003e optimizer and a \u003cem\u003eMean Absolute Error\u003c/em\u003e loss. We've fitted the model for 150 epochs, with a validation split of \u003ccode\u003e0.2\u003c/code\u003e, and a \u003ccode\u003eModelCheckpoint\u003c/code\u003e callback to save the weights in a file. Running this results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e...\nEpoch 150/150\n387/387 [==============================] - 3s 7ms/step - loss: 0.6279 - mae: 0.5976 - val_loss: 0.6346 - val_mae: 0.6042\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe could visualize the learning curves to gain some basic insight into how the training went, but it doesn't tell us the whole story - these are just aggregate means over the training and validation sets during training:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003emodel_history = pd.DataFrame(history.history)\nmodel_history[\u003cspan class=\"hljs-string\"\u003e\u0026#x27;epoch\u0026#x27;\u003c/span\u003e] = history.epoch\n\nfig, ax = plt.subplots(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e))\nnum_epochs = model_history.shape[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\nax.plot(np.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, num_epochs), model_history[\u003cspan class=\"hljs-string\"\u003e\u0026quot;mae\u0026quot;\u003c/span\u003e], \n        label=\u003cspan class=\"hljs-string\"\u003e\u0026quot;Training MAE\u0026quot;\u003c/span\u003e)\nax.plot(np.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, num_epochs), model_history[\u003cspan class=\"hljs-string\"\u003e\u0026quot;val_mae\u0026quot;\u003c/span\u003e], \n        label=\u003cspan class=\"hljs-string\"\u003e\u0026quot;Validation MAE\u0026quot;\u003c/span\u003e)\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://stackabuse.s3.amazonaws.com/media/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations-2.png\" alt=\"deep learning model learning curves\"\u003e\u003c/p\u003e\n\u003cp\u003eAnd we can evaluate our model with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003emodel.evaluate(x_test, y_test)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e162/162 [==============================] - 0s 2ms/step - loss: 0.5695 - mae: 0.5451 - mape: 32.2959\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs the target variable is measured in multiples of \u003cem\u003e$100.000\u003c/em\u003e, which means our network misses the price by up to about \u003cem\u003e$54.000\u003c/em\u003e, which is a \u003cem\u003eMean Absolute Percentage Error\u003c/em\u003e of ~32%. Most traditional Machine Learning methods such as Random Forest Regression, even after more extensive data pre-processing for this dataset achieve around \u003cem\u003e$52.000\u003c/em\u003e, with tuned hyperparameters - so this is actually a pretty decent result, although, it could be improved with more preprocessing, better tuning and different architectures.\u003c/p\u003e\n\u003cp\u003eThe point here wasn't to build a particularly accurate model, but we did choose a dataset using which the model wouldn't converge very quickly, so we can observe its dance around the target variables.\u003c/p\u003e\n\u003cp\u003eA more illustrative way to evaluate how the model's working ditches the aggregate \u003cem\u003eMean Absolute Error\u003c/em\u003e and \u003cem\u003eMean Absolute Percentage Error\u003c/em\u003e fully, and we can plot a \u003ca target=\"_blank\" href=\"https://stackabuse.com/matplotlib-scatterplot-tutorial-and-examples/\"\u003escatter plot\u003c/a\u003e of the \u003cem\u003epredicted prices\u003c/em\u003e against the \u003cem\u003eactual prices\u003c/em\u003e. If they're equal - the plotted markers will follow a straight trajectory diagonally. For reference and scope - we can also plot a diagonal line and evaluate how close each marker is to the line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003etest_predictions = model.predict(x_test)\ntest_labels = y_test\n\nfig, ax = plt.subplots(figsize=(\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\nplt.scatter(test_labels, test_predictions, alpha=\u003cspan class=\"hljs-number\"\u003e0.6\u003c/span\u003e, \n            color=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;#FF0000\u0026#x27;\u003c/span\u003e, lw=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, ec=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;black\u0026#x27;\u003c/span\u003e)\nlims = [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e]\n\nplt.plot(lims, lims, lw=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, color=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;#0000FF\u0026#x27;\u003c/span\u003e)\nplt.ticklabel_format(useOffset=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e, style=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;plain\u0026#x27;\u003c/span\u003e)\nplt.xticks(fontsize=\u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e)\nplt.yticks(fontsize=\u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e)\nplt.xlim(lims)\nplt.ylim(lims)\n\nplt.tight_layout()\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRunning this code results in:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://stackabuse.s3.amazonaws.com/media/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations-3.png\" alt=\"deep learning regression performance evaluation\"\u003e\u003c/p\u003e\n\u003cp\u003eThe network overprices cheaper houses and underprices more expensive ones - and the estimates have a pretty generous scope (with some predictions on the right being totally out of scope - though, this happens because we haven't cleaned the dataset and many house prices \u003cem\u003eare\u003c/em\u003e capped at that value when imported).\u003c/p\u003e\n\u003cp\u003eThis isn't the insight you get from the learning curves, and a network that had the opposite effect - underpricing cheaper houses and overpricing expensive ones might have the same MAE and MAPE but behave totally differently.\u003c/p\u003e\n\u003cp\u003eWhat we're also interested in is \u003cem\u003ehow\u003c/em\u003e the model got here and how these predictions changed through time and the learning process. This is just the end point of the training process, and there was a fair bit of training involved to get here.\u003c/p\u003e\n\u003cp\u003eLet's go ahead and write a \u003cem\u003ecustom callback\u003c/em\u003e to add to the list of callbacks in the training process, that will run a prediction on the test set on each epoch, visualize the predictions and save them as an image.\u003c/p\u003e\n\u003ch3 id=\"custompredictionkerascallbackwithplots\"\u003eCustom Prediction Keras Callback with Plots\u003c/h3\u003e\n\u003cp\u003eJust like we've used the \u003ccode\u003eModelCheckpoint\u003c/code\u003e callback to check whether a model is in its best-performing state on each epoch, and save it into a \u003ccode\u003e.h5\u003c/code\u003e file and persist it - we can write a \u003cem\u003ecustom callback\u003c/em\u003e that'll run predictions, visualize them, and save the images on our disk.\u003c/p\u003e\n\u003cp\u003eCreating a custom callback boils down to extending the \u003ccode\u003eCallback\u003c/code\u003e class and overriding any of the methods it provides - the ones you \u003cem\u003edon't\u003c/em\u003e override, retain their default behavior:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-class\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003ePerformancePlotCallback\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ekeras.callbacks.Callback\u003c/span\u003e):\u003c/span\u003e\n       \n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003eon_train_end\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, logs=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n      ...\n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003eon_epoch_begin\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, epoch, logs=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n      ...\n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003eon_epoch_end\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, epoch, logs=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n      ...\n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003eon_test_begin\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, logs=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n      ...\n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003eon_test_end\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, logs=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n      ...\n    \u003cspan class=\"hljs-comment\"\u003e# Etc.\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDepending on \u003cem\u003ewhen\u003c/em\u003e you'd like to predict using your in-the-training model, you'll choose the appropriate method. A good measure of how it's progressing is an \u003cem\u003eepoch\u003c/em\u003e, so on the end of each training epoch, we'll test the model on our test set.\u003c/p\u003e\n\u003cp\u003eWe need a way to provide the test set to the callback, since this is external data. The easiest way to do that is to define a \u003cem\u003econstructor\u003c/em\u003e that accepts the test set and evaluates the \u003cem\u003ecurrent model\u003c/em\u003e on it, giving you a consistent result:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-class\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003ePerformancePlotCallback\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ekeras.callbacks.Callback\u003c/span\u003e):\u003c/span\u003e\n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, x_test, y_test\u003c/span\u003e):\u003c/span\u003e\n        self.x_test = x_test\n        self.y_test = y_test\n        \n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003eon_epoch_end\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, epoch, logs=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Evaluating Model...\u0026#x27;\u003c/span\u003e)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Model Evaluation: \u0026#x27;\u003c/span\u003e, self.model.evaluate(self.x_test))   \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis simple callback accepts the test set of houses and relevant target variables and evaluates itself on each epoch, printing the result to the console, right alongside the usual Keras output.\u003c/p\u003e\n\u003cp\u003eIf we were to instantiate and add this callback to the model, and \u003ccode\u003efit()\u003c/code\u003e it again, we'd see a different result from before:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eperformance_simple = PerformancePlotCallback(x_test, y_test)\n\n\u003cspan class=\"hljs-comment\"\u003e# Model definition and compilation...\u003c/span\u003e\n\nhistory = model.fit(\n    x_train, y_train,\n    epochs=\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e,\n    validation_split=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e,\n    callbacks=[performance_simple]\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch 1/150\n387/387 [==============================] - 3s 7ms/step - loss: 1.0785 - mae: 1.0140 - val_loss: 0.9455 - val_mae: 0.8927\nEvaluating Model...\n162/162 [==============================] - 0s 1ms/step - loss: 0.0528 - mae: 0.0000e+00\nModel Evaluation:  [0.05277165770530701, 0.0]\nEpoch 2/150\n387/387 [==============================] - 3s 7ms/step - loss: 0.9048 - mae: 0.8553 - val_loss: 0.8547 - val_mae: 0.8077\nEvaluating Model...\n162/162 [==============================] - 0s 1ms/step - loss: 0.0471 - mae: 0.0000e+00\nModel Evaluation:  [0.04705655574798584, 0.0]\n...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAwesome! The model is evaluating itself on each epoch, on the data we've passed into the callback. Now, let's modify the callback so it visualizes the predictions instead of printing them to the already cluttered output.\u003c/p\u003e\n\u003cp\u003eTo simplify things, we'll get the callback to save the images to a folder, so that we can stitch them together into a video or a Gif later on. We'll also include a \u003ccode\u003emodel_name\u003c/code\u003e in the constructor to help us differentiate models when generating the images and their filenames:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-class\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003ePerformancePlotCallback\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ekeras.callbacks.Callback\u003c/span\u003e):\u003c/span\u003e\n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, x_test, y_test, model_name\u003c/span\u003e):\u003c/span\u003e\n        self.x_test = x_test\n        self.y_test = y_test\n        self.model_name = model_name\n        \n    \u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003eon_epoch_end\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, epoch, logs={}\u003c/span\u003e):\u003c/span\u003e\n        y_pred = self.model.predict(self.x_test)\n        fig, ax = plt.subplots(figsize=(\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n        plt.scatter(y_test, y_pred, alpha=\u003cspan class=\"hljs-number\"\u003e0.6\u003c/span\u003e, \n            color=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;#FF0000\u0026#x27;\u003c/span\u003e, lw=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, ec=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;black\u0026#x27;\u003c/span\u003e)\n        \n        lims = [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e]\n\n        plt.plot(lims, lims, lw=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, color=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;#0000FF\u0026#x27;\u003c/span\u003e)\n        plt.ticklabel_format(useOffset=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e, style=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;plain\u0026#x27;\u003c/span\u003e)\n        plt.xticks(fontsize=\u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e)\n        plt.yticks(fontsize=\u003cspan class=\"hljs-number\"\u003e18\u003c/span\u003e)\n        plt.xlim(lims)\n        plt.ylim(lims)\n\n        plt.tight_layout()\n        plt.title(\u003cspan class=\"hljs-string\"\u003ef\u0026#x27;Prediction Visualization Keras Callback - Epoch: \u003cspan class=\"hljs-subst\"\u003e{epoch}\u003c/span\u003e\u0026#x27;\u003c/span\u003e)\n        plt.savefig(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;model_train_images/\u0026#x27;\u003c/span\u003e+self.model_name+\u003cspan class=\"hljs-string\"\u003e\u0026quot;_\u0026quot;\u003c/span\u003e+\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(epoch))\n        plt.close()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, we create a Matplotlib figure on each epoch, and plot a \u003ca target=\"_blank\" href=\"https://stackabuse.com/matplotlib-scatterplot-tutorial-and-examples/\"\u003escatter plot\u003c/a\u003e of the predicted prices against the actual prices. Additionally, we've added a diagonal reference line - the closer our scatter plot markers are to the diagonal line, the more accurate our model's predictions were.\u003c/p\u003e\n\u003cp\u003eThe plot is then saved via \u003ca target=\"_blank\" href=\"https://stackabuse.com/save-plot-as-image-with-matplotlib/\"\u003e\u003ccode\u003eplt.savefig()\u003c/code\u003e\u003c/a\u003e with the model's name and the epoch number, alongside an informative title that lets you know which epoch the model is in during training.\u003c/p\u003e\n\u003cp\u003eNow, let's use this custom callback again, providing a model name in addition to the \u003ccode\u003ex_test\u003c/code\u003e and \u003ccode\u003ey_test\u003c/code\u003e sets:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003echeckpoint = keras.callbacks.ModelCheckpoint(\u003cspan class=\"hljs-string\"\u003e\u0026quot;california.h5\u0026quot;\u003c/span\u003e, save_best_only=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nperformance = PerformancePlotCallback(x_test, y_test, \u003cspan class=\"hljs-string\"\u003e\u0026quot;california_model\u0026quot;\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# Model definition and compilation...\u003c/span\u003e\n              \nhistory = model.fit(\n    x_train, y_train,\n    epochs=\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e,\n    validation_split=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e,\n    callbacks=[checkpoint, performance]\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003ePerformancePlotCallback\u003c/code\u003e goes into full swing, and in the designated folder generates an image of the performance on each epoch. The \u003ccode\u003emodel_train_images\u003c/code\u003e folder is now filled with 150 plots:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://stackabuse.s3.amazonaws.com/media/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations-4.png\" alt=\"saving predictions on each epoch keras\"\u003e\u003c/p\u003e\n\u003cp\u003eYou can now use your favorite tool to stitch the images together into a video or a Gif file, or simply peruse them manually. Here's a Gif of the model we've built training on this data:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://stackabuse.s3.amazonaws.com/media/custom-keras-callback-for-saving-prediction-on-each-epoch-with-visualizations-5.gif\" alt=\"animated learning process on each epoch with keras\"\u003e\u003c/p\u003e\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eIn this guide, we've built a simple model to predict the price of a house in the California Housing Dataset with okay-ish accuracy. We've then taken a look at how to write a custom Keras callback to test a Deep Learning model's performance and visualize it during training, on each epoch.\u003c/p\u003e\n\u003cp\u003eWe've proceeded to save these images to the disk and created a  Gif from them, giving us a different perspective on the training process than the one we get from analyzing the learning curves of a model.\u003c/p\u003e\n","parent_id":null,"type":"article","status":"published","visibility":"public","img_feature":null,"is_featured":false,"locale":"en","custom_excerpt":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"comment_id":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In this guide, learn how to save the prediction of your deep learning models during training, on each epoch, with a custom Keras Callback in Python, and visualize/animate the predictions.","read_time_min":null,"published_by":null,"published_at":1637148600000,"created_by":16,"updated_by":null,"created_at":1637080692582,"updated_at":1637694837743,"contributors":[{"id":16,"name":"David Landup","slug":"david","email":"thealduinmaster@gmail.com","password_hash":"$2a$10$W/oMJdUBSTeG3trWAHa1xO0pQruxuLgD/6hS7VuxPafcmAxeBXmVi","role_id":2,"img_profile":"//s3.stackabuse.com/media/users/865cd7d217ea11c9d9555c4f666e2d73.jpg","img_cover":null,"bio_md":"Entrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs. \n\nGreat passion for accessible education and promotion of reason, science, humanism, and progress.","bio_html":"\u003cp\u003eEntrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs.\u003c/p\u003e\n\u003cp\u003eGreat passion for accessible education and promotion of reason, science, humanism, and progress.\u003c/p\u003e\n","website":"https://www.upwork.com/freelancers/~017664e499a2766871","location":"Serbia","facebook":"","twitter":"","github":null,"status":"active","locale":null,"last_seen_at":1622229427000,"created_by":null,"updated_by":null,"created_at":1534532687000,"updated_at":1640861394795,"role":"editor","secret_token":"2a2be92558fae38f89cfbd0c6a4ba90c","is_email_confirmed":false,"_pivot_content_id":1143,"_pivot_user_id":16,"_pivot_role":"author","_pivot_sort_order":0}],"tags":[{"id":9,"name":"python","slug":"python","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1431358631000,"updated_at":1431358631000,"_pivot_content_id":1143,"_pivot_tag_id":9,"_pivot_sort_order":0},{"id":57,"name":"artificial intelligence","slug":"artificial-intelligence","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1442517465000,"updated_at":1442517465000,"_pivot_content_id":1143,"_pivot_tag_id":57,"_pivot_sort_order":5},{"id":75,"name":"machine learning","slug":"machine-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507305534000,"updated_at":1507305534000,"_pivot_content_id":1143,"_pivot_tag_id":75,"_pivot_sort_order":3},{"id":78,"name":"tensorflow","slug":"tensorflow","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1508009047000,"updated_at":1508009047000,"_pivot_content_id":1143,"_pivot_tag_id":78,"_pivot_sort_order":2},{"id":106,"name":"data science","slug":"data-science","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1536865458000,"updated_at":1536865458000,"_pivot_content_id":1143,"_pivot_tag_id":106,"_pivot_sort_order":4},{"id":131,"name":"keras","slug":"keras","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1558780443000,"updated_at":1558780443000,"_pivot_content_id":1143,"_pivot_tag_id":131,"_pivot_sort_order":1}],"_pivot_tag_id":75,"_pivot_content_id":1143},{"id":1018,"old_id":null,"uuid":"97c88b47-2bad-48f4-adcd-8d1a2e8c4d7e","title":"Scikit-Learn's train_test_split() - Training, Testing and Validation Sets","slug":"scikit-learns-traintestsplit-training-testing-and-validation-sets","body_md":"### Introduction\n\n**_Scikit-Learn_** is one of the most widely-used Machine Learning library in Python. It's optimized and efficient - and its high-level API is simple and easy to use.\n\nScikit-Learn has a plethora of convenience tools and methods that make preprocessing, evaluating and other painstaking processes as easy as calling a single method - and splitting data between a **_training_** and **_testing_** set is no exception.\n\nGenerally speaking, the rule-of-thumb for splitting data is 80/20 - where 80% of the data is used for *training* a model, while 20% is used for testing it. This depends on the dataset you're working with, but an 80/20 split is very common and would get you through most datasets just fine.\n\nIn this guide - we'll take a look at how to use the `split_train_test()` method in Scikit-Learn, and how to configure the parameters so that you have control over the splitting process.\n\n### Installing Scikit-Learn\n\nAssuming it isn't already installed - Scikit-Learn can easily be installed via `pip`:\n\n```console\n$ pip install scikit-learn\n```\n\nOnce installed, you can import the library itself via:\n\n```python\nimport sklearn\n```\n\nIn most cases, people avoid importing the entire library, as it's pretty vast, and import specific classes or modules that they'll be using specifically.\n\n::: note\n**Note:** This tends to mean that people have a hefty import list when using Scikit-Learn.\n:::\n\n### Importance of Training and Testing Sets\n\nThe most common procedure when training a (basic) model in Machine Learning follows the same rough outline:\n\n- Acquiring and processing data which we'll feed into a model. \n\nScikit-learn has various datasets to be loaded and used for training the model (*iris, diabetes, digits...*), mainly for benchmarking/learning.\n\n- Splitting sets into training and test sets\n- Building a model and defining the architecture\n- Compiling the model\n- Training the model\n- Verifying the results\n\nThe **training set** is a subset of the whole dataset and we generally don't train a model on the *entirety* of    the data. In non-generative models, a training set usually contains around 80% of the main dataset's data. As the name implies, it is used for training the model. This procedure is also referred to as *fitting the model*.\n\nThere are exceptions to this rule, though. \n\nFor instance, when training _Generative Adversarial Networks (GANs)_ that generate images - how do you *test* the results? They're highly subjective in some cases, as they represent *new* instances that were never seen before. In *most* generative models, at least as of now, a human is typically required to judge the outputs, in which cases, a *test* set is totally redundant.\n\nAdditionally, sometimes you need more or less than 20% for testing, and if you're using techniques such as *cross-validation*, you might want to have a tiny bit less testing data as to not \"take away\" too much from the training data. For instance, if you have 1.000.000 instances in a dataset, holding out just 5% for a testing set amounts to 50.000 instances, which is most likely *more than enough* for any model to be tested on.\n\nThe **test set** is a subset of the whole dataset, and is used to evaluate the model and check how well it learnt from the training set. \n\n\u003e The model mustn't interact or see the test set before evaluating. The data must be unknown when first evaluating, otherwise it's not **really** testing the model.\n\n### What About Validation Sets?\n\nValidation sets are a common sight in professional and academic models. Validation sets are taken out of the training set, and used *during training* to validate the model's accuracy *approximately*.\n\nThe testing set is fully disconnected until the model is finished training - but the validation set is used to validate it *during* training.\n\n::: note\n**Note:** The validation set isn't used *for* training, and the model doesn't train on the data. It just validates the current epoch. This way - it *indirectly* trains on the data, as it does affect its prior beliefs, so the validation set can't be used for testing.\n:::\n\nSimilar to how you'll learn more about your own knowledge if you hear it's incorrect - even if you don't know why. This is why validation sets *approximate* a models accuracy, and testing sets are still required even when using a validation set.\n\nThey help with approximating a model's actual performance during training, so you don't end up with an illusory overfit model without realizing it after testing it via a test set. You can also use validation sets to tune models, and approximately evaluate their ability *without* exposing them to a testing set.\n\nDeep Learning frameworks such as Keras can display a `val_accuracy` besides your regular training `accuracy` as a good sign of overfitting. If they start diverging, your model is overfitting during training, and you don't need to waste time training it further when they diverge enough. Additionally, callbacks such as `EarlyStopping` can be used to automatically stop a model's training if the `val_accuracy` doesn't improve after `n` epochs, even if the `accuracy` is increasing.\n\n\u003e Creating a validation set is easy.\n\nYou can, quite literally, just run the `train_test_split()` method on the *training set*, which was already split by the `train_test_split()` method and extract a validation set from it. Alternatively, you can manually create a validation set.\n\nThe validation set size is typically split similar to a testing set - anywhere between 10-20% of the training set is typical. For huge datasets, you can do much lower than this, but for small datasets, you can take out too much, making it hard for the model to fit the data in the training set.\n\n\u003e In the proceeding sections, we'll also take out a **validation set** using the same `train_test_split()` method.\n\n### Scikit-Learn's datasets Module\n\nSeveral clean and popular datasets are available built-into Scikit-Learn, typically used during leearning and for benchmarking models on simple tasks. \n\nIf you've ever read resources regarding Machine Learning in Python - you've probably seen some of these most popular datasets:\n\n- **_Iris_** - set of 3 classes (flowers), with 50 samples per class, *used for classification*.\n- **_Diabetes_** - set with a total of 442 samples, *used for regression*.\n- **_Digits_** - set of 10 classes (hand-written digits), with ~180 samples per class, *used for classification*.\n- **_Wine_** - set of 3 classes (of wine), with total of 178 samples, *used for classification*.\n\nEach of these datasets can be loaded in through the `datasets` module with their respective function:\n\n```python\nfrom sklearn import datasets\n\nX_iris, y_iris = datasets.load_iris(return_X_y=True)\nX_diabetes, y_diabetes = datasets.load_diabetes(return_X_y=True)\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\nX_wine, y_wine = datasets.load_wine(return_X_y=True)\n```\n\nAlternatively, you can load in the specific functions instead:\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.datasets import load_digits\nfrom sklearn.datasets import load_wine\n\nX_iris, y_iris = load_iris(return_X_y=True)\nX_diabetes, y_diabetes = load_diabetes(return_X_y=True)\nX_digits, y_digits = load_digits(return_X_y=True)\nX_wine, y_wine = load_wine(return_X_y=True)\n```\n\nBy default, these methods return a `Bunch` object, containing the data and the targets (data and their clases), however, if you set the `return_X_y` argument to `True`, a tuple of `(data, targets)` is returned, denoting the data you'll be training on and the target classes you want your classifier/regression model to hit.\n\n### Splitting a Dataset with _train_test_split()_\n\nThe `train_test_split()` method resides in the `sklearn.model_selection` module:\n\n```python\nfrom sklearn.model_selection import train_test_split\n```\n\nThere are a couple of arguments we can set while working with this method - and the default is very sensible and performs an 75/25 split. In practice, *all* of Scikit-Learn's default values are fairly reasonable and set to serve well for *most* tasks. However, it's worth noting what these defaults are, in the cases they don't work that well.\n\nThe main two arguments are `train_size` and `test_size`, where their values range between `0` and `1` and their sum has to be `1`. Their values denote the percentage proportion of the dataset, so even if you define just one, such as `train_size`, the `test_size` is equal to `1 - train_size`, and vice versa.\n\n#### Setting the *train_size* and *test_size* Arguments\n\nThis is the most common approach, which, leaves us with 4 subsets - `X_train`, `X_test`, `y_train` and `y_test`:\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n```\n\nWithout setting either `train_size` or `test_size` the default values kick in, setting the `test_size` to `0.25`, and a complementary (`0.75`) `train_size`:\n\n```plaintext\n(112, 4)\n(38, 4)\n(112,)\n(38,)\n```\n\nAs you can see, train and test sets are split 75%/25%, as there are 112 instances in the `X_train` set, and 38 instances in the `X_test` set.\n\nSome other split proportions are: 80%/20% (very common), 67%/33% and more rarely 50%/50%.\n\nSetting any of these boils down to defining either one or both of the arguments in the `train_test_split()` method:\n\n```python\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n```\n\nAll three of these splits would result in the same split of:\n\n```plaintext\n(120, 4)\n(30, 4)\n(120,)\n(30,)\n```\n\n#### Creating a Validation Set with _train_test_split()_\n\nValidation sets are really useful during training and make your life as a Data Scientist significantly easier. \n\n\u003e Whenever possible, try to use a validation set.\n\nThere is no built-in function to extract a validation set from a training set, however, since this boils down to just splitting it like before - why not use the same `train_test_split()` method?\n\nLet's re-use it to get our hands on a validation set, taking 10% of the data from the training set:\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.9)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_valid.shape)\n```\n\nThis won't create a 70%-20%-10% split, as we're splitting 10% from the *already split* `X_train` so we're actually ending up with a 72%-20%-8% split here:\n\n```plaintext\n(108, 4)\n(30, 4)\n(12, 4)\n```\n\nTo account for this, you can either *manually* set a different number, expecting this, *or* you could define your proportions upfront, and calculate an updated split to reference the *original* size, instead of the already truncated size.\n\nTo split the data proportionally into a training, testing and validation set - we need to set the `test_size` argument on the second function call to:\n\n$$\ntest_s = validation_r/(train_r+test_r)\n$$\n\nLet's load in the Diabetes dataset, as it has more instances (due to rounding, small datasets ofetntimes produce slightly different splits even with same ratios):\n\n```python\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_diabetes(return_X_y=True)\n\nprint(X.shape)\n```\n\n```plaintext\n(442, 10)\n```\n\nSay we're aiming for an 80%/10%/10% split - we'd want to have `352`, `45` and `45` instances respectively. Let's define these rations and split the dataset into a training, testing and validation set with the `train_test_split()` function:\n\n```python\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\ntrain_ratio = 0.80\ntest_ratio = 0.10\nvalidation_ratio = 0.10\n\nX, y = load_diabetes(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=validation_ratio/(train_ratio+test_ratio))\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_valid.shape)\n```\n\nThis results in:\n\n```plaintext\n(352, 10)\n(45, 10)\n(45, 10)\n```\n\nAwesome! Our dataset has successfully been split into three sets, which we can now feed into a model and perform validation during training to tune the hyperparameters.\n\n#### Stratified Split\n\nSometimes, there's a different numbers of samples for each class in a dataset. Say, one class has 100 samples, the second one has 50, the third one 30, etc. Splitting without this in mind creates an issue when you're training a classification model (though, regression models don't suffer from this).\n\n\u003e It is best to somehow split the set, so that it preserves the proportions of the classes. This is **a stratified split**.\n\nLuckily, the method `train_test_split` has an argument called `stratify` which takes an array which defines the number of samples by class, when splitting, to stay proportional:\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n```\n\nIn a lot of cases, you can simply use the `y` NumPy array from your dataset for a good `stratify` split array. This ensures that your model can fight the lack of balance between instances of classes and becomes less biased towards some.\n\n### Conclusion\n\nIn this guide, we got familiar with some of the Scikit-Learn library and its `datasets` module. You've learned what training, testing and validation sets are, where they're applied and the benefits of validating your models.\n\nWe've taken a look at how to employ the `train_test_split()` method to split your data into a training and testing set, as well as how to separate a validation set, dynamically perserving the ratios of these sets.","body_html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eScikit-Learn\u003c/em\u003e\u003c/strong\u003e is one of the most widely-used Machine Learning library in Python. It's optimized and efficient - and its high-level API is simple and easy to use.\u003c/p\u003e\n\u003cp\u003eScikit-Learn has a plethora of convenience tools and methods that make preprocessing, evaluating and other painstaking processes as easy as calling a single method - and splitting data between a \u003cstrong\u003e\u003cem\u003etraining\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003etesting\u003c/em\u003e\u003c/strong\u003e set is no exception.\u003c/p\u003e\n\u003cp\u003eGenerally speaking, the rule-of-thumb for splitting data is 80/20 - where 80% of the data is used for \u003cem\u003etraining\u003c/em\u003e a model, while 20% is used for testing it. This depends on the dataset you're working with, but an 80/20 split is very common and would get you through most datasets just fine.\u003c/p\u003e\n\u003cp\u003eIn this guide - we'll take a look at how to use the \u003ccode\u003esplit_train_test()\u003c/code\u003e method in Scikit-Learn, and how to configure the parameters so that you have control over the splitting process.\u003c/p\u003e\n\u003ch3 id=\"installingscikitlearn\"\u003eInstalling Scikit-Learn\u003c/h3\u003e\n\u003cp\u003eAssuming it isn't already installed - Scikit-Learn can easily be installed via \u003ccode\u003epip\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-meta\"\u003e$\u003c/span\u003e\u003cspan class=\"bash\"\u003e pip install scikit-learn\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce installed, you can import the library itself via:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e sklearn\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn most cases, people avoid importing the entire library, as it's pretty vast, and import specific classes or modules that they'll be using specifically.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e This tends to mean that people have a hefty import list when using Scikit-Learn.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003ch3 id=\"importanceoftrainingandtestingsets\"\u003eImportance of Training and Testing Sets\u003c/h3\u003e\n\u003cp\u003eThe most common procedure when training a (basic) model in Machine Learning follows the same rough outline:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAcquiring and processing data which we'll feed into a model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eScikit-learn has various datasets to be loaded and used for training the model (\u003cem\u003eiris, diabetes, digits...\u003c/em\u003e), mainly for benchmarking/learning.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSplitting sets into training and test sets\u003c/li\u003e\n\u003cli\u003eBuilding a model and defining the architecture\u003c/li\u003e\n\u003cli\u003eCompiling the model\u003c/li\u003e\n\u003cli\u003eTraining the model\u003c/li\u003e\n\u003cli\u003eVerifying the results\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cstrong\u003etraining set\u003c/strong\u003e is a subset of the whole dataset and we generally don't train a model on the \u003cem\u003eentirety\u003c/em\u003e of    the data. In non-generative models, a training set usually contains around 80% of the main dataset's data. As the name implies, it is used for training the model. This procedure is also referred to as \u003cem\u003efitting the model\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThere are exceptions to this rule, though.\u003c/p\u003e\n\u003cp\u003eFor instance, when training \u003cem\u003eGenerative Adversarial Networks (GANs)\u003c/em\u003e that generate images - how do you \u003cem\u003etest\u003c/em\u003e the results? They're highly subjective in some cases, as they represent \u003cem\u003enew\u003c/em\u003e instances that were never seen before. In \u003cem\u003emost\u003c/em\u003e generative models, at least as of now, a human is typically required to judge the outputs, in which cases, a \u003cem\u003etest\u003c/em\u003e set is totally redundant.\u003c/p\u003e\n\u003cp\u003eAdditionally, sometimes you need more or less than 20% for testing, and if you're using techniques such as \u003cem\u003ecross-validation\u003c/em\u003e, you might want to have a tiny bit less testing data as to not \u0026quot;take away\u0026quot; too much from the training data. For instance, if you have 1.000.000 instances in a dataset, holding out just 5% for a testing set amounts to 50.000 instances, which is most likely \u003cem\u003emore than enough\u003c/em\u003e for any model to be tested on.\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003etest set\u003c/strong\u003e is a subset of the whole dataset, and is used to evaluate the model and check how well it learnt from the training set.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe model mustn't interact or see the test set before evaluating. The data must be unknown when first evaluating, otherwise it's not \u003cstrong\u003ereally\u003c/strong\u003e testing the model.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"whataboutvalidationsets\"\u003eWhat About Validation Sets?\u003c/h3\u003e\n\u003cp\u003eValidation sets are a common sight in professional and academic models. Validation sets are taken out of the training set, and used \u003cem\u003eduring training\u003c/em\u003e to validate the model's accuracy \u003cem\u003eapproximately\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThe testing set is fully disconnected until the model is finished training - but the validation set is used to validate it \u003cem\u003eduring\u003c/em\u003e training.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e The validation set isn't used \u003cem\u003efor\u003c/em\u003e training, and the model doesn't train on the data. It just validates the current epoch. This way - it \u003cem\u003eindirectly\u003c/em\u003e trains on the data, as it does affect its prior beliefs, so the validation set can't be used for testing.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eSimilar to how you'll learn more about your own knowledge if you hear it's incorrect - even if you don't know why. This is why validation sets \u003cem\u003eapproximate\u003c/em\u003e a models accuracy, and testing sets are still required even when using a validation set.\u003c/p\u003e\n\u003cp\u003eThey help with approximating a model's actual performance during training, so you don't end up with an illusory overfit model without realizing it after testing it via a test set. You can also use validation sets to tune models, and approximately evaluate their ability \u003cem\u003ewithout\u003c/em\u003e exposing them to a testing set.\u003c/p\u003e\n\u003cp\u003eDeep Learning frameworks such as Keras can display a \u003ccode\u003eval_accuracy\u003c/code\u003e besides your regular training \u003ccode\u003eaccuracy\u003c/code\u003e as a good sign of overfitting. If they start diverging, your model is overfitting during training, and you don't need to waste time training it further when they diverge enough. Additionally, callbacks such as \u003ccode\u003eEarlyStopping\u003c/code\u003e can be used to automatically stop a model's training if the \u003ccode\u003eval_accuracy\u003c/code\u003e doesn't improve after \u003ccode\u003en\u003c/code\u003e epochs, even if the \u003ccode\u003eaccuracy\u003c/code\u003e is increasing.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCreating a validation set is easy.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eYou can, quite literally, just run the \u003ccode\u003etrain_test_split()\u003c/code\u003e method on the \u003cem\u003etraining set\u003c/em\u003e, which was already split by the \u003ccode\u003etrain_test_split()\u003c/code\u003e method and extract a validation set from it. Alternatively, you can manually create a validation set.\u003c/p\u003e\n\u003cp\u003eThe validation set size is typically split similar to a testing set - anywhere between 10-20% of the training set is typical. For huge datasets, you can do much lower than this, but for small datasets, you can take out too much, making it hard for the model to fit the data in the training set.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn the proceeding sections, we'll also take out a \u003cstrong\u003evalidation set\u003c/strong\u003e using the same \u003ccode\u003etrain_test_split()\u003c/code\u003e method.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"scikitlearnsdatasetsmodule\"\u003eScikit-Learn's datasets Module\u003c/h3\u003e\n\u003cp\u003eSeveral clean and popular datasets are available built-into Scikit-Learn, typically used during leearning and for benchmarking models on simple tasks.\u003c/p\u003e\n\u003cp\u003eIf you've ever read resources regarding Machine Learning in Python - you've probably seen some of these most popular datasets:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eIris\u003c/em\u003e\u003c/strong\u003e - set of 3 classes (flowers), with 50 samples per class, \u003cem\u003eused for classification\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eDiabetes\u003c/em\u003e\u003c/strong\u003e - set with a total of 442 samples, \u003cem\u003eused for regression\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eDigits\u003c/em\u003e\u003c/strong\u003e - set of 10 classes (hand-written digits), with ~180 samples per class, \u003cem\u003eused for classification\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eWine\u003c/em\u003e\u003c/strong\u003e - set of 3 classes (of wine), with total of 178 samples, \u003cem\u003eused for classification\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these datasets can be loaded in through the \u003ccode\u003edatasets\u003c/code\u003e module with their respective function:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e datasets\n\nX_iris, y_iris = datasets.load_iris(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nX_diabetes, y_diabetes = datasets.load_diabetes(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nX_digits, y_digits = datasets.load_digits(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nX_wine, y_wine = datasets.load_wine(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlternatively, you can load in the specific functions instead:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_iris\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_diabetes\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_digits\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_wine\n\nX_iris, y_iris = load_iris(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nX_diabetes, y_diabetes = load_diabetes(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nX_digits, y_digits = load_digits(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nX_wine, y_wine = load_wine(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBy default, these methods return a \u003ccode\u003eBunch\u003c/code\u003e object, containing the data and the targets (data and their clases), however, if you set the \u003ccode\u003ereturn_X_y\u003c/code\u003e argument to \u003ccode\u003eTrue\u003c/code\u003e, a tuple of \u003ccode\u003e(data, targets)\u003c/code\u003e is returned, denoting the data you'll be training on and the target classes you want your classifier/regression model to hit.\u003c/p\u003e\n\u003ch3 id=\"splittingadatasetwithtrain_test_split\"\u003eSplitting a Dataset with \u003cem\u003etrain_test_split()\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003etrain_test_split()\u003c/code\u003e method resides in the \u003ccode\u003esklearn.model_selection\u003c/code\u003e module:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.model_selection \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e train_test_split\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThere are a couple of arguments we can set while working with this method - and the default is very sensible and performs an 75/25 split. In practice, \u003cem\u003eall\u003c/em\u003e of Scikit-Learn's default values are fairly reasonable and set to serve well for \u003cem\u003emost\u003c/em\u003e tasks. However, it's worth noting what these defaults are, in the cases they don't work that well.\u003c/p\u003e\n\u003cp\u003eThe main two arguments are \u003ccode\u003etrain_size\u003c/code\u003e and \u003ccode\u003etest_size\u003c/code\u003e, where their values range between \u003ccode\u003e0\u003c/code\u003e and \u003ccode\u003e1\u003c/code\u003e and their sum has to be \u003ccode\u003e1\u003c/code\u003e. Their values denote the percentage proportion of the dataset, so even if you define just one, such as \u003ccode\u003etrain_size\u003c/code\u003e, the \u003ccode\u003etest_size\u003c/code\u003e is equal to \u003ccode\u003e1 - train_size\u003c/code\u003e, and vice versa.\u003c/p\u003e\n\u003ch4 id=\"settingthetrain_sizeandtest_sizearguments\"\u003eSetting the \u003cem\u003etrain_size\u003c/em\u003e and \u003cem\u003etest_size\u003c/em\u003e Arguments\u003c/h4\u003e\n\u003cp\u003eThis is the most common approach, which, leaves us with 4 subsets - \u003ccode\u003eX_train\u003c/code\u003e, \u003ccode\u003eX_test\u003c/code\u003e, \u003ccode\u003ey_train\u003c/code\u003e and \u003ccode\u003ey_test\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_iris\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.model_selection \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e train_test_split\n\nX, y = load_iris(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(X_train.shape)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(X_test.shape)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(y_train.shape)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(y_test.shape)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWithout setting either \u003ccode\u003etrain_size\u003c/code\u003e or \u003ccode\u003etest_size\u003c/code\u003e the default values kick in, setting the \u003ccode\u003etest_size\u003c/code\u003e to \u003ccode\u003e0.25\u003c/code\u003e, and a complementary (\u003ccode\u003e0.75\u003c/code\u003e) \u003ccode\u003etrain_size\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e(112, 4)\n(38, 4)\n(112,)\n(38,)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs you can see, train and test sets are split 75%/25%, as there are 112 instances in the \u003ccode\u003eX_train\u003c/code\u003e set, and 38 instances in the \u003ccode\u003eX_test\u003c/code\u003e set.\u003c/p\u003e\n\u003cp\u003eSome other split proportions are: 80%/20% (very common), 67%/33% and more rarely 50%/50%.\u003c/p\u003e\n\u003cp\u003eSetting any of these boils down to defining either one or both of the arguments in the \u003ccode\u003etrain_test_split()\u003c/code\u003e method:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=\u003cspan class=\"hljs-number\"\u003e0.8\u003c/span\u003e)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=\u003cspan class=\"hljs-number\"\u003e0.8\u003c/span\u003e, test_size=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll three of these splits would result in the same split of:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e(120, 4)\n(30, 4)\n(120,)\n(30,)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"creatingavalidationsetwithtrain_test_split\"\u003eCreating a Validation Set with \u003cem\u003etrain_test_split()\u003c/em\u003e\u003c/h4\u003e\n\u003cp\u003eValidation sets are really useful during training and make your life as a Data Scientist significantly easier.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWhenever possible, try to use a validation set.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThere is no built-in function to extract a validation set from a training set, however, since this boils down to just splitting it like before - why not use the same \u003ccode\u003etrain_test_split()\u003c/code\u003e method?\u003c/p\u003e\n\u003cp\u003eLet's re-use it to get our hands on a validation set, taking 10% of the data from the training set:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_iris\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.model_selection \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e train_test_split\n\nX, y = load_iris(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=\u003cspan class=\"hljs-number\"\u003e0.8\u003c/span\u003e)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=\u003cspan class=\"hljs-number\"\u003e0.9\u003c/span\u003e)\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(X_train.shape)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(X_test.shape)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(X_valid.shape)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis won't create a 70%-20%-10% split, as we're splitting 10% from the \u003cem\u003ealready split\u003c/em\u003e \u003ccode\u003eX_train\u003c/code\u003e so we're actually ending up with a 72%-20%-8% split here:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e(108, 4)\n(30, 4)\n(12, 4)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo account for this, you can either \u003cem\u003emanually\u003c/em\u003e set a different number, expecting this, \u003cem\u003eor\u003c/em\u003e you could define your proportions upfront, and calculate an updated split to reference the \u003cem\u003eoriginal\u003c/em\u003e size, instead of the already truncated size.\u003c/p\u003e\n\u003cp\u003eTo split the data proportionally into a training, testing and validation set - we need to set the \u003ccode\u003etest_size\u003c/code\u003e argument on the second function call to:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\ntest_s = validation_r/(train_r+test_r)\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cp\u003eLet's load in the Diabetes dataset, as it has more instances (due to rounding, small datasets ofetntimes produce slightly different splits even with same ratios):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_diabetes\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.model_selection \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e train_test_split\n\nX, y = load_diabetes(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(X.shape)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e(442, 10)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSay we're aiming for an 80%/10%/10% split - we'd want to have \u003ccode\u003e352\u003c/code\u003e, \u003ccode\u003e45\u003c/code\u003e and \u003ccode\u003e45\u003c/code\u003e instances respectively. Let's define these rations and split the dataset into a training, testing and validation set with the \u003ccode\u003etrain_test_split()\u003c/code\u003e function:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_diabetes\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.model_selection \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e train_test_split\n\ntrain_ratio = \u003cspan class=\"hljs-number\"\u003e0.80\u003c/span\u003e\ntest_ratio = \u003cspan class=\"hljs-number\"\u003e0.10\u003c/span\u003e\nvalidation_ratio = \u003cspan class=\"hljs-number\"\u003e0.10\u003c/span\u003e\n\nX, y = load_diabetes(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=validation_ratio/(train_ratio+test_ratio))\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(X_train.shape)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(X_test.shape)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(X_valid.shape)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e(352, 10)\n(45, 10)\n(45, 10)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAwesome! Our dataset has successfully been split into three sets, which we can now feed into a model and perform validation during training to tune the hyperparameters.\u003c/p\u003e\n\u003ch4 id=\"stratifiedsplit\"\u003eStratified Split\u003c/h4\u003e\n\u003cp\u003eSometimes, there's a different numbers of samples for each class in a dataset. Say, one class has 100 samples, the second one has 50, the third one 30, etc. Splitting without this in mind creates an issue when you're training a classification model (though, regression models don't suffer from this).\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIt is best to somehow split the set, so that it preserves the proportions of the classes. This is \u003cstrong\u003ea stratified split\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLuckily, the method \u003ccode\u003etrain_test_split\u003c/code\u003e has an argument called \u003ccode\u003estratify\u003c/code\u003e which takes an array which defines the number of samples by class, when splitting, to stay proportional:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_iris\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.model_selection \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e train_test_split\n\nX, y = load_iris(return_X_y=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e, stratify=y)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn a lot of cases, you can simply use the \u003ccode\u003ey\u003c/code\u003e NumPy array from your dataset for a good \u003ccode\u003estratify\u003c/code\u003e split array. This ensures that your model can fight the lack of balance between instances of classes and becomes less biased towards some.\u003c/p\u003e\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eIn this guide, we got familiar with some of the Scikit-Learn library and its \u003ccode\u003edatasets\u003c/code\u003e module. You've learned what training, testing and validation sets are, where they're applied and the benefits of validating your models.\u003c/p\u003e\n\u003cp\u003eWe've taken a look at how to employ the \u003ccode\u003etrain_test_split()\u003c/code\u003e method to split your data into a training and testing set, as well as how to separate a validation set, dynamically perserving the ratios of these sets.\u003c/p\u003e\n","parent_id":null,"type":"article","status":"published","visibility":"public","img_feature":null,"is_featured":false,"locale":"en","custom_excerpt":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"comment_id":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In this guide, we'll take a look at how to split a dataset into a training, testing and validation set using Scikit-Learn's train_test_split() method, with practical examples and tips for best practices.","read_time_min":null,"published_by":null,"published_at":1634034600000,"created_by":16,"updated_by":null,"created_at":1625677393657,"updated_at":1643227939402,"contributors":[{"id":16,"name":"David Landup","slug":"david","email":"thealduinmaster@gmail.com","password_hash":"$2a$10$W/oMJdUBSTeG3trWAHa1xO0pQruxuLgD/6hS7VuxPafcmAxeBXmVi","role_id":2,"img_profile":"//s3.stackabuse.com/media/users/865cd7d217ea11c9d9555c4f666e2d73.jpg","img_cover":null,"bio_md":"Entrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs. \n\nGreat passion for accessible education and promotion of reason, science, humanism, and progress.","bio_html":"\u003cp\u003eEntrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs.\u003c/p\u003e\n\u003cp\u003eGreat passion for accessible education and promotion of reason, science, humanism, and progress.\u003c/p\u003e\n","website":"https://www.upwork.com/freelancers/~017664e499a2766871","location":"Serbia","facebook":"","twitter":"","github":null,"status":"active","locale":null,"last_seen_at":1622229427000,"created_by":null,"updated_by":null,"created_at":1534532687000,"updated_at":1640861394795,"role":"editor","secret_token":"2a2be92558fae38f89cfbd0c6a4ba90c","is_email_confirmed":false,"_pivot_content_id":1018,"_pivot_user_id":16,"_pivot_role":"editor","_pivot_sort_order":0}],"tags":[{"id":9,"name":"python","slug":"python","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1431358631000,"updated_at":1431358631000,"_pivot_content_id":1018,"_pivot_tag_id":9,"_pivot_sort_order":0},{"id":57,"name":"artificial intelligence","slug":"artificial-intelligence","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1442517465000,"updated_at":1442517465000,"_pivot_content_id":1018,"_pivot_tag_id":57,"_pivot_sort_order":4},{"id":75,"name":"machine learning","slug":"machine-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507305534000,"updated_at":1507305534000,"_pivot_content_id":1018,"_pivot_tag_id":75,"_pivot_sort_order":2},{"id":76,"name":"scikit-learn","slug":"scikit-learn","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507308143000,"updated_at":1507308143000,"_pivot_content_id":1018,"_pivot_tag_id":76,"_pivot_sort_order":1},{"id":106,"name":"data science","slug":"data-science","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1536865458000,"updated_at":1536865458000,"_pivot_content_id":1018,"_pivot_tag_id":106,"_pivot_sort_order":3}],"_pivot_tag_id":75,"_pivot_content_id":1018},{"id":1085,"old_id":null,"uuid":"efd6ed82-029d-42a1-a885-34fea29d8839","title":"Machine Learning: Overfitting Is Your Friend, Not Your Foe","slug":"machine-learning-overfitting-is-your-friend-not-your-foe","body_md":":::note\n**Note:** These are the musings of a man - flawed and prone to misjudgement. The point of writing this is to promote a discussion on the topic, not to be right or contrarian. If any glaring mistakes are present in the writing, *please* let me know.\n:::\n\nLet me preface the potentially provocative title with:\n\n\u003e It's true, nobody wants **overfitting** end models, just like nobody wants **underfitting** end models.\n\n**Overfit models** perform great on training data, but can't generalize well to new instances. What you end up with is a model that's approaching a fully hard-coded model tailored to a specific dataset.\n\n**Underfit models** can't generalize to new data, but they can't model the original training set either.\n\nThe **right model** is one that fits the data in such a way that it performs well predicting values in the training, validation and test set, as well as new instances.\n\n### Overfitting vs. Data Scientists\n\nBattling overfitting is given a spotlight because it's more illusory, and more tempting for a rookie to create overfit models when they start with their Machine Learning journey. Throughout books, blog posts and courses, a common scenario is given:\n\n\u003e \"This model has a **100% accuracy rate!** It's perfect! Or not. Actually, it just badly overfits the dataset, and when testing it on new instances, it performs with **just X%**, which is equal to random guessing.\"\n\nAfter these sections, entire book and course chapters are dedicated to **battling overfitting** and how to avoid it. The word itself became stigmatized as a *generally bad thing*. And this is where the general conception arises:\n\n\u003e \"I must avoid overfitting at all costs.\"\n\nIt's given much more spotlight than underfitting, which is equally as \"bad\". It's worth noting that \"bad\" is an arbitrary term, and none of these conditions are inherently \"good\" or \"bad\". Some may claim that overfit models are technically more _useful_, because they at least perform well on *some data* while underfit models perform well on *no data*, but the illusion of success is a good candidate for outweighing this benefit.\n\nFor reference, let's consult _Google Trends_ and the _Google Ngram Viewer_. Google Trends display trends of search data, while the Google Ngram Viewer counts number of occurences of _n-grams_ (sequences of *n* items, such as words) in literature, parsing through a vast number of books through the ages:\n\n![overfitting vs underfitting search trends and ngram viewer](https://s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-1.jpg)\n\nEverybody talks about overfitting and mostly in the context of avoiding it - which oftentimes leads people to a general notion that it's **inherently a bad thing**.\n\nThis is *true*, to a _degree_. Yes - you don't want the end model to overfit badly, otherwise, it's practically useless. But you don't arrive at the end model right away - you tweak it numerous times, with various hyperparameters. During this process is where you **shouldn't mind seeing overfitting happening** - it's a **_good sign_**, though, **not a good result**.\n\n### How Overfitting Isn’t as Bad as It’s Made Out to Be\n\n\u003e **A model and architecture that has the ability to overfit, is more likely to have the ability to generalize well to new instances, if you simplify it (and/or tweak the data).**\n\n* \u003csmall\u003eSometimes, it isn't just about the model, as we'll see a bit later. \u003c/small\u003e\n\nIf a model *can* overfit, it has enough *entropic capacity* to extract features (in a meaningful and non-meaningful way) from data. From there, it's either that the model has more than required entropic capacity (complexity/power) or that the data itself isn't enough (very common case).\n\nThe reverse statement can also be true, but more rarely. If a given model or architecture underfits, you can try tweaking the model to see if it picks up certain features, but the type of model might just be plain wrong for the task and you won't be able to fit the data with it no matter what you do. Some models just get stuck at some level of accuracy, as they simply can't extract enough features to distinguish between certain classes, or predict values.\n\nIn **cooking** - a reverse analogy can be created. It's better to undersalt the stew early on, as you can always add salt later to taste, but it's hard to take it away once already put in.\n\nIn **Machine Learning** - it's the opposite. It's better to have a model overfit, then simplify it, change hyperparameters, augment the data, etc. to make it generalize well, but it's harder (in practical settings) to do the opposite. Avoiding overfitting _before_ it happens might very well keep you away from finding the right model and/or architecture for a longer period of time.\n\nIn practice, and in some of the most fascinating use cases of Machine Learning, and Deep Learning, you'll be working on datasets that you'll be having trouble overfitting. These will be datasets that you'll routinely be underfitting, without the ability of finding models and architectures that can generalize well and extract features. \n\nIt's also worth noting the difference between what I call **true overfitting** and **partial overfitting**. A model that overfits a dataset, and achieves 60% accuracy on the training set, with only 40% on the validation and test sets is overfitting a part of the data. However, it's not **truly overfitting** in the sense of eclipsing the entire dataset, and achieving a near 100% (false) accuracy rate, while its validation and test sets sit low at, say, ~40%.\n\nA model that partially overfits isn't one that'll be able to generalize well with simplification, as it doesn't have *enough* entropic capacity to truly (over)fit. Once it does, my argument applies, though it doesn't guarantee success, as clarified in the proceeding sections.\n\n### Case Study - Friendly Overfitting Argument\n\nThe \u003ca target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"http://yann.lecun.com/exdb/mnist/\"\u003eMNIST handwritten digits dataset\u003c/a\u003e, compiled by Yann LeCun is one of the classical benchmark datasets used for training classification models.\n\n\u003e It's also the most overused dataset, potentially ever.\n\nNothing wrong with the dataset itself - it's actually pretty good, but finding example upon example on the same dataset is boring. At one point - **we overfit ourselves** looking at it. How much? Here's my attempt at listing the first ten MNIST digits from the top of my head:\n\n```plaintext\n5, 0, 4, 1, 9, 2, 2, 4, 3\n```\n\nHow did I do?\n\n```python\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import and normalize the images, splitting out a validation set\n(X_train_full, Y_train_full), (X_test, Y_test) = keras.datasets.mnist.load_data()\n\nX_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\nY_valid, Y_train = Y_train_full[:5000], Y_train_full[5000:]\n\nX_test = X_test/255.0\n\n# Print out the first ten digits\nfig, ax = plt.subplots(1, 10, figsize=(10,2))\nfor i in range(10):\n    ax[i].imshow(X_train_full[i])\n    ax[i].axis('off')\n    plt.subplots_adjust(wspace=1) \n\nplt.show()\n```\n\n![](https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-2.png)\n\nAlmost there. \n\n\u003e I'll use this chance to make a public plea to all content creators to not overuse this dataset beyond the introductory parts, where the simplicity of the dataset can be used to lower the barrier to entry. *Please*.\n\nAdditionally, this dataset makes it hard to build a model that underfits. It's just too simple - and even a fairly small **Multilayer Perceptron (MLP)** classifier built with an intuitive number of layers and neurons per layer can easily reach upwards of 98% accuracy on the training, testing and validation set. \u003ca target=\"_blank\" href=\"https://github.com/StackAbuse/friendly-overfitting-argument/blob/main/Friendly-Ovetfitting-Argument.ipynb\"\u003eHere's a Jupyter Notebook\u003c/a\u003e of a simple MPL achieving ~98% accuracy on both the training, validation and testing sets, which I spun up with sensible defaults.\n\n\u003e I haven't even bothered to try tuning it to perform better than the initial setup.\n\n#### The CIFAR10 and CIFAR100 Datasets\n\nLet's use a dataset that's more complicated than MNIST handwritten digits, and which makes a simple MLP underfit but which is simple enough to let a decently-sized CNN to truly overfit on it. A good candidate is the \u003ca target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"https://www.cs.toronto.edu/~kriz/cifar.html\"\u003e_CIFAR dataset_\u003c/a\u003e. \n\n\u003e There's a 10 classes of images in CIFAR10, and 100 in CIFAR100. Additionally, the CIFAR100 dataset has 20 families of *similar* classes, which means the network additionally has to learn the minute differences between similar, but different classes. These are known as **\"fine labels\" (100)** and **\"coarse labels\" (20)** and predicting these is equal to predicting the specific class, or just the family it belongs to.\n\nFor instance, here's a superclass (coarse label) and it's subclasses (fine labels):\n\n\u003ctable class=\"table table-striped\"\u003e\n    \u003ctr\u003e\n        \u003ctd\u003eSuperclass\u003c/td\u003e\n        \u003ctd\u003eSubclasses\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd\u003efood containers\u003c/td\u003e\n        \u003ctd\u003ebottles, bowls, cans, cups, plates\u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\n\nA cup is a cylinder, similar to a soda can, and some bottles may be too. Since these low-level features are relatively similar, it's easy to chuck them all into the _\"food container\"_ category, but higher-level abstraction is required to properly guess whether something is a _\"cup\"_ or a _\"can\"_.\n\nWhat makes this job even harder is that CIFAR10 has 6000 images per class, while CIFAR100 has 600 images per class, giving the network less images to learn the ever so subtle differences from. Cups without handles exist, and cans without ridges do too. From a profile - it might not be too easy to tell them apart.\n\n![](https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-3.png)\n\nThis is where, say, a _Multilayer Perceptron_ simply doesn't have the abstraction power to learn, and it's doomed to fail, horribly underfitting. _Convolutional Neural Networks_ are built based on the \u003ca target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"https://link.springer.com/article/10.1007/BF00344251\"\u003eNeocognitron\u003c/a\u003e, which took hints from neuroscience and the hierarchical pattern recognition that the brain performs. These networks are able to extract features like this, and excel at the task. So much so that they oftentimes overfit badly and can't be used as is in the end - where we typically sacrifice some accuracy for the sake of generalization ability.\n\nLet's train two different network architectures on the CIFAR10 and CIFAR100 dataset as an illustration of my point.\n\n\u003e This is also where we'll be able to see how even when a network overfits, it's no guarantee that the network itself will definitely generalize well if simplified - it might not be able to generalize if simplified, though there is a tendency. The network might be **right**, but the **data** might not be enough.\n\nIn the case of CIFAR100 - just 500 images for training (and 100 for testing) per class is not enough for a simple CNN to *really* generalize well on the entire 100 classes, and we'll have to perform data augmentation to help it along. Even with data augmentation, we might not get a highly accurate network as there's just so much you can do to the data. If the same architecture performs well on CIFAR10, but not CIFAR100 - it means it simply can't distinguish from some of the more fine-grained details that make the difference between cylindrical objects that we call a \"cup\", \"can\" and \"bottle\", for instance. \n\n\u003e The \u003ca rel=\"nofollow noopener noreferrer\" target=\"_blank\" href=\"https://paperswithcode.com/sota/image-classification-on-cifar-100\"\u003evast majority\u003c/a\u003e of advanced network architectures that achieve a high accuracy on the CIFAR100 dataset perform data augmentation or otherwise expand the training set.\n\nMost of them *have to*, and that's not a sign of bad engineering. In fact - the fact that we can expand these datasets and help networks generalize better is a sign of engineering ingenuity.\n\nAdditionally, I'd invite any human to try and guess what these are, if they're convinced that image classification isn't too hard with images as small as 32x32:\n\n![](https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-4.png)\n\nIs *Image 4* a few oranges? Ping pong balls? Egg yolks? Well, probably not egg yolks, but that requires prior knowledge on what \"eggs\" are and whether you're likely to find yolks sitting on the table, which a network won't have. Consider the amount of prior knowledge you may have regarding the world and how much it affects what you see.\n\n#### Importing the Data\n\nWe'll be using Keras as the deep learning library of choice, but you can follow along with other libraries or even your custom models if you're up for it.\n\nBut first off, let's load it in, separate the data into a training, testing and validation set, normalizing the image values to `0..1`:\n\n```python\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Starting with CIFAR10\n(X_train_full, Y_train_full), (X_test, Y_test) = keras.datasets.cifar10.load_data()\n\nX_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\nY_valid, Y_train = Y_train_full[:5000], Y_train_full[5000:]\n\nX_test = X_test/255.0\n```\n\nThen, let's visualize some of the images in the dataset to get an idea of what we're up against:\n\n```python\nfig, ax = plt.subplots(5, 5, figsize=(10, 10))\nax = ax.ravel()\n\n# Labels come as numbers of [0..9], so here are the class names for humans\nclass_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n\nfor i in range(25):\n    ax[i].imshow(X_train_full[i])\n    ax[i].set_title(class_names[Y_train_full[i][0]])\n    ax[i].axis('off')\n    plt.subplots_adjust(wspace=1) \n\nplt.show()\n```\n\n![](https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-5.png)\n\n#### Underfitting Multilayer Perceptron\n\nPretty much no matter what we do, the MLP won't perform that well. It'll definitely reach some level of accuracy based on the raw sequences of information coming in - but this number is capped and probably won't be too high. \n\nThe network will start overfitting at one point, learning the concrete sequences of data denoting images, but will still have low accuracy on the training set even when overfitting, which is the prime time to stop training it, since it simply can't fit the data well. \u003csmall\u003eTraining networks has a carbon footprint, you know.\u003c/small\u003e\n\nLet's add in an `EarlyStopping` callback to avoid running the network beyond the point of common sense, and set the `epochs` to a number beyond what we'll run it for (so `EarlyStopping` can kick in). \n\nWe'll use the Sequential API to add a couple of layers with `BatchNormalization` and a bit of `Dropout`. They help with generalization and we want to at least *try* to get this model to learn something.\n\nThe main hyperparameters we can tweak here are the number of layers, their sizes, activation functions, kernel initializers and dropout rates, and here's a \"decently\" performing setup:\n\n```python\ncheckpoint = keras.callbacks.ModelCheckpoint(\"simple_dense.h5\", save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n\nmodel = keras.Sequential([\n  keras.layers.Flatten(input_shape=[32, 32, 3]),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dense(75),\n    \n  keras.layers.Dense((50), activation='elu'),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dropout(0.1),\n    \n  keras.layers.Dense((50), activation='elu'),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dropout(0.1),\n    \n  keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.Nadam(learning_rate=1e-4),\n              metrics=[\"accuracy\"])\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=150, \n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint, early_stopping])\n```\n\nLet's see if the starting hypothesis is true - it'll start out learning and generalizing to some extent but will end up having low accuracy on both the training set as well as the testing and validation set, resulting in an overall low accuracy.\n\nFor CIFAR10, the network performs \"okay\"-ish:\n\n```python\nEpoch 1/150\n1407/1407 [==============================] - 5s 3ms/step - loss: 1.9706 - accuracy: 0.3108 - val_loss: 1.6841 - val_accuracy: 0.4100\n...\nEpoch 50/150\n1407/1407 [==============================] - 4s 3ms/step - loss: 1.2927 - accuracy: 0.5403 - val_loss: 1.3893 - val_accuracy: 0.5122\n```\n\nLet's take a look at the history of its learning:\n\n```python\npd.DataFrame(history.history).plot()\nplt.show()\n\nmodel.evaluate(X_test, Y_test)\n```\n\n![](https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-6.png)\n\n```plaintext\n313/313 [==============================] - 0s 926us/step - loss: 1.3836 - accuracy: 0.5058\n[1.383605718612671, 0.5058000087738037]\n```\n\nThe overall accuracy gets up to ~50% and the network gets here pretty quickly and starts plateauing. 5/10 images being correctly classified sounds like tossing a coin, but remember that there are 10 classes here, so if it were randomly guessing, it'd on average guess a single image out of ten. Let's switch to the CIFAR100 dataset, which also necessitates a network with at least a tiny bit more power, as there are less training instances per class, as well as a vastly higher number of classes:\n\n```python\ncheckpoint = keras.callbacks.ModelCheckpoint(\"bigger_dense.h5\", save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n\n# Changing the loaded data\n(X_train_full, Y_train_full), (X_test, Y_test) = keras.datasets.cifar100.load_data()\n\n# Modify the model\nmodel1 = keras.Sequential([\n  keras.layers.Flatten(input_shape=[32, 32, 3]),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dense(256, activation='relu', kernel_initializer=\"he_normal\"),\n    \n  keras.layers.Dense(128, activation='relu'),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dropout(0.1),\n\n  keras.layers.Dense(100, activation='softmax')\n])\n\n\nmodel1.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.Nadam(learning_rate=1e-4),\n              metrics=[\"accuracy\"])\n\nhistory = model1.fit(X_train, \n                    Y_train, \n                    epochs=150, \n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint, early_stopping])\n```\n\nThe network performs fairly badly:\n\n```python\nEpoch 1/150\n1407/1407 [==============================] - 13s 9ms/step - loss: 4.2260 - accuracy: 0.0836 - val_loss: 3.8682 - val_accuracy: 0.1238\n...\nEpoch 24/150\n1407/1407 [==============================] - 12s 8ms/step - loss: 2.3598 - accuracy: 0.4006 - val_loss: 3.3577 - val_accuracy: 0.2434\n```\n\nAnd let's plot the history of its progress, as well as evaluate it on the testing set (which will likely perform as well as the validation set):\n\n```python\npd.DataFrame(history.history).plot()\nplt.show()\n\nmodel.evaluate(X_test, Y_test)\n```\n\n![](https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-7.png)\n\n```plaintext\n313/313 [==============================] - 0s 2ms/step - loss: 3.2681 - accuracy: 0.2408\n[3.2681326866149902, 0.24079999327659607]\n```\n\nAs expected, the network wasn't able to grasp the data well. It ended up having an overfit accuracy of 40%, and an actual accuracy of ~24%.\n\nThe accuracy capped at 40% - it wasn't *really* able to overfit the dataset, even if it overfit some parts of it that it was able to discern given the limited architecture. This model doesn't have the necessary entropic capacity required for it to truly overfit for the sake of my argument.\n\nThis model and its architecture simply isn't well suited for this task - and while we could technically get it to (over)fit more, it'll still have issues on the long-run. For instance, let's turn it into a bigger network, which would theoretically let it recognize more complex patterns:\n\n```python\nmodel2 = keras.Sequential([\n  keras.layers.Flatten(input_shape=[32, 32, 3]),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dense(512, activation='relu', kernel_initializer=\"he_normal\"),\n    \n  keras.layers.Dense(256, activation='relu'),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dropout(0.1),\n    \n  keras.layers.Dense(128, activation='relu'),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dropout(0.1),\n\n  keras.layers.Dense(100, activation='softmax')\n])\n```\n\nThough, this doesn't do much better at all:\n\n```python\nEpoch 24/150\n1407/1407 [==============================] - 28s 20ms/step - loss: 2.1202 - accuracy: 0.4507 - val_loss: 3.2796 - val_accuracy: 0.2528\n```\n\nIt's much more complex (density explodes), yet it simply cannot extract much more:\n\n```python\nmodel1.summary()\nmodel2.summary()\n```\n\n```python\nModel: \"sequential_17\"\n...\nTotal params: 845,284\nTrainable params: 838,884\nNon-trainable params: 6,400\n_________________________________________________________________\nModel: \"sequential_18\"\n...\nTotal params: 1,764,324\nTrainable params: 1,757,412\nNon-trainable params: 6,912\n```\n\n#### Overfitting Convolutional Neural Network on CIFAR10\n\nNow, let's try doing something different. Switching to a CNN will significantly help with extracting features from the dataset, thereby allowing the model to *truly* overfit, reaching much higher (illusory) accuracy.\n\nWe'll kick out the `EarlyStopping` callback to let it do its thing. Additionally, we won't be using `Dropout` layers, and instead try to force the network to learn the features through more layers.\n\n:::note\n**Note:** Outside of the context of trying to prove the argument, this would be horrible advice. This is opposite of what you'd want to do by the end. Dropout helps networks generalize better, by forcing the non-dropped neurons to pick up the slack. Forcing the network to learn through more layers it more likely to lead to an overfit model.\n:::\n\nThe reason I'm purposefully doing this is to allow the network to horribly overfit as **a sign of it's ability to actually discern features, before simplifying it and adding `Dropout` to really allow it to generalize.** If it reaches high (illusory) accuracy, it can extract much more than the MLP model, which means we can start simplyfing it.\n\nLet's once again use the Sequential API to build a CNN, firstly on the CIFAR10 dataset:\n\n```python\ncheckpoint = keras.callbacks.ModelCheckpoint(\"overcomplicated_cnn_cifar10.h5\", save_best_only=True)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(64, 3, activation='relu', \n                        kernel_initializer=\"he_normal\", \n                        kernel_regularizer=keras.regularizers.l2(l=0.01), \n                        padding='same', \n                        input_shape=[32, 32, 3]),\n    keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n    keras.layers.MaxPooling2D(2),\n    \n    keras.layers.Conv2D(128, 2, activation='relu', padding='same'),\n    keras.layers.Conv2D(128, 2, activation='relu', padding='same'),\n    keras.layers.MaxPooling2D(2),\n    \n    keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n    keras.layers.MaxPooling2D(2),\n    \n    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n    keras.layers.MaxPooling2D(2),\n    \n    keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n    keras.layers.MaxPooling2D(2),\n    \n    keras.layers.Flatten(),    \n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\n\nmodel.summary()\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=150,\n                    batch_size=64,\n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint])\n```\n\nAwesome, it overfit pretty quickly! Within just a few epochs, it started overfitting the data, and by epoch 31, it got up to 98%, with a lower validation accuracy:\n\n```python\nEpoch 1/150\n704/704 [==============================] - 149s 210ms/step - loss: 1.9561 - accuracy: 0.4683 - val_loss: 2.5060 - val_accuracy: 0.3760\n...\nEpoch 31/150\n704/704 [==============================] - 149s 211ms/step - loss: 0.0610 - accuracy: 0.9841 - val_loss: 1.0433 - val_accuracy: 0.6958\n```\n\nSince there are only 10 output classes, even though we tried overfitting it *a lot* by creating an unnecessarily big CNN, the validation accuracy is still fairly high. \n\n#### Simplifying the Convolutional Neural Network on CIFAR10\n\nNow, let's simplify it to see how it'll fare with a more reasonable architecture. We'll add in `BatchNormalization` and `Dropout` as both help with the generalization:\n\n```python\ncheckpoint = keras.callbacks.ModelCheckpoint(\"simplified_cnn_cifar10.h5\", save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(32, 3, activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(l=0.01), padding='same', input_shape=[32, 32, 3]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    keras.layers.Dropout(0.4),\n    \n    keras.layers.Conv2D(64, 2, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(64, 2, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    keras.layers.Dropout(0.4),\n    \n    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    keras.layers.Dropout(0.5),\n    \n    keras.layers.Flatten(),    \n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\n\nmodel.summary()\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=150,\n                    batch_size=64,\n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint, early_stopping])\n```\n\nThis model has a (modest) count of 323,146 trainable parameters, compared to 1,579,178 from the previous CNN. How does it perform?\n\n```python\nEpoch 1/150\n704/704 [==============================] - 91s 127ms/step - loss: 2.1327 - accuracy: 0.3910 - val_loss: 1.5495 - val_accuracy: 0.5406\n...\nEpoch 52/150\n704/704 [==============================] - 89s 127ms/step - loss: 0.4091 - accuracy: 0.8648 - val_loss: 0.4694 - val_accuracy: 0.8500\n```\n\nIt actually achieves a pretty decent ~85% accuracy! Occam's Razor strikes again. Let's take a look at some of the results:\n\n```python\ny_preds = model.predict(X_test)\nprint(y_preds[1])\nprint(np.argmax(y_preds[1]))\n\nfig, ax = plt.subplots(6, 6, figsize=(10, 10))\nax = ax.ravel()\n\nfor i in range(0, 36):\n    ax[i].imshow(X_test[i])\n    ax[i].set_title(\"Actual: %s\\nPred: %s\" % (class_names[Y_test[i][0]], class_names[np.argmax(y_preds[i])]))\n    ax[i].axis('off')\n    plt.subplots_adjust(wspace=1)\n    \nplt.show()\n```\n\n![](https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-8.png)\n\nThe main misclassifications are two images in this small set - a dog was misclassified as a deer (respectable enough), but a closeup of an emu bird was classified as a cat (funny enough so we'll let it slide).\n\n#### Overfitting Convolutional Neural Network on CIFAR100\n\nWhat happens when we go for the CIFAR100 dataset?\n\n```python\ncheckpoint = keras.callbacks.ModelCheckpoint(\"overcomplicated_cnn_model_cifar100.h5\", save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(32, 3, activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(l=0.01), padding='same', input_shape=[32, 32, 3]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    \n    keras.layers.Conv2D(64, 2, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(64, 2, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    \n    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    \n    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    \n    keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    \n    keras.layers.Flatten(),    \n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.BatchNormalization(),\n    \n    keras.layers.Dense(100, activation='softmax')\n])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\n\nmodel.summary()\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=150,\n                    batch_size=64,\n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint])\n```\n\n```python\nEpoch 1/150\n704/704 [==============================] - 97s 137ms/step - loss: 4.1752 - accuracy: 0.1336 - val_loss: 3.9696 - val_accuracy: 0.1392\n...\nEpoch 42/150\n704/704 [==============================] - 95s 135ms/step - loss: 0.1543 - accuracy: 0.9572 - val_loss: 4.1394 - val_accuracy: 0.4458\n```\n\nWonderful! ~96% accuracy on the training set! Don't mind the ~44% validation accuracy just yet. Let's simplify the model real quick to get it to generalize better.\n\n#### Failure to Generalize After Simplification\n\nAnd this is where it becomes clear that the ability to overfit doesn't **guarantee** that the model could generalize better when simplified. In the case of CIFAR100, there aren't many training instances per class, and this will likely prevent a simplified version of the previous model learn well. Let's try it out:\n\n```python\ncheckpoint = keras.callbacks.ModelCheckpoint(\"simplified_cnn_model_cifar100.h5\", save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(32, 3, activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(l=0.01), padding='same', input_shape=[32, 32, 3]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    keras.layers.Dropout(0.4),\n    \n    keras.layers.Conv2D(64, 2, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(64, 2, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    keras.layers.Dropout(0.4),\n    \n    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    keras.layers.Dropout(0.5),\n    \n    keras.layers.Flatten(),    \n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(100, activation='softmax')\n])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n              metrics=[\"accuracy\"])\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=150,\n                    batch_size=64,\n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint, early_stopping])\n```\n\n```python\nEpoch 1/150\n704/704 [==============================] - 96s 135ms/step - loss: 4.4432 - accuracy: 0.1112 - val_loss: 3.7893 - val_accuracy: 0.1702\n...\nEpoch 48/150\n704/704 [==============================] - 92s 131ms/step - loss: 1.2550 - accuracy: 0.6370 - val_loss: 1.7147 - val_accuracy: 0.5466\n```\n\nIt's plateauing and can't really get to generalize the data. In this case, it might not be the model's fault - maybe it's just right for the task, especially given the high accuracy on the CIFAR10 dataset, which has the same input shape and similar images in the dataset. It appears that the model can be reasonably accurate with the general shapes, but not the distinction between fine shapes.\n\nThe simpler model actually performs better than the more complicated one in terms of validation accuracy - so the more complex CNN doesn't get these fine details much better at all. Here, the problem most likely lies in the fact that there are only 500 training images per class, which really isn't enough. In the more complex network, this leads to overfitting, because there's not enough diversity - when simplified to avoid overfitting, this causes underfitting as again, there's no diversity.\n\n\u003e  This is why the vast majority of the papers linked before, and the vast majority of networks augment the data of the CIFAR100 dataset.\n\nIt's genuinely not a dataset for which it's easy to get high accuracy on, unlike the MNIST handwritten digits dataset, and a simple CNN like we're building probably won't cut it for high accuracy. Just remember the number of quite specific classes, how uninformative some of the images are, and *just how much prior knowledge humans have to discern between these*.\n\nLet's do our best by augmenting a few images and artificially expanding the training data, to at least try to get a higher accuracy. Keep in mind that the CIFAR100 is, again, a genuinely difficult dataset to get high accuracy on with simple models. The state of the art models use different and novel techniques to shave off errors, and many of these models aren't even *CNNs* - they're *Transformers*.\n\n\u003e If you'd like to take a look at the landscape of these models, \u003ca rel=\"nofollow noopener noreferrer\" target=\"_blank\" href=\"https://paperswithcode.com/sota/image-classification-on-cifar-100\"\u003ePapersWithCode\u003c/a\u003e has done a beautiful compilation of papers, source code and results.\n\n#### Data Augmentation with Keras' ImageDataGenerator Class\n\nWill data augmentation help? Usually, it does, but with a *serious* lack of training data like we're facing, there's just so much you can do with random rotations, flipping, cropping, etc. If an architecture can't generalize well on a dataset, you'll likely boost it via data augmentation, but it probably won't be a whole lot.\n\nThat being said, let's use Keras' `ImageDataGenerator` class to try and generate some new training data with random changes, in hopes of improving the model's accuracy. If it does improve, it shouldn't be by a huge amount, and it'll likely get back to partially overfitting the dataset without an ability to either generalize well or fully overfit the data.\n\nGiven the constant random variations in the data, the model is less likely to overfit on the same number of epochs, as the variations make it keep adjusting to \"new\" data. Let's run it for, say, 300 epochs, which is significantly more than the rest of the networks we've trained. This is possible without *major* overfitting, again, due to the random modifications made to the images while they're flowing in:\n\n```python\ncheckpoint = keras.callbacks.ModelCheckpoint(\"augmented_cnn.h5\", save_best_only=True)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(64, 3, activation='relu', kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(l=0.01), padding='same', input_shape=[32, 32, 3]),\n    keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    keras.layers.Dropout(0.4),\n    \n    keras.layers.Conv2D(128, 2, activation='relu', padding='same'),\n    keras.layers.Conv2D(128, 2, activation='relu', padding='same'),\n    keras.layers.Conv2D(128, 2, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    keras.layers.Dropout(0.4),\n    \n    keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n    keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n    keras.layers.Conv2D(256, 3, activation='relu', padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(2),\n    keras.layers.Dropout(0.4),\n    \n    keras.layers.Flatten(),    \n    keras.layers.Dense(512, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(100, activation='softmax')\n])\n\n    \ntrain_datagen = ImageDataGenerator(rotation_range=30,\n        height_shift_range=0.2,\n        width_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        vertical_flip=True,\n        fill_mode='nearest')\n\nvalid_datagen = ImageDataGenerator()\n\ntrain_datagen.fit(X_train)\nvalid_datagen.fit(X_valid)\n\ntrain_generator = train_datagen.flow(X_train, Y_train, batch_size=128)\nvalid_generator = valid_datagen.flow(X_valid, Y_valid, batch_size=128)\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=keras.optimizers.Adam(learning_rate=1e-3, decay=1e-6),\n              metrics=[\"accuracy\"])\n\nhistory = model.fit(train_generator, \n                    epochs=300,\n                    batch_size=128,\n                    steps_per_epoch=len(X_train)//128,\n                    validation_data=valid_generator,\n                    callbacks=[checkpoint])\n```\n\n```\nEpoch 1/300\n351/351 [==============================] - 16s 44ms/step - loss: 5.3788 - accuracy: 0.0487 - val_loss: 5.3474 - val_accuracy: 0.0440\n...\nEpoch 300/300\n351/351 [==============================] - 15s 43ms/step - loss: 1.0571 - accuracy: 0.6895 - val_loss: 2.0005 - val_accuracy: 0.5532\n```\n\n![](https://s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-9.png)\n\nThe model is performing with ~55% on the validation set, and is still overfitting the data partially. The `val_loss` has stopped going down, and is quite rocky, even with a higher `batch_size`. \n\nThis network simply can't learn and fit the data with high accuracy, even though variations off it do have the entropic capacity to overfit the data.\n\n### Conclusion?\n\nOverfitting isn't inherently a bad thing - it's just *a thing*. No, you don't want overfit end-models, but it shouldn't be treated as the plague and can even be a good sign that a model could perform better given more data and a simplification step. This isn't guaranteed, by any means, and the CIFAR100 dataset has been used as an example of a dataset that's not easy to generalize well to.\n\nThe point of this rambling is, again, not to be contrarian - but to incite discussion on the topic, which doesn't appear to be taking much place.\n\n\u003e Who am I to make this claim? \n\nJust someone who sits home, practicing the craft, with a deep fascination towards tomorrow.\n\n\u003e Do I have the ability to be wrong? \n\nVery much so. \n\n\u003e How should you take this piece? \n\nTake it as you may - think for yourself if it makes sense or not. If you *don't* think I'm out of my place for noting this, let me know. If you think I'm wrong on this - by all means, please feel let me know and don't mince your words. :)","body_html":"\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e These are the musings of a man - flawed and prone to misjudgement. The point of writing this is to promote a discussion on the topic, not to be right or contrarian. If any glaring mistakes are present in the writing, \u003cem\u003eplease\u003c/em\u003e let me know.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eLet me preface the potentially provocative title with:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIt's true, nobody wants \u003cstrong\u003eoverfitting\u003c/strong\u003e end models, just like nobody wants \u003cstrong\u003eunderfitting\u003c/strong\u003e end models.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cstrong\u003eOverfit models\u003c/strong\u003e perform great on training data, but can't generalize well to new instances. What you end up with is a model that's approaching a fully hard-coded model tailored to a specific dataset.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUnderfit models\u003c/strong\u003e can't generalize to new data, but they can't model the original training set either.\u003c/p\u003e\n\u003cp\u003eThe \u003cstrong\u003eright model\u003c/strong\u003e is one that fits the data in such a way that it performs well predicting values in the training, validation and test set, as well as new instances.\u003c/p\u003e\n\u003ch3 id=\"overfittingvsdatascientists\"\u003eOverfitting vs. Data Scientists\u003c/h3\u003e\n\u003cp\u003eBattling overfitting is given a spotlight because it's more illusory, and more tempting for a rookie to create overfit models when they start with their Machine Learning journey. Throughout books, blog posts and courses, a common scenario is given:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026quot;This model has a \u003cstrong\u003e100% accuracy rate!\u003c/strong\u003e It's perfect! Or not. Actually, it just badly overfits the dataset, and when testing it on new instances, it performs with \u003cstrong\u003ejust X%\u003c/strong\u003e, which is equal to random guessing.\u0026quot;\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAfter these sections, entire book and course chapters are dedicated to \u003cstrong\u003ebattling overfitting\u003c/strong\u003e and how to avoid it. The word itself became stigmatized as a \u003cem\u003egenerally bad thing\u003c/em\u003e. And this is where the general conception arises:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026quot;I must avoid overfitting at all costs.\u0026quot;\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIt's given much more spotlight than underfitting, which is equally as \u0026quot;bad\u0026quot;. It's worth noting that \u0026quot;bad\u0026quot; is an arbitrary term, and none of these conditions are inherently \u0026quot;good\u0026quot; or \u0026quot;bad\u0026quot;. Some may claim that overfit models are technically more \u003cem\u003euseful\u003c/em\u003e, because they at least perform well on \u003cem\u003esome data\u003c/em\u003e while underfit models perform well on \u003cem\u003eno data\u003c/em\u003e, but the illusion of success is a good candidate for outweighing this benefit.\u003c/p\u003e\n\u003cp\u003eFor reference, let's consult \u003cem\u003eGoogle Trends\u003c/em\u003e and the \u003cem\u003eGoogle Ngram Viewer\u003c/em\u003e. Google Trends display trends of search data, while the Google Ngram Viewer counts number of occurences of \u003cem\u003en-grams\u003c/em\u003e (sequences of \u003cem\u003en\u003c/em\u003e items, such as words) in literature, parsing through a vast number of books through the ages:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-1.jpg\" alt=\"overfitting vs underfitting search trends and ngram viewer\"\u003e\u003c/p\u003e\n\u003cp\u003eEverybody talks about overfitting and mostly in the context of avoiding it - which oftentimes leads people to a general notion that it's \u003cstrong\u003einherently a bad thing\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThis is \u003cem\u003etrue\u003c/em\u003e, to a \u003cem\u003edegree\u003c/em\u003e. Yes - you don't want the end model to overfit badly, otherwise, it's practically useless. But you don't arrive at the end model right away - you tweak it numerous times, with various hyperparameters. During this process is where you \u003cstrong\u003eshouldn't mind seeing overfitting happening\u003c/strong\u003e - it's a \u003cstrong\u003e\u003cem\u003egood sign\u003c/em\u003e\u003c/strong\u003e, though, \u003cstrong\u003enot a good result\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 id=\"howoverfittingisntasbadasitsmadeouttobe\"\u003eHow Overfitting Isn’t as Bad as It’s Made Out to Be\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eA model and architecture that has the ability to overfit, is more likely to have the ability to generalize well to new instances, if you simplify it (and/or tweak the data).\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003e\u003csmall\u003eSometimes, it isn't just about the model, as we'll see a bit later. \u003c/small\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf a model \u003cem\u003ecan\u003c/em\u003e overfit, it has enough \u003cem\u003eentropic capacity\u003c/em\u003e to extract features (in a meaningful and non-meaningful way) from data. From there, it's either that the model has more than required entropic capacity (complexity/power) or that the data itself isn't enough (very common case).\u003c/p\u003e\n\u003cp\u003eThe reverse statement can also be true, but more rarely. If a given model or architecture underfits, you can try tweaking the model to see if it picks up certain features, but the type of model might just be plain wrong for the task and you won't be able to fit the data with it no matter what you do. Some models just get stuck at some level of accuracy, as they simply can't extract enough features to distinguish between certain classes, or predict values.\u003c/p\u003e\n\u003cp\u003eIn \u003cstrong\u003ecooking\u003c/strong\u003e - a reverse analogy can be created. It's better to undersalt the stew early on, as you can always add salt later to taste, but it's hard to take it away once already put in.\u003c/p\u003e\n\u003cp\u003eIn \u003cstrong\u003eMachine Learning\u003c/strong\u003e - it's the opposite. It's better to have a model overfit, then simplify it, change hyperparameters, augment the data, etc. to make it generalize well, but it's harder (in practical settings) to do the opposite. Avoiding overfitting \u003cem\u003ebefore\u003c/em\u003e it happens might very well keep you away from finding the right model and/or architecture for a longer period of time.\u003c/p\u003e\n\u003cp\u003eIn practice, and in some of the most fascinating use cases of Machine Learning, and Deep Learning, you'll be working on datasets that you'll be having trouble overfitting. These will be datasets that you'll routinely be underfitting, without the ability of finding models and architectures that can generalize well and extract features.\u003c/p\u003e\n\u003cp\u003eIt's also worth noting the difference between what I call \u003cstrong\u003etrue overfitting\u003c/strong\u003e and \u003cstrong\u003epartial overfitting\u003c/strong\u003e. A model that overfits a dataset, and achieves 60% accuracy on the training set, with only 40% on the validation and test sets is overfitting a part of the data. However, it's not \u003cstrong\u003etruly overfitting\u003c/strong\u003e in the sense of eclipsing the entire dataset, and achieving a near 100% (false) accuracy rate, while its validation and test sets sit low at, say, ~40%.\u003c/p\u003e\n\u003cp\u003eA model that partially overfits isn't one that'll be able to generalize well with simplification, as it doesn't have \u003cem\u003eenough\u003c/em\u003e entropic capacity to truly (over)fit. Once it does, my argument applies, though it doesn't guarantee success, as clarified in the proceeding sections.\u003c/p\u003e\n\u003ch3 id=\"casestudyfriendlyoverfittingargument\"\u003eCase Study - Friendly Overfitting Argument\u003c/h3\u003e\n\u003cp\u003eThe \u003ca target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"http://yann.lecun.com/exdb/mnist/\"\u003eMNIST handwritten digits dataset\u003c/a\u003e, compiled by Yann LeCun is one of the classical benchmark datasets used for training classification models.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIt's also the most overused dataset, potentially ever.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNothing wrong with the dataset itself - it's actually pretty good, but finding example upon example on the same dataset is boring. At one point - \u003cstrong\u003ewe overfit ourselves\u003c/strong\u003e looking at it. How much? Here's my attempt at listing the first ten MNIST digits from the top of my head:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e5, 0, 4, 1, 9, 2, 2, 4, 3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHow did I do?\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e keras\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\n\u003cspan class=\"hljs-comment\"\u003e# Import and normalize the images, splitting out a validation set\u003c/span\u003e\n(X_train_full, Y_train_full), (X_test, Y_test) = keras.datasets.mnist.load_data()\n\nX_valid, X_train = X_train_full[:\u003cspan class=\"hljs-number\"\u003e5000\u003c/span\u003e]/\u003cspan class=\"hljs-number\"\u003e255.0\u003c/span\u003e, X_train_full[\u003cspan class=\"hljs-number\"\u003e5000\u003c/span\u003e:]/\u003cspan class=\"hljs-number\"\u003e255.0\u003c/span\u003e\nY_valid, Y_train = Y_train_full[:\u003cspan class=\"hljs-number\"\u003e5000\u003c/span\u003e], Y_train_full[\u003cspan class=\"hljs-number\"\u003e5000\u003c/span\u003e:]\n\nX_test = X_test/\u003cspan class=\"hljs-number\"\u003e255.0\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# Print out the first ten digits\u003c/span\u003e\nfig, ax = plt.subplots(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e))\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e):\n    ax[i].imshow(X_train_full[i])\n    ax[i].axis(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;off\u0026#x27;\u003c/span\u003e)\n    plt.subplots_adjust(wspace=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) \n\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-2.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAlmost there.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI'll use this chance to make a public plea to all content creators to not overuse this dataset beyond the introductory parts, where the simplicity of the dataset can be used to lower the barrier to entry. \u003cem\u003ePlease\u003c/em\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAdditionally, this dataset makes it hard to build a model that underfits. It's just too simple - and even a fairly small \u003cstrong\u003eMultilayer Perceptron (MLP)\u003c/strong\u003e classifier built with an intuitive number of layers and neurons per layer can easily reach upwards of 98% accuracy on the training, testing and validation set. \u003ca target=\"_blank\" href=\"https://github.com/StackAbuse/friendly-overfitting-argument/blob/main/Friendly-Ovetfitting-Argument.ipynb\"\u003eHere's a Jupyter Notebook\u003c/a\u003e of a simple MPL achieving ~98% accuracy on both the training, validation and testing sets, which I spun up with sensible defaults.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI haven't even bothered to try tuning it to perform better than the initial setup.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4 id=\"thecifar10andcifar100datasets\"\u003eThe CIFAR10 and CIFAR100 Datasets\u003c/h4\u003e\n\u003cp\u003eLet's use a dataset that's more complicated than MNIST handwritten digits, and which makes a simple MLP underfit but which is simple enough to let a decently-sized CNN to truly overfit on it. A good candidate is the \u003ca target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"https://www.cs.toronto.edu/~kriz/cifar.html\"\u003e\u003cem\u003eCIFAR dataset\u003c/em\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThere's a 10 classes of images in CIFAR10, and 100 in CIFAR100. Additionally, the CIFAR100 dataset has 20 families of \u003cem\u003esimilar\u003c/em\u003e classes, which means the network additionally has to learn the minute differences between similar, but different classes. These are known as \u003cstrong\u003e\u0026quot;fine labels\u0026quot; (100)\u003c/strong\u003e and \u003cstrong\u003e\u0026quot;coarse labels\u0026quot; (20)\u003c/strong\u003e and predicting these is equal to predicting the specific class, or just the family it belongs to.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFor instance, here's a superclass (coarse label) and it's subclasses (fine labels):\u003c/p\u003e\n\u003ctable class=\"table table-striped\"\u003e\n    \u003ctr\u003e\n        \u003ctd\u003eSuperclass\u003c/td\u003e\n        \u003ctd\u003eSubclasses\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd\u003efood containers\u003c/td\u003e\n        \u003ctd\u003ebottles, bowls, cans, cups, plates\u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003eA cup is a cylinder, similar to a soda can, and some bottles may be too. Since these low-level features are relatively similar, it's easy to chuck them all into the \u003cem\u003e\u0026quot;food container\u0026quot;\u003c/em\u003e category, but higher-level abstraction is required to properly guess whether something is a \u003cem\u003e\u0026quot;cup\u0026quot;\u003c/em\u003e or a \u003cem\u003e\u0026quot;can\u0026quot;\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eWhat makes this job even harder is that CIFAR10 has 6000 images per class, while CIFAR100 has 600 images per class, giving the network less images to learn the ever so subtle differences from. Cups without handles exist, and cans without ridges do too. From a profile - it might not be too easy to tell them apart.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-3.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is where, say, a \u003cem\u003eMultilayer Perceptron\u003c/em\u003e simply doesn't have the abstraction power to learn, and it's doomed to fail, horribly underfitting. \u003cem\u003eConvolutional Neural Networks\u003c/em\u003e are built based on the \u003ca target=\"_blank\" rel=\"nofollow noopener noreferrer\" href=\"https://link.springer.com/article/10.1007/BF00344251\"\u003eNeocognitron\u003c/a\u003e, which took hints from neuroscience and the hierarchical pattern recognition that the brain performs. These networks are able to extract features like this, and excel at the task. So much so that they oftentimes overfit badly and can't be used as is in the end - where we typically sacrifice some accuracy for the sake of generalization ability.\u003c/p\u003e\n\u003cp\u003eLet's train two different network architectures on the CIFAR10 and CIFAR100 dataset as an illustration of my point.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis is also where we'll be able to see how even when a network overfits, it's no guarantee that the network itself will definitely generalize well if simplified - it might not be able to generalize if simplified, though there is a tendency. The network might be \u003cstrong\u003eright\u003c/strong\u003e, but the \u003cstrong\u003edata\u003c/strong\u003e might not be enough.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn the case of CIFAR100 - just 500 images for training (and 100 for testing) per class is not enough for a simple CNN to \u003cem\u003ereally\u003c/em\u003e generalize well on the entire 100 classes, and we'll have to perform data augmentation to help it along. Even with data augmentation, we might not get a highly accurate network as there's just so much you can do to the data. If the same architecture performs well on CIFAR10, but not CIFAR100 - it means it simply can't distinguish from some of the more fine-grained details that make the difference between cylindrical objects that we call a \u0026quot;cup\u0026quot;, \u0026quot;can\u0026quot; and \u0026quot;bottle\u0026quot;, for instance.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe \u003ca rel=\"nofollow noopener noreferrer\" target=\"_blank\" href=\"https://paperswithcode.com/sota/image-classification-on-cifar-100\"\u003evast majority\u003c/a\u003e of advanced network architectures that achieve a high accuracy on the CIFAR100 dataset perform data augmentation or otherwise expand the training set.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eMost of them \u003cem\u003ehave to\u003c/em\u003e, and that's not a sign of bad engineering. In fact - the fact that we can expand these datasets and help networks generalize better is a sign of engineering ingenuity.\u003c/p\u003e\n\u003cp\u003eAdditionally, I'd invite any human to try and guess what these are, if they're convinced that image classification isn't too hard with images as small as 32x32:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-4.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIs \u003cem\u003eImage 4\u003c/em\u003e a few oranges? Ping pong balls? Egg yolks? Well, probably not egg yolks, but that requires prior knowledge on what \u0026quot;eggs\u0026quot; are and whether you're likely to find yolks sitting on the table, which a network won't have. Consider the amount of prior knowledge you may have regarding the world and how much it affects what you see.\u003c/p\u003e\n\u003ch4 id=\"importingthedata\"\u003eImporting the Data\u003c/h4\u003e\n\u003cp\u003eWe'll be using Keras as the deep learning library of choice, but you can follow along with other libraries or even your custom models if you're up for it.\u003c/p\u003e\n\u003cp\u003eBut first off, let's load it in, separate the data into a training, testing and validation set, normalizing the image values to \u003ccode\u003e0..1\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tensorflow \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e keras\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\n\u003cspan class=\"hljs-comment\"\u003e# Starting with CIFAR10\u003c/span\u003e\n(X_train_full, Y_train_full), (X_test, Y_test) = keras.datasets.cifar10.load_data()\n\nX_valid, X_train = X_train_full[:\u003cspan class=\"hljs-number\"\u003e5000\u003c/span\u003e]/\u003cspan class=\"hljs-number\"\u003e255.0\u003c/span\u003e, X_train_full[\u003cspan class=\"hljs-number\"\u003e5000\u003c/span\u003e:]/\u003cspan class=\"hljs-number\"\u003e255.0\u003c/span\u003e\nY_valid, Y_train = Y_train_full[:\u003cspan class=\"hljs-number\"\u003e5000\u003c/span\u003e], Y_train_full[\u003cspan class=\"hljs-number\"\u003e5000\u003c/span\u003e:]\n\nX_test = X_test/\u003cspan class=\"hljs-number\"\u003e255.0\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen, let's visualize some of the images in the dataset to get an idea of what we're up against:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig, ax = plt.subplots(\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e))\nax = ax.ravel()\n\n\u003cspan class=\"hljs-comment\"\u003e# Labels come as numbers of [0..9], so here are the class names for humans\u003c/span\u003e\nclass_names = [\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Airplane\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Automobile\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Bird\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Cat\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Deer\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Dog\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Frog\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Horse\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Ship\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Truck\u0026#x27;\u003c/span\u003e]\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e25\u003c/span\u003e):\n    ax[i].imshow(X_train_full[i])\n    ax[i].set_title(class_names[Y_train_full[i][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]])\n    ax[i].axis(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;off\u0026#x27;\u003c/span\u003e)\n    plt.subplots_adjust(wspace=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) \n\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-5.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch4 id=\"underfittingmultilayerperceptron\"\u003eUnderfitting Multilayer Perceptron\u003c/h4\u003e\n\u003cp\u003ePretty much no matter what we do, the MLP won't perform that well. It'll definitely reach some level of accuracy based on the raw sequences of information coming in - but this number is capped and probably won't be too high.\u003c/p\u003e\n\u003cp\u003eThe network will start overfitting at one point, learning the concrete sequences of data denoting images, but will still have low accuracy on the training set even when overfitting, which is the prime time to stop training it, since it simply can't fit the data well. \u003csmall\u003eTraining networks has a carbon footprint, you know.\u003c/small\u003e\u003c/p\u003e\n\u003cp\u003eLet's add in an \u003ccode\u003eEarlyStopping\u003c/code\u003e callback to avoid running the network beyond the point of common sense, and set the \u003ccode\u003eepochs\u003c/code\u003e to a number beyond what we'll run it for (so \u003ccode\u003eEarlyStopping\u003c/code\u003e can kick in).\u003c/p\u003e\n\u003cp\u003eWe'll use the Sequential API to add a couple of layers with \u003ccode\u003eBatchNormalization\u003c/code\u003e and a bit of \u003ccode\u003eDropout\u003c/code\u003e. They help with generalization and we want to at least \u003cem\u003etry\u003c/em\u003e to get this model to learn something.\u003c/p\u003e\n\u003cp\u003eThe main hyperparameters we can tweak here are the number of layers, their sizes, activation functions, kernel initializers and dropout rates, and here's a \u0026quot;decently\u0026quot; performing setup:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003echeckpoint = keras.callbacks.ModelCheckpoint(\u003cspan class=\"hljs-string\"\u003e\u0026quot;simple_dense.h5\u0026quot;\u003c/span\u003e, save_best_only=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nearly_stopping = keras.callbacks.EarlyStopping(patience=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, restore_best_weights=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nmodel = keras.Sequential([\n  keras.layers.Flatten(input_shape=[\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e75\u003c/span\u003e),\n    \n  keras.layers.Dense((\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e), activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;elu\u0026#x27;\u003c/span\u003e),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e),\n    \n  keras.layers.Dense((\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e), activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;elu\u0026#x27;\u003c/span\u003e),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e),\n    \n  keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e)\n])\n\nmodel.\u003cspan class=\"hljs-built_in\"\u003ecompile\u003c/span\u003e(loss=\u003cspan class=\"hljs-string\"\u003e\u0026quot;sparse_categorical_crossentropy\u0026quot;\u003c/span\u003e,\n              optimizer=keras.optimizers.Nadam(learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-4\u003c/span\u003e),\n              metrics=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;accuracy\u0026quot;\u003c/span\u003e])\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e, \n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint, early_stopping])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet's see if the starting hypothesis is true - it'll start out learning and generalizing to some extent but will end up having low accuracy on both the training set as well as the testing and validation set, resulting in an overall low accuracy.\u003c/p\u003e\n\u003cp\u003eFor CIFAR10, the network performs \u0026quot;okay\u0026quot;-ish:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e1407\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e1407\u003c/span\u003e [==============================] - 5s 3ms/step - loss: \u003cspan class=\"hljs-number\"\u003e1.9706\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.3108\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e1.6841\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.4100\u003c/span\u003e\n...\nEpoch \u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e1407\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e1407\u003c/span\u003e [==============================] - 4s 3ms/step - loss: \u003cspan class=\"hljs-number\"\u003e1.2927\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.5403\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e1.3893\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.5122\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet's take a look at the history of its learning:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003epd.DataFrame(history.history).plot()\nplt.show()\n\nmodel.evaluate(X_test, Y_test)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-6.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e313/313 [==============================] - 0s 926us/step - loss: 1.3836 - accuracy: 0.5058\n[1.383605718612671, 0.5058000087738037]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe overall accuracy gets up to ~50% and the network gets here pretty quickly and starts plateauing. 5/10 images being correctly classified sounds like tossing a coin, but remember that there are 10 classes here, so if it were randomly guessing, it'd on average guess a single image out of ten. Let's switch to the CIFAR100 dataset, which also necessitates a network with at least a tiny bit more power, as there are less training instances per class, as well as a vastly higher number of classes:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003echeckpoint = keras.callbacks.ModelCheckpoint(\u003cspan class=\"hljs-string\"\u003e\u0026quot;bigger_dense.h5\u0026quot;\u003c/span\u003e, save_best_only=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nearly_stopping = keras.callbacks.EarlyStopping(patience=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, restore_best_weights=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# Changing the loaded data\u003c/span\u003e\n(X_train_full, Y_train_full), (X_test, Y_test) = keras.datasets.cifar100.load_data()\n\n\u003cspan class=\"hljs-comment\"\u003e# Modify the model\u003c/span\u003e\nmodel1 = keras.Sequential([\n  keras.layers.Flatten(input_shape=[\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, kernel_initializer=\u003cspan class=\"hljs-string\"\u003e\u0026quot;he_normal\u0026quot;\u003c/span\u003e),\n    \n  keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e),\n\n  keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e)\n])\n\n\nmodel1.\u003cspan class=\"hljs-built_in\"\u003ecompile\u003c/span\u003e(loss=\u003cspan class=\"hljs-string\"\u003e\u0026quot;sparse_categorical_crossentropy\u0026quot;\u003c/span\u003e,\n              optimizer=keras.optimizers.Nadam(learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-4\u003c/span\u003e),\n              metrics=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;accuracy\u0026quot;\u003c/span\u003e])\n\nhistory = model1.fit(X_train, \n                    Y_train, \n                    epochs=\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e, \n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint, early_stopping])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe network performs fairly badly:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e1407\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e1407\u003c/span\u003e [==============================] - 13s 9ms/step - loss: \u003cspan class=\"hljs-number\"\u003e4.2260\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.0836\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e3.8682\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.1238\u003c/span\u003e\n...\nEpoch \u003cspan class=\"hljs-number\"\u003e24\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e1407\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e1407\u003c/span\u003e [==============================] - 12s 8ms/step - loss: \u003cspan class=\"hljs-number\"\u003e2.3598\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.4006\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e3.3577\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.2434\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd let's plot the history of its progress, as well as evaluate it on the testing set (which will likely perform as well as the validation set):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003epd.DataFrame(history.history).plot()\nplt.show()\n\nmodel.evaluate(X_test, Y_test)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-7.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e313/313 [==============================] - 0s 2ms/step - loss: 3.2681 - accuracy: 0.2408\n[3.2681326866149902, 0.24079999327659607]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs expected, the network wasn't able to grasp the data well. It ended up having an overfit accuracy of 40%, and an actual accuracy of ~24%.\u003c/p\u003e\n\u003cp\u003eThe accuracy capped at 40% - it wasn't \u003cem\u003ereally\u003c/em\u003e able to overfit the dataset, even if it overfit some parts of it that it was able to discern given the limited architecture. This model doesn't have the necessary entropic capacity required for it to truly overfit for the sake of my argument.\u003c/p\u003e\n\u003cp\u003eThis model and its architecture simply isn't well suited for this task - and while we could technically get it to (over)fit more, it'll still have issues on the long-run. For instance, let's turn it into a bigger network, which would theoretically let it recognize more complex patterns:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003emodel2 = keras.Sequential([\n  keras.layers.Flatten(input_shape=[\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e512\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, kernel_initializer=\u003cspan class=\"hljs-string\"\u003e\u0026quot;he_normal\u0026quot;\u003c/span\u003e),\n    \n  keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e),\n    \n  keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e),\n  keras.layers.BatchNormalization(),\n  keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e),\n\n  keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e)\n])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThough, this doesn't do much better at all:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch \u003cspan class=\"hljs-number\"\u003e24\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e1407\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e1407\u003c/span\u003e [==============================] - 28s 20ms/step - loss: \u003cspan class=\"hljs-number\"\u003e2.1202\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.4507\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e3.2796\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.2528\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt's much more complex (density explodes), yet it simply cannot extract much more:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003emodel1.summary()\nmodel2.summary()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eModel: \u003cspan class=\"hljs-string\"\u003e\u0026quot;sequential_17\u0026quot;\u003c/span\u003e\n...\nTotal params: \u003cspan class=\"hljs-number\"\u003e845\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e284\u003c/span\u003e\nTrainable params: \u003cspan class=\"hljs-number\"\u003e838\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e884\u003c/span\u003e\nNon-trainable params: \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e400\u003c/span\u003e\n_________________________________________________________________\nModel: \u003cspan class=\"hljs-string\"\u003e\u0026quot;sequential_18\u0026quot;\u003c/span\u003e\n...\nTotal params: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e764\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e324\u003c/span\u003e\nTrainable params: \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e757\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e412\u003c/span\u003e\nNon-trainable params: \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e912\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"overfittingconvolutionalneuralnetworkoncifar10\"\u003eOverfitting Convolutional Neural Network on CIFAR10\u003c/h4\u003e\n\u003cp\u003eNow, let's try doing something different. Switching to a CNN will significantly help with extracting features from the dataset, thereby allowing the model to \u003cem\u003etruly\u003c/em\u003e overfit, reaching much higher (illusory) accuracy.\u003c/p\u003e\n\u003cp\u003eWe'll kick out the \u003ccode\u003eEarlyStopping\u003c/code\u003e callback to let it do its thing. Additionally, we won't be using \u003ccode\u003eDropout\u003c/code\u003e layers, and instead try to force the network to learn the features through more layers.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e Outside of the context of trying to prove the argument, this would be horrible advice. This is opposite of what you'd want to do by the end. Dropout helps networks generalize better, by forcing the non-dropped neurons to pick up the slack. Forcing the network to learn through more layers it more likely to lead to an overfit model.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eThe reason I'm purposefully doing this is to allow the network to horribly overfit as \u003cstrong\u003ea sign of it's ability to actually discern features, before simplifying it and adding \u003ccode\u003eDropout\u003c/code\u003e to really allow it to generalize.\u003c/strong\u003e If it reaches high (illusory) accuracy, it can extract much more than the MLP model, which means we can start simplyfing it.\u003c/p\u003e\n\u003cp\u003eLet's once again use the Sequential API to build a CNN, firstly on the CIFAR10 dataset:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003echeckpoint = keras.callbacks.ModelCheckpoint(\u003cspan class=\"hljs-string\"\u003e\u0026quot;overcomplicated_cnn_cifar10.h5\u0026quot;\u003c/span\u003e, save_best_only=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, \n                        kernel_initializer=\u003cspan class=\"hljs-string\"\u003e\u0026quot;he_normal\u0026quot;\u003c/span\u003e, \n                        kernel_regularizer=keras.regularizers.l2(l=\u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e), \n                        padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e, \n                        input_shape=[\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    \n    keras.layers.Flatten(),    \n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e),\n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e)\n])\n\nmodel.\u003cspan class=\"hljs-built_in\"\u003ecompile\u003c/span\u003e(loss=\u003cspan class=\"hljs-string\"\u003e\u0026quot;sparse_categorical_crossentropy\u0026quot;\u003c/span\u003e,\n              optimizer=keras.optimizers.Adam(learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-3\u003c/span\u003e),\n              metrics=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;accuracy\u0026quot;\u003c/span\u003e])\n\nmodel.summary()\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e,\n                    batch_size=\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e,\n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAwesome, it overfit pretty quickly! Within just a few epochs, it started overfitting the data, and by epoch 31, it got up to 98%, with a lower validation accuracy:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e [==============================] - 149s 210ms/step - loss: \u003cspan class=\"hljs-number\"\u003e1.9561\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.4683\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e2.5060\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.3760\u003c/span\u003e\n...\nEpoch \u003cspan class=\"hljs-number\"\u003e31\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e [==============================] - 149s 211ms/step - loss: \u003cspan class=\"hljs-number\"\u003e0.0610\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.9841\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e1.0433\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.6958\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSince there are only 10 output classes, even though we tried overfitting it \u003cem\u003ea lot\u003c/em\u003e by creating an unnecessarily big CNN, the validation accuracy is still fairly high.\u003c/p\u003e\n\u003ch4 id=\"simplifyingtheconvolutionalneuralnetworkoncifar10\"\u003eSimplifying the Convolutional Neural Network on CIFAR10\u003c/h4\u003e\n\u003cp\u003eNow, let's simplify it to see how it'll fare with a more reasonable architecture. We'll add in \u003ccode\u003eBatchNormalization\u003c/code\u003e and \u003ccode\u003eDropout\u003c/code\u003e as both help with the generalization:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003echeckpoint = keras.callbacks.ModelCheckpoint(\u003cspan class=\"hljs-string\"\u003e\u0026quot;simplified_cnn_cifar10.h5\u0026quot;\u003c/span\u003e, save_best_only=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nearly_stopping = keras.callbacks.EarlyStopping(patience=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, restore_best_weights=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, kernel_initializer=\u003cspan class=\"hljs-string\"\u003e\u0026quot;he_normal\u0026quot;\u003c/span\u003e, kernel_regularizer=keras.regularizers.l2(l=\u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e), padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e, input_shape=[\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e),\n    \n    keras.layers.Flatten(),    \n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e),\n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e)\n])\n\nmodel.\u003cspan class=\"hljs-built_in\"\u003ecompile\u003c/span\u003e(loss=\u003cspan class=\"hljs-string\"\u003e\u0026quot;sparse_categorical_crossentropy\u0026quot;\u003c/span\u003e,\n              optimizer=keras.optimizers.Adam(learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-3\u003c/span\u003e),\n              metrics=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;accuracy\u0026quot;\u003c/span\u003e])\n\nmodel.summary()\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e,\n                    batch_size=\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e,\n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint, early_stopping])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis model has a (modest) count of 323,146 trainable parameters, compared to 1,579,178 from the previous CNN. How does it perform?\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e [==============================] - 91s 127ms/step - loss: \u003cspan class=\"hljs-number\"\u003e2.1327\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.3910\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e1.5495\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.5406\u003c/span\u003e\n...\nEpoch \u003cspan class=\"hljs-number\"\u003e52\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e [==============================] - 89s 127ms/step - loss: \u003cspan class=\"hljs-number\"\u003e0.4091\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.8648\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e0.4694\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.8500\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt actually achieves a pretty decent ~85% accuracy! Occam's Razor strikes again. Let's take a look at some of the results:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003ey_preds = model.predict(X_test)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(y_preds[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(np.argmax(y_preds[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]))\n\nfig, ax = plt.subplots(\u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e))\nax = ax.ravel()\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e36\u003c/span\u003e):\n    ax[i].imshow(X_test[i])\n    ax[i].set_title(\u003cspan class=\"hljs-string\"\u003e\u0026quot;Actual: %s\\nPred: %s\u0026quot;\u003c/span\u003e % (class_names[Y_test[i][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]], class_names[np.argmax(y_preds[i])]))\n    ax[i].axis(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;off\u0026#x27;\u003c/span\u003e)\n    plt.subplots_adjust(wspace=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n    \nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.amazonaws.com/s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-8.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe main misclassifications are two images in this small set - a dog was misclassified as a deer (respectable enough), but a closeup of an emu bird was classified as a cat (funny enough so we'll let it slide).\u003c/p\u003e\n\u003ch4 id=\"overfittingconvolutionalneuralnetworkoncifar100\"\u003eOverfitting Convolutional Neural Network on CIFAR100\u003c/h4\u003e\n\u003cp\u003eWhat happens when we go for the CIFAR100 dataset?\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003echeckpoint = keras.callbacks.ModelCheckpoint(\u003cspan class=\"hljs-string\"\u003e\u0026quot;overcomplicated_cnn_model_cifar100.h5\u0026quot;\u003c/span\u003e, save_best_only=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nearly_stopping = keras.callbacks.EarlyStopping(patience=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, restore_best_weights=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, kernel_initializer=\u003cspan class=\"hljs-string\"\u003e\u0026quot;he_normal\u0026quot;\u003c/span\u003e, kernel_regularizer=keras.regularizers.l2(l=\u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e), padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e, input_shape=[\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    \n    keras.layers.Flatten(),    \n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    \n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e)\n])\n\nmodel.\u003cspan class=\"hljs-built_in\"\u003ecompile\u003c/span\u003e(loss=\u003cspan class=\"hljs-string\"\u003e\u0026quot;sparse_categorical_crossentropy\u0026quot;\u003c/span\u003e,\n              optimizer=keras.optimizers.Adam(learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-3\u003c/span\u003e),\n              metrics=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;accuracy\u0026quot;\u003c/span\u003e])\n\nmodel.summary()\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e,\n                    batch_size=\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e,\n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e [==============================] - 97s 137ms/step - loss: \u003cspan class=\"hljs-number\"\u003e4.1752\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.1336\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e3.9696\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.1392\u003c/span\u003e\n...\nEpoch \u003cspan class=\"hljs-number\"\u003e42\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e [==============================] - 95s 135ms/step - loss: \u003cspan class=\"hljs-number\"\u003e0.1543\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.9572\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e4.1394\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.4458\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWonderful! ~96% accuracy on the training set! Don't mind the ~44% validation accuracy just yet. Let's simplify the model real quick to get it to generalize better.\u003c/p\u003e\n\u003ch4 id=\"failuretogeneralizeaftersimplification\"\u003eFailure to Generalize After Simplification\u003c/h4\u003e\n\u003cp\u003eAnd this is where it becomes clear that the ability to overfit doesn't \u003cstrong\u003eguarantee\u003c/strong\u003e that the model could generalize better when simplified. In the case of CIFAR100, there aren't many training instances per class, and this will likely prevent a simplified version of the previous model learn well. Let's try it out:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003echeckpoint = keras.callbacks.ModelCheckpoint(\u003cspan class=\"hljs-string\"\u003e\u0026quot;simplified_cnn_model_cifar100.h5\u0026quot;\u003c/span\u003e, save_best_only=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\nearly_stopping = keras.callbacks.EarlyStopping(patience=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, restore_best_weights=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, kernel_initializer=\u003cspan class=\"hljs-string\"\u003e\u0026quot;he_normal\u0026quot;\u003c/span\u003e, kernel_regularizer=keras.regularizers.l2(l=\u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e), padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e, input_shape=[\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e),\n    \n    keras.layers.Flatten(),    \n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e),\n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e)\n])\n\nmodel.\u003cspan class=\"hljs-built_in\"\u003ecompile\u003c/span\u003e(loss=\u003cspan class=\"hljs-string\"\u003e\u0026quot;sparse_categorical_crossentropy\u0026quot;\u003c/span\u003e,\n              optimizer=keras.optimizers.Adam(learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-3\u003c/span\u003e),\n              metrics=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;accuracy\u0026quot;\u003c/span\u003e])\n\nhistory = model.fit(X_train, \n                    Y_train, \n                    epochs=\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e,\n                    batch_size=\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e,\n                    validation_data=(X_valid, Y_valid),\n                    callbacks=[checkpoint, early_stopping])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e [==============================] - 96s 135ms/step - loss: \u003cspan class=\"hljs-number\"\u003e4.4432\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.1112\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e3.7893\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.1702\u003c/span\u003e\n...\nEpoch \u003cspan class=\"hljs-number\"\u003e48\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e/\u003cspan class=\"hljs-number\"\u003e704\u003c/span\u003e [==============================] - 92s 131ms/step - loss: \u003cspan class=\"hljs-number\"\u003e1.2550\u003c/span\u003e - accuracy: \u003cspan class=\"hljs-number\"\u003e0.6370\u003c/span\u003e - val_loss: \u003cspan class=\"hljs-number\"\u003e1.7147\u003c/span\u003e - val_accuracy: \u003cspan class=\"hljs-number\"\u003e0.5466\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt's plateauing and can't really get to generalize the data. In this case, it might not be the model's fault - maybe it's just right for the task, especially given the high accuracy on the CIFAR10 dataset, which has the same input shape and similar images in the dataset. It appears that the model can be reasonably accurate with the general shapes, but not the distinction between fine shapes.\u003c/p\u003e\n\u003cp\u003eThe simpler model actually performs better than the more complicated one in terms of validation accuracy - so the more complex CNN doesn't get these fine details much better at all. Here, the problem most likely lies in the fact that there are only 500 training images per class, which really isn't enough. In the more complex network, this leads to overfitting, because there's not enough diversity - when simplified to avoid overfitting, this causes underfitting as again, there's no diversity.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis is why the vast majority of the papers linked before, and the vast majority of networks augment the data of the CIFAR100 dataset.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIt's genuinely not a dataset for which it's easy to get high accuracy on, unlike the MNIST handwritten digits dataset, and a simple CNN like we're building probably won't cut it for high accuracy. Just remember the number of quite specific classes, how uninformative some of the images are, and \u003cem\u003ejust how much prior knowledge humans have to discern between these\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eLet's do our best by augmenting a few images and artificially expanding the training data, to at least try to get a higher accuracy. Keep in mind that the CIFAR100 is, again, a genuinely difficult dataset to get high accuracy on with simple models. The state of the art models use different and novel techniques to shave off errors, and many of these models aren't even \u003cem\u003eCNNs\u003c/em\u003e - they're \u003cem\u003eTransformers\u003c/em\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIf you'd like to take a look at the landscape of these models, \u003ca rel=\"nofollow noopener noreferrer\" target=\"_blank\" href=\"https://paperswithcode.com/sota/image-classification-on-cifar-100\"\u003ePapersWithCode\u003c/a\u003e has done a beautiful compilation of papers, source code and results.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4 id=\"dataaugmentationwithkerasimagedatageneratorclass\"\u003eData Augmentation with Keras' ImageDataGenerator Class\u003c/h4\u003e\n\u003cp\u003eWill data augmentation help? Usually, it does, but with a \u003cem\u003eserious\u003c/em\u003e lack of training data like we're facing, there's just so much you can do with random rotations, flipping, cropping, etc. If an architecture can't generalize well on a dataset, you'll likely boost it via data augmentation, but it probably won't be a whole lot.\u003c/p\u003e\n\u003cp\u003eThat being said, let's use Keras' \u003ccode\u003eImageDataGenerator\u003c/code\u003e class to try and generate some new training data with random changes, in hopes of improving the model's accuracy. If it does improve, it shouldn't be by a huge amount, and it'll likely get back to partially overfitting the dataset without an ability to either generalize well or fully overfit the data.\u003c/p\u003e\n\u003cp\u003eGiven the constant random variations in the data, the model is less likely to overfit on the same number of epochs, as the variations make it keep adjusting to \u0026quot;new\u0026quot; data. Let's run it for, say, 300 epochs, which is significantly more than the rest of the networks we've trained. This is possible without \u003cem\u003emajor\u003c/em\u003e overfitting, again, due to the random modifications made to the images while they're flowing in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003echeckpoint = keras.callbacks.ModelCheckpoint(\u003cspan class=\"hljs-string\"\u003e\u0026quot;augmented_cnn.h5\u0026quot;\u003c/span\u003e, save_best_only=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, kernel_initializer=\u003cspan class=\"hljs-string\"\u003e\u0026quot;he_normal\u0026quot;\u003c/span\u003e, kernel_regularizer=keras.regularizers.l2(l=\u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e), padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e, input_shape=[\u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e32\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e]),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e),\n    \n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.Conv2D(\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e, padding=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;same\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPooling2D(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e),\n    \n    keras.layers.Flatten(),    \n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e512\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;relu\u0026#x27;\u003c/span\u003e),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(\u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e),\n    keras.layers.Dense(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, activation=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;softmax\u0026#x27;\u003c/span\u003e)\n])\n\n    \ntrain_datagen = ImageDataGenerator(rotation_range=\u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e,\n        height_shift_range=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e,\n        width_shift_range=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e,\n        shear_range=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e,\n        zoom_range=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e,\n        horizontal_flip=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n        vertical_flip=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n        fill_mode=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;nearest\u0026#x27;\u003c/span\u003e)\n\nvalid_datagen = ImageDataGenerator()\n\ntrain_datagen.fit(X_train)\nvalid_datagen.fit(X_valid)\n\ntrain_generator = train_datagen.flow(X_train, Y_train, batch_size=\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e)\nvalid_generator = valid_datagen.flow(X_valid, Y_valid, batch_size=\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e)\n\nmodel.\u003cspan class=\"hljs-built_in\"\u003ecompile\u003c/span\u003e(loss=\u003cspan class=\"hljs-string\"\u003e\u0026quot;sparse_categorical_crossentropy\u0026quot;\u003c/span\u003e,\n              optimizer=keras.optimizers.Adam(learning_rate=\u003cspan class=\"hljs-number\"\u003e1e-3\u003c/span\u003e, decay=\u003cspan class=\"hljs-number\"\u003e1e-6\u003c/span\u003e),\n              metrics=[\u003cspan class=\"hljs-string\"\u003e\u0026quot;accuracy\u0026quot;\u003c/span\u003e])\n\nhistory = model.fit(train_generator, \n                    epochs=\u003cspan class=\"hljs-number\"\u003e300\u003c/span\u003e,\n                    batch_size=\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e,\n                    steps_per_epoch=\u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(X_train)//\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e,\n                    validation_data=valid_generator,\n                    callbacks=[checkpoint])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eEpoch 1/300\n351/351 [==============================] - 16s 44ms/step - loss: 5.3788 - accuracy: 0.0487 - val_loss: 5.3474 - val_accuracy: 0.0440\n...\nEpoch 300/300\n351/351 [==============================] - 15s 43ms/step - loss: 1.0571 - accuracy: 0.6895 - val_loss: 2.0005 - val_accuracy: 0.5532\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/overfitting-is-your-friend-not-your-foe-9.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe model is performing with ~55% on the validation set, and is still overfitting the data partially. The \u003ccode\u003eval_loss\u003c/code\u003e has stopped going down, and is quite rocky, even with a higher \u003ccode\u003ebatch_size\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThis network simply can't learn and fit the data with high accuracy, even though variations off it do have the entropic capacity to overfit the data.\u003c/p\u003e\n\u003ch3 id=\"conclusion\"\u003eConclusion?\u003c/h3\u003e\n\u003cp\u003eOverfitting isn't inherently a bad thing - it's just \u003cem\u003ea thing\u003c/em\u003e. No, you don't want overfit end-models, but it shouldn't be treated as the plague and can even be a good sign that a model could perform better given more data and a simplification step. This isn't guaranteed, by any means, and the CIFAR100 dataset has been used as an example of a dataset that's not easy to generalize well to.\u003c/p\u003e\n\u003cp\u003eThe point of this rambling is, again, not to be contrarian - but to incite discussion on the topic, which doesn't appear to be taking much place.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWho am I to make this claim?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eJust someone who sits home, practicing the craft, with a deep fascination towards tomorrow.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDo I have the ability to be wrong?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eVery much so.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eHow should you take this piece?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eTake it as you may - think for yourself if it makes sense or not. If you \u003cem\u003edon't\u003c/em\u003e think I'm out of my place for noting this, let me know. If you think I'm wrong on this - by all means, please feel let me know and don't mince your words. :)\u003c/p\u003e\n","parent_id":null,"type":"article","status":"published","visibility":"public","img_feature":null,"is_featured":false,"locale":"en","custom_excerpt":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"comment_id":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In this piece, the Friendly Overfitting Argument is laid out, as well as when it doesn't hold. Is overfitting in Machine Learning and Artificial Intelligence really as bad as people make it out to be?","read_time_min":null,"published_by":16,"published_at":1632220200000,"created_by":16,"updated_by":null,"created_at":1631998110024,"updated_at":1637694886912,"contributors":[{"id":16,"name":"David Landup","slug":"david","email":"thealduinmaster@gmail.com","password_hash":"$2a$10$W/oMJdUBSTeG3trWAHa1xO0pQruxuLgD/6hS7VuxPafcmAxeBXmVi","role_id":2,"img_profile":"//s3.stackabuse.com/media/users/865cd7d217ea11c9d9555c4f666e2d73.jpg","img_cover":null,"bio_md":"Entrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs. \n\nGreat passion for accessible education and promotion of reason, science, humanism, and progress.","bio_html":"\u003cp\u003eEntrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs.\u003c/p\u003e\n\u003cp\u003eGreat passion for accessible education and promotion of reason, science, humanism, and progress.\u003c/p\u003e\n","website":"https://www.upwork.com/freelancers/~017664e499a2766871","location":"Serbia","facebook":"","twitter":"","github":null,"status":"active","locale":null,"last_seen_at":1622229427000,"created_by":null,"updated_by":null,"created_at":1534532687000,"updated_at":1640861394795,"role":"editor","secret_token":"2a2be92558fae38f89cfbd0c6a4ba90c","is_email_confirmed":false,"_pivot_content_id":1085,"_pivot_user_id":16,"_pivot_role":"author","_pivot_sort_order":0}],"tags":[{"id":9,"name":"python","slug":"python","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1431358631000,"updated_at":1431358631000,"_pivot_content_id":1085,"_pivot_tag_id":9,"_pivot_sort_order":0},{"id":57,"name":"artificial intelligence","slug":"artificial-intelligence","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1442517465000,"updated_at":1442517465000,"_pivot_content_id":1085,"_pivot_tag_id":57,"_pivot_sort_order":1},{"id":75,"name":"machine learning","slug":"machine-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507305534000,"updated_at":1507305534000,"_pivot_content_id":1085,"_pivot_tag_id":75,"_pivot_sort_order":2},{"id":78,"name":"tensorflow","slug":"tensorflow","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1508009047000,"updated_at":1508009047000,"_pivot_content_id":1085,"_pivot_tag_id":78,"_pivot_sort_order":4},{"id":131,"name":"keras","slug":"keras","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1558780443000,"updated_at":1558780443000,"_pivot_content_id":1085,"_pivot_tag_id":131,"_pivot_sort_order":3}],"_pivot_tag_id":75,"_pivot_content_id":1085},{"id":905,"old_id":"605b4d5b2c5e1a08f1ec9cf7","uuid":"11e25962-ad7e-48c0-9523-c10857237d2a","title":"Random Projection: Theory and Implementation in Python with Scikit-Learn","slug":"random-projection-theory-and-implementation-in-python-with-scikit-learn","body_md":"### Introduction\n\nThis guide is an in-depth introduction to an unsupervised dimensionality reduction technique called **_Random Projections_**. A Random Projection can be used to reduce the complexity and size of data, making the data easier to process and visualize. It is also a preprocessing technique for input preparation to a classifier or a regressor.\n\n\u003e **Random Projection** is typically applied to highly-dimensional data, where other techniques such as **\u003ca target=\"_blank\" href=\"https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/\"\u003ePrincipal Component Analysis (PCA)\u003c/a\u003e** can't do the data justice.\n\nIn this guide, we'll delve into the details of **_Johnson-Lindenstrauss lemma_**, which lays the mathematical foundation of Random Projections. We'll also show how to perform Random Projection using Python's Scikit-Learn library, and use it to transform input data to a lower-dimensional space.\n\n\u003e **Theory is theory, and practice is practice**. As a practical illustration, we'll load the \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf\"\u003eReuters Corpus Volume I Dataset\u003c/a\u003e, and apply Gaussian Random Projection and Sparse Random Projection to it.\n\n### What is a Random Projection of a Dataset?\n\nPut simply:\n\n\u003e Random Projection is a method of **dimensionality reduction** and **data visualization** that simplifies the complexity of high-dimensional datasets.\n\nThe method generates a new dataset by taking the projection of each data point along a randomly chosen set of directions. The projection of a single data point onto a vector is mathematically equivalent to taking the **dot product of the point with the vector**.\n\n![random projections illustration](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-1.png)\n\nGiven a data matrix \\\\(X\\\\) of dimensions \\\\(mxn\\\\) and a \\\\(dxn\\\\) matrix \\\\(R\\\\) whose columns are the vectors representing random directions, the Random Projection of \\\\(X\\\\) is given by \\\\(X_p\\\\).\n\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmsub\u003e\n    \u003cmi\u003eX\u003c/mi\u003e\n    \u003cmi\u003ep\u003c/mi\u003e\n  \u003c/msub\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmi\u003eX\u003c/mi\u003e\n  \u003cmi\u003eR\u003c/mi\u003e\n\u003c/math\u003e\n\nEach vector representing a random direction, has dimensionality \\\\(n\\\\), which is the same as all data points of \\\\(X\\\\). If we take \\\\(d\\\\) random directions, then we end up with a \\\\(d\\\\) dimensional transformed dataset. For the purpose of this tutorial, we'll fix a few notations:\n\n- `m`: Total example points/samples of input data.\n- `n`: Total features/attributes of the input data. It is also the dimensionality of the original data.\n- `d`: Dimensionality of the transformed data.\n\nThe idea of Random Projections is very similar to **Principal Component Analysis (PCA)**, fundementally. However, in PCA, the projection matrix is computed via **eigenvectors**, which can be computationally expensive for large matrices.\n\n\u003e When performing Random Projection, the vectors are chosen randomly making it very efficient. The name _\"projection\"_ may be a little misleading as the vectors are chosen **randomly**, the transformed points are mathematically not true projections but close to being true projections.\n\nThe data with reduced dimensions is easier to work with. Not only can it be visualized but it can also be used in the pre-processing stage to reduce the size of the original data. \n\n### A Simple Example\n\nJust to understand how the transformation works, let's take the following simple example. \n\nSuppose our input matrix \\\\(X\\\\) is given by:\n\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmi\u003eX\u003c/mi\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmrow\u003e\n    \u003cmo\u003e[\u003c/mo\u003e\n    \u003cmtable rowspacing=\"4pt\" columnspacing=\"1em\"\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e3\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e2\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e2\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e3\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n    \u003c/mtable\u003e\n    \u003cmo\u003e]\u003c/mo\u003e\n  \u003c/mrow\u003e\n\u003c/math\u003e\n\nAnd the projection matrix is given by:\n\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmi\u003eR\u003c/mi\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmfrac\u003e\n    \u003cmn\u003e1\u003c/mn\u003e\n    \u003cmn\u003e2\u003c/mn\u003e\n  \u003c/mfrac\u003e\n  \u003cmrow\u003e\n    \u003cmo\u003e[\u003c/mo\u003e\n    \u003cmtable rowspacing=\"4pt\" columnspacing=\"1em\"\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e\u0026#x2212;\u003c!-- − --\u003e\u003c/mo\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e\u0026#x2212;\u003c!-- − --\u003e\u003c/mo\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n    \u003c/mtable\u003e\n    \u003cmo\u003e]\u003c/mo\u003e\n  \u003c/mrow\u003e\n\u003c/math\u003e\n\nThe projection of X onto R is:\n\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmsub\u003e\n    \u003cmi\u003eX\u003c/mi\u003e\n    \u003cmi\u003ep\u003c/mi\u003e\n  \u003c/msub\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmi\u003eX\u003c/mi\u003e\n  \u003cmi\u003eR\u003c/mi\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmfrac\u003e\n    \u003cmn\u003e1\u003c/mn\u003e\n    \u003cmn\u003e2\u003c/mn\u003e\n  \u003c/mfrac\u003e\n  \u003cmrow\u003e\n    \u003cmo\u003e[\u003c/mo\u003e\n    \u003cmtable rowspacing=\"4pt\" columnspacing=\"1em\"\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e6\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e4\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e4\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e2\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n    \u003c/mtable\u003e\n    \u003cmo\u003e]\u003c/mo\u003e\n  \u003c/mrow\u003e\n\u003c/math\u003e\n\n\u003e We started with three points in a **four-dimensional space**, and with clever matrix operations ended up with three transformed points in a **two-dimensional space**. \n\nNote, some important attributes of the projection matrix \\\\(R\\\\). Each column is a unit matrix, i.e., the norm of each column is one. Also, the dot product of all columns taken pairwise (in this case only column 1 and column 2) is zero, indicating that both column vectors are orthogonal to each other. \n\nThis makes the matrix, an **_Orthonormal Matrix_**. However, in case of the Random Projection technique, the projection matrix does not have to be a true orthonormal matrix when very high-dimensional data is involved.\n\nThe success of Random Projection is based on an _awesome_ mathematical finding known as **_Johnson-Lindenstrauss lemma_**, which is explained in detail in the following section!\n\n#### The Johnson-Lindenstrauss lemma\n\nThe Johnson-Lindenstrauss lemma is the mathematical basis for Random Projection:\n\n\u003e The Johnson-Lindenstrauss lemma states that if the data points lie in a **very high-dimensional space**, then projecting such points on simple **random directions preserves their pairwise distances**. \n\n_Preserving pairwise distances_ implies that the pairwise distances between points in the original space are the same or almost the same as the pairwise distance in the projected lower-dimensional space. \n\n\u003e Thus, the structure of data and clusters within data are maintained in a lower-dimensional space, while the complexity and size of data are reduced substantially. \n\nIn this guide, we refer to the difference in the actual and projected pairwise distances as the **_\"distortion\"_** in data, which is introduced due to its projection in a new space.\n\nJohnson-Lindenstrauss lemma also provides a **_\"safe\"_** measure of the number of dimensions to project the data points onto so that the error/distortion lies within a certain range, so finding the target number of dimensions is made easy.\n\nMathematically, given a pair of points \\\\((x_1,x_2)\\\\) and their corresponding projections \\\\((x_1',x_2')\\\\) defines an **_eps-embedding_**:\n\n$$\n(1 - \\epsilon) \\|x_1 - x_2\\|^2 \u003c \\|x_1' - x_2'\\|^2 \u003c (1 + \\epsilon) \\|x_1 - x_2\\|^2\n$$\n\nThe Johnson-Lindenstrauss lemma specifies the minimum dimensions of the lower-dimensional space so that the above _eps-embedding_ is maintained.\n\n#### Determining the Random Directions of the Projection Matrix\n\nTwo well-known methods for determining the projection matrix are:\n\n- **Gaussian Random Projection**: The projection matrix is constructed by choosing elements randomly from a Gaussian distribution with mean zero. \n\n- **Sparse Random Projection**: This is a comparatively simpler method, where each vector component is a value from the set {-k,0,+k}, where k is a constant. One simple scheme for generating the elements of this matrix, also called the `Achlioptas` method is to set \\\\(k=\\sqrt 3\\\\):\n\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmsub\u003e\n    \u003cmi\u003eR\u003c/mi\u003e\n    \u003cmrow class=\"MJX-TeXAtom-ORD\"\u003e\n      \u003cmi\u003ei\u003c/mi\u003e\n      \u003cmi\u003ej\u003c/mi\u003e\n    \u003c/mrow\u003e\n  \u003c/msub\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmsqrt\u003e\n    \u003cmn\u003e3\u003c/mn\u003e\n  \u003c/msqrt\u003e\n  \u003cmrow\u003e\n    \u003cmo\u003e{\u003c/mo\u003e\n    \u003cmtable columnalign=\"left left\" rowspacing=\".2em\" columnspacing=\"1em\" displaystyle=\"false\"\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e+\u003c/mo\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e1\u003c/mn\u003e\n            \u003cmn\u003e6\u003c/mn\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e2\u003c/mn\u003e\n            \u003cmn\u003e3\u003c/mn\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e\u0026#x2212;\u003c!-- − --\u003e\u003c/mo\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e1\u003c/mn\u003e\n            \u003cmn\u003e6\u003c/mn\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n    \u003c/mtable\u003e\n    \u003cmo fence=\"true\" stretchy=\"true\" symmetric=\"true\"\u003e\u003c/mo\u003e\n  \u003c/mrow\u003e\n\u003c/math\u003e\n\nThe method above is equivalent to choosing the numbers from `{+k,0,-k}` based on the outcome of the roll of a dice. If the dice score is _1_, then choose _+k_. If the dice score is in the range `[2,5]`, choose _0_, and choose _-k_ for a dice score of _6_.\n\nA more general method uses a `density` parameter to choose the Random Projection matrix. Setting \\\\(s=\\frac{1}{\\text{density}}\\\\), the elements of the Random Projection matrix are chosen as:\n\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmsub\u003e\n    \u003cmi\u003eR\u003c/mi\u003e\n    \u003cmrow class=\"MJX-TeXAtom-ORD\"\u003e\n      \u003cmi\u003ei\u003c/mi\u003e\n      \u003cmi\u003ej\u003c/mi\u003e\n    \u003c/mrow\u003e\n  \u003c/msub\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmrow\u003e\n    \u003cmo\u003e{\u003c/mo\u003e\n    \u003cmtable columnalign=\"left left\" rowspacing=\".2em\" columnspacing=\"1em\" displaystyle=\"false\"\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e+\u003c/mo\u003e\n          \u003cmsqrt\u003e\n            \u003cmfrac\u003e\n              \u003cmi\u003es\u003c/mi\u003e\n              \u003cmi\u003ed\u003c/mi\u003e\n            \u003c/mfrac\u003e\n          \u003c/msqrt\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e1\u003c/mn\u003e\n            \u003cmrow\u003e\n              \u003cmn\u003e2\u003c/mn\u003e\n              \u003cmi\u003es\u003c/mi\u003e\n            \u003c/mrow\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n          \u003cmo\u003e\u0026#x2212;\u003c!-- − --\u003e\u003c/mo\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e1\u003c/mn\u003e\n            \u003cmi\u003es\u003c/mi\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e\u0026#x2212;\u003c!-- − --\u003e\u003c/mo\u003e\n          \u003cmsqrt\u003e\n            \u003cmfrac\u003e\n              \u003cmi\u003es\u003c/mi\u003e\n              \u003cmi\u003ed\u003c/mi\u003e\n            \u003c/mfrac\u003e\n          \u003c/msqrt\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e1\u003c/mn\u003e\n            \u003cmrow\u003e\n              \u003cmn\u003e2\u003c/mn\u003e\n              \u003cmi\u003es\u003c/mi\u003e\n            \u003c/mrow\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n    \u003c/mtable\u003e\n    \u003cmo fence=\"true\" stretchy=\"true\" symmetric=\"true\"\u003e\u003c/mo\u003e\n  \u003c/mrow\u003e\n\u003c/math\u003e\n\nThe general recommendation is to set the `density` parameter to \\\\(\\frac{1}{\\sqrt n}\\\\). \n\nAs mentioned earlier, for both the Gaussian and sparse methods, the projection matrix is not a true orthonormal matrix. However, it has been shown that in high dimensional spaces, the randomly chosen matrix using either of the above two methods is **_close to_** an orthonormal matrix.\n\n### Random Projection Using Scikit-Learn\n\nThe Scikit-Learn library provides us with the `random_projection` module, that has three important classes/modules:\n\n- `johnson_lindenstrauss_min_dim()`: For determining the minimum number of dimensions of transformed data when given a sample size `m`.\n- `GaussianRandomProjection`: Performs Gaussian Random Projections.\n- `SparseRandomProjection`: Performs Sparse Random Projections.\n\nWe'll demonstrate all the above three in the sections below, but first let's import the classes and functions we'll be using:\n\n```python\nfrom sklearn.random_projection import SparseRandomProjection, johnson_lindenstrauss_min_dim\nfrom sklearn.random_projection import GaussianRandomProjection\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport sklearn.datasets as dt\nfrom sklearn.metrics.pairwise import euclidean_distances\n```\n\n#### Determining the Minimum Number of Dimensions Via Johnson Lindenstrauss lemma\n\nThe `johnson_lindenstrauss_min_dim()` function determines the minimum number of dimensions `d`, which the input data can be mapped to when given the number of examples `m`, and the `eps` or \\\\(\\epsilon\\\\) parameter. \n\nThe code below experiments with a different number of samples to determine the minimum size of the lower-dimensional space, which maintains a certain **_\"safe\"_** distortion of data.\n\nAdditionally, it plots `log(d)` against different values of `eps` for different sample sizes `m`. \n\nAn important thing to note is that the Johnson Lindenstrauss lemma determines the size of the lower-dimensional space \\\\(d\\\\) only based on the number of example points \\\\(m\\\\) in the input data. The number of attributes or features \\\\(n\\\\) of the original data is irrelevant:\n\n```python\neps = np.arange(0.001, 0.999, 0.01)\ncolors = ['b', 'g', 'm', 'c']\nm = [1e1, 1e3, 1e7, 1e10]\nfor i in range(4):\n    min_dim = johnson_lindenstrauss_min_dim(n_samples=m[i], eps=eps)\n    label = 'Total samples = ' + str(m[i])\n    plt.plot(eps, np.log10(min_dim), c=colors[i], label=label)\n    \nplt.xlabel('eps')\nplt.ylabel('log$_{10}$(d)')\nplt.axhline(y=3.5, color='k', linestyle=':')\nplt.legend()\nplt.show()\n```\n\n![how to determine size of lower dimensional space for random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-2.png)\n\nFrom the plot above, we can see that for small values of `eps`, `d` is quite large but decreases as `eps` approaches one. The dimensionality is below 3500 (the dotted black line) for mid to large values of `eps`. \n\n\u003e This shows that applying **Random Projections** only makes sense to **high-dimensional data**, of the order of thousands of features. In such cases, a high reduction in dimensionality can be achieved.\n\nRandom Projections are, therefore, very successful for text or image data, which involve a large number of input features, where Principal Component Analysis would\n\n#### Data Transformation\n\nPython includes the implementation of both Gaussian Random Projections and Sparse Random Projections in its `sklearn` library via the two classes `GaussianRandomProjection` and `SparseRandomProjection` respectively. Some important attributes for these classes are (the list is not exhaustive):\n\n- `n_components`: Number of dimensions of the transformed data. If it is set to `auto`, then the optimal dimensions are determined before projection\n- `eps`: The parameter of Johnson-Lindenstrauss lemma, which controls the number of dimensions so that the distortion in projected data is kept within a certain bound.\n- `density`: Only applicable for `SparseRandomProjection`. The default value is `auto`, which sets \\\\(s=\\frac{1}{\\sqrt n}\\\\) for the selection of the projection matrix. \n\nLike other dimensionality reduction classes of `sklearn`, both these classes include the standard `fit()` and `fit_transform()` methods. A notable set of attributes, which come in handy are:\n\n- `n_components`: The number of dimensions of the new space on which the data is projected.\n- `components_`: The transformation or projection matrix. \n- `density_`: Only applicable to `SparseRandomProjection`. It is the value of `density` based on which the elements of the projection matrix are computed.\n\n##### Random Projection with *GaussianRandomProjection*\n\nLet's start off with the `GaussianRandomProjection` class. The values of the projection matrix are plotted as a histogram and we can see that they follow a Gaussian distribution with mean zero. The size of the data matrix is reduced from 5000 to 3947:\n\n```python\nX_rand = np.random.RandomState(0).rand(100, 5000)\nproj_gauss = GaussianRandomProjection(random_state=0)\nX_transformed = proj_gauss.fit_transform(X_rand)\n\n# Print the size of the transformed data\nprint('Shape of transformed data: ' + str(X_transformed.shape))\n\n# Generate a histogram of the elements of the transformation matrix\nplt.hist(proj_gauss.components_.flatten())\nplt.title('Histogram of the flattened transformation matrix')\nplt.show()\n```\n\nThis code results in:\n\n```plaintext\nShape of transformed data: (100, 3947)\n```\n\n![gaussian random projection scikit learn](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-3.png)\n\n##### Random Projection with _SparseRandomProjection_\n\nThe code below demonstrates how data transformation can be made using a Sparse Random Projection. The entire transformation matrix is composed of three distinct values, whose frequency plot is also shown below. \n\nNote that the transformation matrix is a `SciPy` sparse `csr_matrix`. The following code accesses the non-zero values of the `csr_matrix` and stores them in `p`. Next, it uses `p` to get the counts of the elements of the sparse projection matrix:\n\n```python\nproj_sparse = SparseRandomProjection(random_state=0)\nX_transformed = proj_sparse.fit_transform(X_rand)\n\n# Print the size of the transformed data\nprint('Shape of transformed data: ' + str(X_transformed.shape))\n\n# Get data of the transformation matrix and store in p. \n# p consists of only 2 non-zero distinct values, i.e., pos and neg\n# pos and neg are determined below\np = proj_sparse.components_.data\ntotal_elements = proj_sparse.components_.shape[0] *\\\n                  proj_sparse.components_.shape[1]\npos = p[p\u003e0][0]\nneg = p[p\u003c0][0]\nprint('Shape of transformation matrix: '+ str(proj_sparse.components_.shape))\ncounts = (sum(p==neg), total_elements - len(p), sum(p==pos))\n# Histogram of the elements of the transformation matrix\nplt.bar([neg, 0, pos], counts, width=0.1)\nplt.xticks([neg, 0, pos])\nplt.suptitle('Histogram of flattened transformation matrix, ' + \n             'density = ' +\n             '{:.2f}'.format(proj_sparse.density_))\nplt.show()\n```\n\nThis results in:\n\n```plaintext\nShape of transformed data: (100, 3947)\nShape of transformation matrix: (3947, 5000)\n```\n\n![sparse random projections scikit learn](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-4.png)\n\nThe histogram is in agreement with the method of generating a sparse Random Projection matrix as discussed in the previous section. The zero is selected with probability (1-1/100 = 0.99), hence around 99% of values of this matrix are zero. Utilizing the data structures and routines for sparse matrices makes this transformation method very fast and efficient on large datasets.\n\n### Practical Random Projections With the _Reuters Corpus Volume 1 Dataset_\n\nThis section illustrates Random Projections on the \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf\"\u003eReuters Corpus Volume I Dataset\u003c/a\u003e. The dataset is freely accessible online, though for our purposes, it's easiest to looad via Scikit-Learn.\n\nThe \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://scikit-learn.org/stable/datasets/real_world.html#rcv1-dataset\"\u003e`sklearn.datasets`\u003c/a\u003e module contains a `fetch_rcv1()` function that downloads and imports the dataset.\n\n:::note\n**Note:** The dataset may take a few minutes to download, if you've never imported it beforehand through this method. Since there's no progress bar, it may appear as if the script is hanging without progressing further. Give it a bit of time, when you run it initially.\n:::\n\nThe RCV1 dataset is a multilabel dataset, i.e., each data point can belong to multiple classes at the same time, and consists of 103 classes. Each data point has a dimensionality of a **whopping 47,236**, making it an ideal case for applying fast and cheap Random Projections.\n\nTo demonstrate the effectiveness of Random Projections, and to keep things simple, we'll select 500 data points that belong to at least one of the first three classes. The `fetch_rcv1()` function retrieves the dataset and returns an object with data and targets, both of which are sparse `CSR` matrices from `SciPy`. \n\nLet's fetch the Reuters Corpus and prepare it for data transformation:\n\n```python\ntotal_points = 500\n# Fetch the dataset\ndat = dt.fetch_rcv1()\n# Select the sparse matrix's non-zero targets\ntarget_nz = dat.target.nonzero()\n# Select only indices of target_nz for data points that belong to \n# either of class 1,2,3\nind_class_123 = np.asarray(np.where((target_nz[1]==0) |\\\n                                    (target_nz[1]==1) |\\\n                                    (target_nz[1] == 2))).flatten()\n# Choose only 500 indices randomly\nnp.random.seed(0)\nind_class_123 = np.random.choice(ind_class_123, total_points, \n                                 replace=False)\n\n# Retreive the row indices of data matrix and target matrix\nrow_ind = target_nz[0][ind_class_123]\nX = dat.data[row_ind,:]\ny = np.array(dat.target[row_ind,0:3].todense())\n```\n\nAfter data preparation, we need a function that creates a visualization of the projected data. To have an idea of the quality of transformation, we can compute the following three matrices:\n\n- `dist_raw`: Matrix of the pairwise Euclidean distances of the actual data points.\n- `dist_transform`: Matrix of the pairwise Euclidean distances of the transformed data points.\n- `abs_diff`: Matrix of the absolute difference of `dist_raw` and `dist_actual`\n\nThe `abs_diff_dist` matrix is a good indicator of the quality of the data transformation. Close to zero or small values in this matrix indicate low distortion and a good transformation. We can directly display an image of this matrix or generate a histogram of its values to visually assess the transformation. We can also compute the average of all the values of this matrix to get a single quantitative measure for comparison.\n\nThe function `create_visualization()` creates three plots. The first graph is a scatter plot of projected points along the first two random directions. The second plot is an image of the absolute difference matrix and the third is the histogram of the values of the absolute difference matrix:\n\n```python\ndef create_visualization(X_transform, y, abs_diff):\n    fig,ax = plt.subplots(nrows=1, ncols=3, figsize=(20,7))\n\n    plt.subplot(131)\n    plt.scatter(X_transform[y[:,0]==1,0], X_transform[y[:,0]==1,1], c='r', alpha=0.4)\n    plt.scatter(X_transform[y[:,1]==1,0], X_transform[y[:,1]==1,1], c='b', alpha=0.4)\n    plt.scatter(X_transform[y[:,2]==1,0], X_transform[y[:,2]==1,1], c='g', alpha=0.4)\n    plt.legend(['Class 1', 'Class 2', 'Class 3'])\n    plt.title('Projected data along first two dimensions')\n\n    plt.subplot(132)\n    plt.imshow(abs_diff)\n    plt.colorbar()\n    plt.title('Visualization of absolute differences')\n\n    plt.subplot(133)\n    ax = plt.hist(abs_diff.flatten())\n    plt.title('Histogram of absolute differences')\n\n    fig.subplots_adjust(wspace=.3) \n```\n\n#### Reuters Dataset: Gaussian Random Projection\n\nLet's apply Gaussian Random Projection to the Reuters dataset. The code below runs a `for` loop for different `eps` values. If the minimum safe dimensions returned by `johnson_lindenstrauss_min_dim` is less than the actual data dimensions, then it calls the `fit_transform()` method of `GaussianRandomProjection`. The `create_visualization()` function is then called to create a visualization for that value of `eps`.\n\nAt every iteration, the code also stores the mean absolute difference and the percentage reduction in dimensionality achieved by Gaussian Random Projection:\n\n```python\nreduction_dim_gauss = []\neps_arr_gauss = []\nmean_abs_diff_gauss = []\nfor eps in np.arange(0.1, 0.999, 0.2):\n\n    min_dim = johnson_lindenstrauss_min_dim(n_samples=total_points, eps=eps)\n    if min_dim \u003e X.shape[1]:\n        continue\n    gauss_proj = GaussianRandomProjection(random_state=0, eps=eps)\n    X_transform = gauss_proj.fit_transform(X)\n    dist_raw = euclidean_distances(X)\n    dist_transform = euclidean_distances(X_transform)\n    abs_diff_gauss = abs(dist_raw - dist_transform) \n\n    create_visualization(X_transform, y, abs_diff_gauss)\n    plt.suptitle('eps = ' + '{:.2f}'.format(eps) + ', n_components = ' + str(X_transform.shape[1]))\n    \n    reduction_dim_gauss.append(100-X_transform.shape[1]/X.shape[1]*100)\n    eps_arr_gauss.append(eps)\n    mean_abs_diff_gauss.append(np.mean(abs_diff_gauss.flatten()))\n```\n\n\n![RCV1 dataset gaussian random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-5.png)\n\n\n![RCV1 dataset gaussian random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-6.png)\n\n\n![RCV1 dataset gaussian random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-7.png)\n\n\n![RCV1 dataset gaussian random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-8.png)\n\n\n![RCV1 dataset gaussian random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-9.png)\n\n\nThe images of the absolute difference matrix and its corresponding histogram indicate that most of the values are close to zero. Hence, a large majority of the pair of points maintain their actual distance in the low dimensional space, retaining the original structure of data.\n\nTo assess the quality of transformation, let's plot the mean absolute difference against `eps`. Also, the higher the value of `eps`, the greater the dimensionality reduction. Let's also plot the percentage reduction vs. `eps` in a second sub-plot:\n\n```python\nfig,ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\nplt.subplot(121)\nplt.plot(eps_arr_gauss, mean_abs_diff_gauss, marker='o', c='g')\nplt.xlabel('eps')\nplt.ylabel('Mean absolute difference')\n\nplt.subplot(122)\nplt.plot(eps_arr_gauss, reduction_dim_gauss, marker = 'o', c='m')\nplt.xlabel('eps')\nplt.ylabel('Percentage reduction in dimensionality')\n\nfig.subplots_adjust(wspace=.4) \nplt.suptitle('Assessing the Quality of Gaussian Random Projections')\nplt.show()\n```\n\n![RCV1 random projections reduction quality](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-10.png)\n\nWe can see that using Gaussian Random Projection we can reduce the dimensionality of data to **more than 99%**! Though, this *does* come at the cost of a higher distortion of data.\n\n#### Reuters Dataset: Sparse Random Projection\n\nWe can do a similar comparison with sparse Random Projection:\n\n```python\nreduction_dim_sparse = []\neps_arr_sparse = []\nmean_abs_diff_sparse = []\nfor eps in np.arange(0.1, 0.999, 0.2):\n\n    min_dim = johnson_lindenstrauss_min_dim(n_samples=total_points, eps=eps)\n    if min_dim \u003e X.shape[1]:\n        continue\n    sparse_proj = SparseRandomProjection(random_state=0, eps=eps, dense_output=1)\n    X_transform = sparse_proj.fit_transform(X)\n    dist_raw = euclidean_distances(X)\n    dist_transform = euclidean_distances(X_transform)\n    abs_diff_sparse = abs(dist_raw - dist_transform) \n\n    create_visualization(X_transform, y, abs_diff_sparse)\n    plt.suptitle('eps = ' + '{:.2f}'.format(eps) + ', n_components = ' + str(X_transform.shape[1]))\n    \n    reduction_dim_sparse.append(100-X_transform.shape[1]/X.shape[1]*100)\n    eps_arr_sparse.append(eps)\n    mean_abs_diff_sparse.append(np.mean(abs_diff_sparse.flatten()))\n```\n\n![RCV1 dataset sparse random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-11.png)\n\n![RCV1 dataset sparse random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-12.png)\n\n![RCV1 dataset sparse random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-13.png)\n\n![RCV1 dataset sparse random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-14.png)\n\n![RCV1 dataset sparse random projections](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-15.png)\n\nIn the case of Random Projection, the absolute difference matrix appears similar to the one of Gaussian projection. The projected data on the first two dimensions, however, has a more interesting pattern, with many points mapped on the coordinate axis. \n\nLet's also plot the mean absolute difference and percentage reduction in dimensionality for various values of the `eps` parameter:\n\n```python\nfig,ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\nplt.subplot(121)\nplt.plot(eps_arr_sparse, mean_abs_diff_sparse, marker='o', c='g')\nplt.xlabel('eps')\nplt.ylabel('Mean absolute difference')\n\nplt.subplot(122)\nplt.plot(eps_arr_sparse, reduction_dim_sparse, marker = 'o', c='m')\nplt.xlabel('eps')\nplt.ylabel('Percentage reduction in dimensionality')\n\nfig.subplots_adjust(wspace=.4) \nplt.suptitle('Assessing the Quality of Sparse Random Projections')\nplt.show()\n```\n\n![sparse random projections reduction quality](https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-17.png)\n\nThe trend of the two graphs is similar to that of a Gaussian Projection. However, the mean absolute difference for Gaussian Projection is lower than that of Random Projection.\n\n### Conclusions\n\nIn this guide, we discussed the details of two main types of Random Projections, i.e., Gaussian and sparse Random Projection. \n\nWe presented the details of the **_Johnson-Lindenstrauss lemma_**, the mathematical basis for these methods. We then showed how this method can be used to transform data using Python's `sklearn` library. \n\nWe also illustrated the two methods on a real-life \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf\"\u003eReuters Corpus Volume I Dataset\u003c/a\u003e. \n\nWe encourage the reader to try out this method in supervised classification or regression tasks at the pre-processing stage when dealing with very high-dimensional datasets.","body_html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThis guide is an in-depth introduction to an unsupervised dimensionality reduction technique called \u003cstrong\u003e\u003cem\u003eRandom Projections\u003c/em\u003e\u003c/strong\u003e. A Random Projection can be used to reduce the complexity and size of data, making the data easier to process and visualize. It is also a preprocessing technique for input preparation to a classifier or a regressor.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eRandom Projection\u003c/strong\u003e is typically applied to highly-dimensional data, where other techniques such as \u003cstrong\u003e\u003ca target=\"_blank\" href=\"https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/\"\u003ePrincipal Component Analysis (PCA)\u003c/a\u003e\u003c/strong\u003e can't do the data justice.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn this guide, we'll delve into the details of \u003cstrong\u003e\u003cem\u003eJohnson-Lindenstrauss lemma\u003c/em\u003e\u003c/strong\u003e, which lays the mathematical foundation of Random Projections. We'll also show how to perform Random Projection using Python's Scikit-Learn library, and use it to transform input data to a lower-dimensional space.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eTheory is theory, and practice is practice\u003c/strong\u003e. As a practical illustration, we'll load the \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf\"\u003eReuters Corpus Volume I Dataset\u003c/a\u003e, and apply Gaussian Random Projection and Sparse Random Projection to it.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"whatisarandomprojectionofadataset\"\u003eWhat is a Random Projection of a Dataset?\u003c/h3\u003e\n\u003cp\u003ePut simply:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRandom Projection is a method of \u003cstrong\u003edimensionality reduction\u003c/strong\u003e and \u003cstrong\u003edata visualization\u003c/strong\u003e that simplifies the complexity of high-dimensional datasets.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe method generates a new dataset by taking the projection of each data point along a randomly chosen set of directions. The projection of a single data point onto a vector is mathematically equivalent to taking the \u003cstrong\u003edot product of the point with the vector\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-1.png\" alt=\"random projections illustration\"\u003e\u003c/p\u003e\n\u003cp\u003eGiven a data matrix \\(X\\) of dimensions \\(mxn\\) and a \\(dxn\\) matrix \\(R\\) whose columns are the vectors representing random directions, the Random Projection of \\(X\\) is given by \\(X_p\\).\u003c/p\u003e\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmsub\u003e\n    \u003cmi\u003eX\u003c/mi\u003e\n    \u003cmi\u003ep\u003c/mi\u003e\n  \u003c/msub\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmi\u003eX\u003c/mi\u003e\n  \u003cmi\u003eR\u003c/mi\u003e\n\u003c/math\u003e\n\u003cp\u003eEach vector representing a random direction, has dimensionality \\(n\\), which is the same as all data points of \\(X\\). If we take \\(d\\) random directions, then we end up with a \\(d\\) dimensional transformed dataset. For the purpose of this tutorial, we'll fix a few notations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003em\u003c/code\u003e: Total example points/samples of input data.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003en\u003c/code\u003e: Total features/attributes of the input data. It is also the dimensionality of the original data.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ed\u003c/code\u003e: Dimensionality of the transformed data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe idea of Random Projections is very similar to \u003cstrong\u003ePrincipal Component Analysis (PCA)\u003c/strong\u003e, fundementally. However, in PCA, the projection matrix is computed via \u003cstrong\u003eeigenvectors\u003c/strong\u003e, which can be computationally expensive for large matrices.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWhen performing Random Projection, the vectors are chosen randomly making it very efficient. The name \u003cem\u003e\u0026quot;projection\u0026quot;\u003c/em\u003e may be a little misleading as the vectors are chosen \u003cstrong\u003erandomly\u003c/strong\u003e, the transformed points are mathematically not true projections but close to being true projections.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe data with reduced dimensions is easier to work with. Not only can it be visualized but it can also be used in the pre-processing stage to reduce the size of the original data.\u003c/p\u003e\n\u003ch3 id=\"asimpleexample\"\u003eA Simple Example\u003c/h3\u003e\n\u003cp\u003eJust to understand how the transformation works, let's take the following simple example.\u003c/p\u003e\n\u003cp\u003eSuppose our input matrix \\(X\\) is given by:\u003c/p\u003e\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmi\u003eX\u003c/mi\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmrow\u003e\n    \u003cmo\u003e[\u003c/mo\u003e\n    \u003cmtable rowspacing=\"4pt\" columnspacing=\"1em\"\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e3\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e2\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e2\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e3\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n    \u003c/mtable\u003e\n    \u003cmo\u003e]\u003c/mo\u003e\n  \u003c/mrow\u003e\n\u003c/math\u003e\n\u003cp\u003eAnd the projection matrix is given by:\u003c/p\u003e\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmi\u003eR\u003c/mi\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmfrac\u003e\n    \u003cmn\u003e1\u003c/mn\u003e\n    \u003cmn\u003e2\u003c/mn\u003e\n  \u003c/mfrac\u003e\n  \u003cmrow\u003e\n    \u003cmo\u003e[\u003c/mo\u003e\n    \u003cmtable rowspacing=\"4pt\" columnspacing=\"1em\"\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e\u0026#x2212;\u003c!-- − --\u003e\u003c/mo\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e\u0026#x2212;\u003c!-- − --\u003e\u003c/mo\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n    \u003c/mtable\u003e\n    \u003cmo\u003e]\u003c/mo\u003e\n  \u003c/mrow\u003e\n\u003c/math\u003e\n\u003cp\u003eThe projection of X onto R is:\u003c/p\u003e\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmsub\u003e\n    \u003cmi\u003eX\u003c/mi\u003e\n    \u003cmi\u003ep\u003c/mi\u003e\n  \u003c/msub\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmi\u003eX\u003c/mi\u003e\n  \u003cmi\u003eR\u003c/mi\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmfrac\u003e\n    \u003cmn\u003e1\u003c/mn\u003e\n    \u003cmn\u003e2\u003c/mn\u003e\n  \u003c/mfrac\u003e\n  \u003cmrow\u003e\n    \u003cmo\u003e[\u003c/mo\u003e\n    \u003cmtable rowspacing=\"4pt\" columnspacing=\"1em\"\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e6\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e4\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e4\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e2\u003c/mn\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n    \u003c/mtable\u003e\n    \u003cmo\u003e]\u003c/mo\u003e\n  \u003c/mrow\u003e\n\u003c/math\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWe started with three points in a \u003cstrong\u003efour-dimensional space\u003c/strong\u003e, and with clever matrix operations ended up with three transformed points in a \u003cstrong\u003etwo-dimensional space\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNote, some important attributes of the projection matrix \\(R\\). Each column is a unit matrix, i.e., the norm of each column is one. Also, the dot product of all columns taken pairwise (in this case only column 1 and column 2) is zero, indicating that both column vectors are orthogonal to each other.\u003c/p\u003e\n\u003cp\u003eThis makes the matrix, an \u003cstrong\u003e\u003cem\u003eOrthonormal Matrix\u003c/em\u003e\u003c/strong\u003e. However, in case of the Random Projection technique, the projection matrix does not have to be a true orthonormal matrix when very high-dimensional data is involved.\u003c/p\u003e\n\u003cp\u003eThe success of Random Projection is based on an \u003cem\u003eawesome\u003c/em\u003e mathematical finding known as \u003cstrong\u003e\u003cem\u003eJohnson-Lindenstrauss lemma\u003c/em\u003e\u003c/strong\u003e, which is explained in detail in the following section!\u003c/p\u003e\n\u003ch4 id=\"thejohnsonlindenstrausslemma\"\u003eThe Johnson-Lindenstrauss lemma\u003c/h4\u003e\n\u003cp\u003eThe Johnson-Lindenstrauss lemma is the mathematical basis for Random Projection:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe Johnson-Lindenstrauss lemma states that if the data points lie in a \u003cstrong\u003every high-dimensional space\u003c/strong\u003e, then projecting such points on simple \u003cstrong\u003erandom directions preserves their pairwise distances\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cem\u003ePreserving pairwise distances\u003c/em\u003e implies that the pairwise distances between points in the original space are the same or almost the same as the pairwise distance in the projected lower-dimensional space.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThus, the structure of data and clusters within data are maintained in a lower-dimensional space, while the complexity and size of data are reduced substantially.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn this guide, we refer to the difference in the actual and projected pairwise distances as the \u003cstrong\u003e\u003cem\u003e\u0026quot;distortion\u0026quot;\u003c/em\u003e\u003c/strong\u003e in data, which is introduced due to its projection in a new space.\u003c/p\u003e\n\u003cp\u003eJohnson-Lindenstrauss lemma also provides a \u003cstrong\u003e\u003cem\u003e\u0026quot;safe\u0026quot;\u003c/em\u003e\u003c/strong\u003e measure of the number of dimensions to project the data points onto so that the error/distortion lies within a certain range, so finding the target number of dimensions is made easy.\u003c/p\u003e\n\u003cp\u003eMathematically, given a pair of points \\((x_1,x_2)\\) and their corresponding projections \\((x_1',x_2')\\) defines an \u003cstrong\u003e\u003cem\u003eeps-embedding\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\n(1 - \\epsilon) |x_1 - x_2|^2 \u0026lt; |x_1' - x_2'|^2 \u0026lt; (1 + \\epsilon) |x_1 - x_2|^2\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cp\u003eThe Johnson-Lindenstrauss lemma specifies the minimum dimensions of the lower-dimensional space so that the above \u003cem\u003eeps-embedding\u003c/em\u003e is maintained.\u003c/p\u003e\n\u003ch4 id=\"determiningtherandomdirectionsoftheprojectionmatrix\"\u003eDetermining the Random Directions of the Projection Matrix\u003c/h4\u003e\n\u003cp\u003eTwo well-known methods for determining the projection matrix are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eGaussian Random Projection\u003c/strong\u003e: The projection matrix is constructed by choosing elements randomly from a Gaussian distribution with mean zero.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSparse Random Projection\u003c/strong\u003e: This is a comparatively simpler method, where each vector component is a value from the set {-k,0,+k}, where k is a constant. One simple scheme for generating the elements of this matrix, also called the \u003ccode\u003eAchlioptas\u003c/code\u003e method is to set \\(k=\\sqrt 3\\):\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmsub\u003e\n    \u003cmi\u003eR\u003c/mi\u003e\n    \u003cmrow class=\"MJX-TeXAtom-ORD\"\u003e\n      \u003cmi\u003ei\u003c/mi\u003e\n      \u003cmi\u003ej\u003c/mi\u003e\n    \u003c/mrow\u003e\n  \u003c/msub\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmsqrt\u003e\n    \u003cmn\u003e3\u003c/mn\u003e\n  \u003c/msqrt\u003e\n  \u003cmrow\u003e\n    \u003cmo\u003e{\u003c/mo\u003e\n    \u003cmtable columnalign=\"left left\" rowspacing=\".2em\" columnspacing=\"1em\" displaystyle=\"false\"\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e+\u003c/mo\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e1\u003c/mn\u003e\n            \u003cmn\u003e6\u003c/mn\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e2\u003c/mn\u003e\n            \u003cmn\u003e3\u003c/mn\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e\u0026#x2212;\u003c!-- − --\u003e\u003c/mo\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e1\u003c/mn\u003e\n            \u003cmn\u003e6\u003c/mn\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n    \u003c/mtable\u003e\n    \u003cmo fence=\"true\" stretchy=\"true\" symmetric=\"true\"\u003e\u003c/mo\u003e\n  \u003c/mrow\u003e\n\u003c/math\u003e\n\u003cp\u003eThe method above is equivalent to choosing the numbers from \u003ccode\u003e{+k,0,-k}\u003c/code\u003e based on the outcome of the roll of a dice. If the dice score is \u003cem\u003e1\u003c/em\u003e, then choose \u003cem\u003e+k\u003c/em\u003e. If the dice score is in the range \u003ccode\u003e[2,5]\u003c/code\u003e, choose \u003cem\u003e0\u003c/em\u003e, and choose \u003cem\u003e-k\u003c/em\u003e for a dice score of \u003cem\u003e6\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eA more general method uses a \u003ccode\u003edensity\u003c/code\u003e parameter to choose the Random Projection matrix. Setting \\(s=\\frac{1}{\\text{density}}\\), the elements of the Random Projection matrix are chosen as:\u003c/p\u003e\n\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"\u003e\n  \u003cmsub\u003e\n    \u003cmi\u003eR\u003c/mi\u003e\n    \u003cmrow class=\"MJX-TeXAtom-ORD\"\u003e\n      \u003cmi\u003ei\u003c/mi\u003e\n      \u003cmi\u003ej\u003c/mi\u003e\n    \u003c/mrow\u003e\n  \u003c/msub\u003e\n  \u003cmo\u003e=\u003c/mo\u003e\n  \u003cmrow\u003e\n    \u003cmo\u003e{\u003c/mo\u003e\n    \u003cmtable columnalign=\"left left\" rowspacing=\".2em\" columnspacing=\"1em\" displaystyle=\"false\"\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e+\u003c/mo\u003e\n          \u003cmsqrt\u003e\n            \u003cmfrac\u003e\n              \u003cmi\u003es\u003c/mi\u003e\n              \u003cmi\u003ed\u003c/mi\u003e\n            \u003c/mfrac\u003e\n          \u003c/msqrt\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e1\u003c/mn\u003e\n            \u003cmrow\u003e\n              \u003cmn\u003e2\u003c/mn\u003e\n              \u003cmi\u003es\u003c/mi\u003e\n            \u003c/mrow\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e0\u003c/mn\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmn\u003e1\u003c/mn\u003e\n          \u003cmo\u003e\u0026#x2212;\u003c!-- − --\u003e\u003c/mo\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e1\u003c/mn\u003e\n            \u003cmi\u003es\u003c/mi\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n      \u003cmtr\u003e\n        \u003cmtd\u003e\n          \u003cmo\u003e\u0026#x2212;\u003c!-- − --\u003e\u003c/mo\u003e\n          \u003cmsqrt\u003e\n            \u003cmfrac\u003e\n              \u003cmi\u003es\u003c/mi\u003e\n              \u003cmi\u003ed\u003c/mi\u003e\n            \u003c/mfrac\u003e\n          \u003c/msqrt\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmtext\u003e\u0026#xA0;with probability\u0026#xA0;\u003c/mtext\u003e\n        \u003c/mtd\u003e\n        \u003cmtd\u003e\n          \u003cmfrac\u003e\n            \u003cmn\u003e1\u003c/mn\u003e\n            \u003cmrow\u003e\n              \u003cmn\u003e2\u003c/mn\u003e\n              \u003cmi\u003es\u003c/mi\u003e\n            \u003c/mrow\u003e\n          \u003c/mfrac\u003e\n        \u003c/mtd\u003e\n      \u003c/mtr\u003e\n    \u003c/mtable\u003e\n    \u003cmo fence=\"true\" stretchy=\"true\" symmetric=\"true\"\u003e\u003c/mo\u003e\n  \u003c/mrow\u003e\n\u003c/math\u003e\n\u003cp\u003eThe general recommendation is to set the \u003ccode\u003edensity\u003c/code\u003e parameter to \\(\\frac{1}{\\sqrt n}\\).\u003c/p\u003e\n\u003cp\u003eAs mentioned earlier, for both the Gaussian and sparse methods, the projection matrix is not a true orthonormal matrix. However, it has been shown that in high dimensional spaces, the randomly chosen matrix using either of the above two methods is \u003cstrong\u003e\u003cem\u003eclose to\u003c/em\u003e\u003c/strong\u003e an orthonormal matrix.\u003c/p\u003e\n\u003ch3 id=\"randomprojectionusingscikitlearn\"\u003eRandom Projection Using Scikit-Learn\u003c/h3\u003e\n\u003cp\u003eThe Scikit-Learn library provides us with the \u003ccode\u003erandom_projection\u003c/code\u003e module, that has three important classes/modules:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ejohnson_lindenstrauss_min_dim()\u003c/code\u003e: For determining the minimum number of dimensions of transformed data when given a sample size \u003ccode\u003em\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eGaussianRandomProjection\u003c/code\u003e: Performs Gaussian Random Projections.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSparseRandomProjection\u003c/code\u003e: Performs Sparse Random Projections.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe'll demonstrate all the above three in the sections below, but first let's import the classes and functions we'll be using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.random_projection \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SparseRandomProjection, johnson_lindenstrauss_min_dim\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.random_projection \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e GaussianRandomProjection\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e matplotlib \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e dt\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.metrics.pairwise \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e euclidean_distances\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"determiningtheminimumnumberofdimensionsviajohnsonlindenstrausslemma\"\u003eDetermining the Minimum Number of Dimensions Via Johnson Lindenstrauss lemma\u003c/h4\u003e\n\u003cp\u003eThe \u003ccode\u003ejohnson_lindenstrauss_min_dim()\u003c/code\u003e function determines the minimum number of dimensions \u003ccode\u003ed\u003c/code\u003e, which the input data can be mapped to when given the number of examples \u003ccode\u003em\u003c/code\u003e, and the \u003ccode\u003eeps\u003c/code\u003e or \\(\\epsilon\\) parameter.\u003c/p\u003e\n\u003cp\u003eThe code below experiments with a different number of samples to determine the minimum size of the lower-dimensional space, which maintains a certain \u003cstrong\u003e\u003cem\u003e\u0026quot;safe\u0026quot;\u003c/em\u003e\u003c/strong\u003e distortion of data.\u003c/p\u003e\n\u003cp\u003eAdditionally, it plots \u003ccode\u003elog(d)\u003c/code\u003e against different values of \u003ccode\u003eeps\u003c/code\u003e for different sample sizes \u003ccode\u003em\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAn important thing to note is that the Johnson Lindenstrauss lemma determines the size of the lower-dimensional space \\(d\\) only based on the number of example points \\(m\\) in the input data. The number of attributes or features \\(n\\) of the original data is irrelevant:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eeps = np.arange(\u003cspan class=\"hljs-number\"\u003e0.001\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.999\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e)\ncolors = [\u003cspan class=\"hljs-string\"\u003e\u0026#x27;b\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;g\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;m\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;c\u0026#x27;\u003c/span\u003e]\nm = [\u003cspan class=\"hljs-number\"\u003e1e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1e3\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1e7\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1e10\u003c/span\u003e]\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e):\n    min_dim = johnson_lindenstrauss_min_dim(n_samples=m[i], eps=eps)\n    label = \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Total samples = \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(m[i])\n    plt.plot(eps, np.log10(min_dim), c=colors[i], label=label)\n    \nplt.xlabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;eps\u0026#x27;\u003c/span\u003e)\nplt.ylabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;log$_{10}$(d)\u0026#x27;\u003c/span\u003e)\nplt.axhline(y=\u003cspan class=\"hljs-number\"\u003e3.5\u003c/span\u003e, color=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;k\u0026#x27;\u003c/span\u003e, linestyle=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;:\u0026#x27;\u003c/span\u003e)\nplt.legend()\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-2.png\" alt=\"how to determine size of lower dimensional space for random projections\"\u003e\u003c/p\u003e\n\u003cp\u003eFrom the plot above, we can see that for small values of \u003ccode\u003eeps\u003c/code\u003e, \u003ccode\u003ed\u003c/code\u003e is quite large but decreases as \u003ccode\u003eeps\u003c/code\u003e approaches one. The dimensionality is below 3500 (the dotted black line) for mid to large values of \u003ccode\u003eeps\u003c/code\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis shows that applying \u003cstrong\u003eRandom Projections\u003c/strong\u003e only makes sense to \u003cstrong\u003ehigh-dimensional data\u003c/strong\u003e, of the order of thousands of features. In such cases, a high reduction in dimensionality can be achieved.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eRandom Projections are, therefore, very successful for text or image data, which involve a large number of input features, where Principal Component Analysis would\u003c/p\u003e\n\u003ch4 id=\"datatransformation\"\u003eData Transformation\u003c/h4\u003e\n\u003cp\u003ePython includes the implementation of both Gaussian Random Projections and Sparse Random Projections in its \u003ccode\u003esklearn\u003c/code\u003e library via the two classes \u003ccode\u003eGaussianRandomProjection\u003c/code\u003e and \u003ccode\u003eSparseRandomProjection\u003c/code\u003e respectively. Some important attributes for these classes are (the list is not exhaustive):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003en_components\u003c/code\u003e: Number of dimensions of the transformed data. If it is set to \u003ccode\u003eauto\u003c/code\u003e, then the optimal dimensions are determined before projection\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eeps\u003c/code\u003e: The parameter of Johnson-Lindenstrauss lemma, which controls the number of dimensions so that the distortion in projected data is kept within a certain bound.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edensity\u003c/code\u003e: Only applicable for \u003ccode\u003eSparseRandomProjection\u003c/code\u003e. The default value is \u003ccode\u003eauto\u003c/code\u003e, which sets \\(s=\\frac{1}{\\sqrt n}\\) for the selection of the projection matrix.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLike other dimensionality reduction classes of \u003ccode\u003esklearn\u003c/code\u003e, both these classes include the standard \u003ccode\u003efit()\u003c/code\u003e and \u003ccode\u003efit_transform()\u003c/code\u003e methods. A notable set of attributes, which come in handy are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003en_components\u003c/code\u003e: The number of dimensions of the new space on which the data is projected.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecomponents_\u003c/code\u003e: The transformation or projection matrix.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edensity_\u003c/code\u003e: Only applicable to \u003ccode\u003eSparseRandomProjection\u003c/code\u003e. It is the value of \u003ccode\u003edensity\u003c/code\u003e based on which the elements of the projection matrix are computed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5 id=\"randomprojectionwithgaussianrandomprojection\"\u003eRandom Projection with \u003cem\u003eGaussianRandomProjection\u003c/em\u003e\u003c/h5\u003e\n\u003cp\u003eLet's start off with the \u003ccode\u003eGaussianRandomProjection\u003c/code\u003e class. The values of the projection matrix are plotted as a histogram and we can see that they follow a Gaussian distribution with mean zero. The size of the data matrix is reduced from 5000 to 3947:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eX_rand = np.random.RandomState(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e).rand(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5000\u003c/span\u003e)\nproj_gauss = GaussianRandomProjection(random_state=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\nX_transformed = proj_gauss.fit_transform(X_rand)\n\n\u003cspan class=\"hljs-comment\"\u003e# Print the size of the transformed data\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Shape of transformed data: \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(X_transformed.shape))\n\n\u003cspan class=\"hljs-comment\"\u003e# Generate a histogram of the elements of the transformation matrix\u003c/span\u003e\nplt.hist(proj_gauss.components_.flatten())\nplt.title(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Histogram of the flattened transformation matrix\u0026#x27;\u003c/span\u003e)\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis code results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eShape of transformed data: (100, 3947)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-3.png\" alt=\"gaussian random projection scikit learn\"\u003e\u003c/p\u003e\n\u003ch5 id=\"randomprojectionwithsparserandomprojection\"\u003eRandom Projection with \u003cem\u003eSparseRandomProjection\u003c/em\u003e\u003c/h5\u003e\n\u003cp\u003eThe code below demonstrates how data transformation can be made using a Sparse Random Projection. The entire transformation matrix is composed of three distinct values, whose frequency plot is also shown below.\u003c/p\u003e\n\u003cp\u003eNote that the transformation matrix is a \u003ccode\u003eSciPy\u003c/code\u003e sparse \u003ccode\u003ecsr_matrix\u003c/code\u003e. The following code accesses the non-zero values of the \u003ccode\u003ecsr_matrix\u003c/code\u003e and stores them in \u003ccode\u003ep\u003c/code\u003e. Next, it uses \u003ccode\u003ep\u003c/code\u003e to get the counts of the elements of the sparse projection matrix:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eproj_sparse = SparseRandomProjection(random_state=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\nX_transformed = proj_sparse.fit_transform(X_rand)\n\n\u003cspan class=\"hljs-comment\"\u003e# Print the size of the transformed data\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Shape of transformed data: \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(X_transformed.shape))\n\n\u003cspan class=\"hljs-comment\"\u003e# Get data of the transformation matrix and store in p. \u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# p consists of only 2 non-zero distinct values, i.e., pos and neg\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# pos and neg are determined below\u003c/span\u003e\np = proj_sparse.components_.data\ntotal_elements = proj_sparse.components_.shape[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e] *\\\n                  proj_sparse.components_.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\npos = p[p\u0026gt;\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\nneg = p[p\u0026lt;\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e][\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Shape of transformation matrix: \u0026#x27;\u003c/span\u003e+ \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(proj_sparse.components_.shape))\ncounts = (\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(p==neg), total_elements - \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(p), \u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(p==pos))\n\u003cspan class=\"hljs-comment\"\u003e# Histogram of the elements of the transformation matrix\u003c/span\u003e\nplt.bar([neg, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, pos], counts, width=\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e)\nplt.xticks([neg, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, pos])\nplt.suptitle(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Histogram of flattened transformation matrix, \u0026#x27;\u003c/span\u003e + \n             \u003cspan class=\"hljs-string\"\u003e\u0026#x27;density = \u0026#x27;\u003c/span\u003e +\n             \u003cspan class=\"hljs-string\"\u003e\u0026#x27;{:.2f}\u0026#x27;\u003c/span\u003e.\u003cspan class=\"hljs-built_in\"\u003eformat\u003c/span\u003e(proj_sparse.density_))\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eShape of transformed data: (100, 3947)\nShape of transformation matrix: (3947, 5000)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-4.png\" alt=\"sparse random projections scikit learn\"\u003e\u003c/p\u003e\n\u003cp\u003eThe histogram is in agreement with the method of generating a sparse Random Projection matrix as discussed in the previous section. The zero is selected with probability (1-1/100 = 0.99), hence around 99% of values of this matrix are zero. Utilizing the data structures and routines for sparse matrices makes this transformation method very fast and efficient on large datasets.\u003c/p\u003e\n\u003ch3 id=\"practicalrandomprojectionswiththereuterscorpusvolume1dataset\"\u003ePractical Random Projections With the \u003cem\u003eReuters Corpus Volume 1 Dataset\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003eThis section illustrates Random Projections on the \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf\"\u003eReuters Corpus Volume I Dataset\u003c/a\u003e. The dataset is freely accessible online, though for our purposes, it's easiest to looad via Scikit-Learn.\u003c/p\u003e\n\u003cp\u003eThe \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://scikit-learn.org/stable/datasets/real_world.html#rcv1-dataset\"\u003e\u003ccode\u003esklearn.datasets\u003c/code\u003e\u003c/a\u003e module contains a \u003ccode\u003efetch_rcv1()\u003c/code\u003e function that downloads and imports the dataset.\u003c/p\u003e\n\n            \u003cdiv class=\"alert alert-note\"\u003e\n                \u003cdiv class=\"flex\"\u003e\n                    \n                        \u003cdiv class=\"flex-shrink-0 mr-3\"\u003e\n                            \u003cimg src=\"/assets/images/icon-information-circle-solid.svg\" class=\"icon\" aria-hidden=\"true\" /\u003e\n                        \u003c/div\u003e\n                        \n            \u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e The dataset may take a few minutes to download, if you've never imported it beforehand through this method. Since there's no progress bar, it may appear as if the script is hanging without progressing further. Give it a bit of time, when you run it initially.\u003c/p\u003e\n\n                \u003c/div\u003e\n            \u003c/div\u003e\n            \u003cp\u003eThe RCV1 dataset is a multilabel dataset, i.e., each data point can belong to multiple classes at the same time, and consists of 103 classes. Each data point has a dimensionality of a \u003cstrong\u003ewhopping 47,236\u003c/strong\u003e, making it an ideal case for applying fast and cheap Random Projections.\u003c/p\u003e\n\u003cp\u003eTo demonstrate the effectiveness of Random Projections, and to keep things simple, we'll select 500 data points that belong to at least one of the first three classes. The \u003ccode\u003efetch_rcv1()\u003c/code\u003e function retrieves the dataset and returns an object with data and targets, both of which are sparse \u003ccode\u003eCSR\u003c/code\u003e matrices from \u003ccode\u003eSciPy\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eLet's fetch the Reuters Corpus and prepare it for data transformation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003etotal_points = \u003cspan class=\"hljs-number\"\u003e500\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# Fetch the dataset\u003c/span\u003e\ndat = dt.fetch_rcv1()\n\u003cspan class=\"hljs-comment\"\u003e# Select the sparse matrix\u0026#x27;s non-zero targets\u003c/span\u003e\ntarget_nz = dat.target.nonzero()\n\u003cspan class=\"hljs-comment\"\u003e# Select only indices of target_nz for data points that belong to \u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# either of class 1,2,3\u003c/span\u003e\nind_class_123 = np.asarray(np.where((target_nz[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]==\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e) |\\\n                                    (target_nz[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]==\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e) |\\\n                                    (target_nz[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e] == \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e))).flatten()\n\u003cspan class=\"hljs-comment\"\u003e# Choose only 500 indices randomly\u003c/span\u003e\nnp.random.seed(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\nind_class_123 = np.random.choice(ind_class_123, total_points, \n                                 replace=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# Retreive the row indices of data matrix and target matrix\u003c/span\u003e\nrow_ind = target_nz[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e][ind_class_123]\nX = dat.data[row_ind,:]\ny = np.array(dat.target[row_ind,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e].todense())\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter data preparation, we need a function that creates a visualization of the projected data. To have an idea of the quality of transformation, we can compute the following three matrices:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003edist_raw\u003c/code\u003e: Matrix of the pairwise Euclidean distances of the actual data points.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edist_transform\u003c/code\u003e: Matrix of the pairwise Euclidean distances of the transformed data points.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eabs_diff\u003c/code\u003e: Matrix of the absolute difference of \u003ccode\u003edist_raw\u003c/code\u003e and \u003ccode\u003edist_actual\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003ccode\u003eabs_diff_dist\u003c/code\u003e matrix is a good indicator of the quality of the data transformation. Close to zero or small values in this matrix indicate low distortion and a good transformation. We can directly display an image of this matrix or generate a histogram of its values to visually assess the transformation. We can also compute the average of all the values of this matrix to get a single quantitative measure for comparison.\u003c/p\u003e\n\u003cp\u003eThe function \u003ccode\u003ecreate_visualization()\u003c/code\u003e creates three plots. The first graph is a scatter plot of projected points along the first two random directions. The second plot is an image of the absolute difference matrix and the third is the histogram of the values of the absolute difference matrix:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003ecreate_visualization\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eX_transform, y, abs_diff\u003c/span\u003e):\u003c/span\u003e\n    fig,ax = plt.subplots(nrows=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, ncols=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e7\u003c/span\u003e))\n\n    plt.subplot(\u003cspan class=\"hljs-number\"\u003e131\u003c/span\u003e)\n    plt.scatter(X_transform[y[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]==\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], X_transform[y[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]==\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], c=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;r\u0026#x27;\u003c/span\u003e, alpha=\u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e)\n    plt.scatter(X_transform[y[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]==\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], X_transform[y[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]==\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], c=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;b\u0026#x27;\u003c/span\u003e, alpha=\u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e)\n    plt.scatter(X_transform[y[:,\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]==\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], X_transform[y[:,\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]==\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], c=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;g\u0026#x27;\u003c/span\u003e, alpha=\u003cspan class=\"hljs-number\"\u003e0.4\u003c/span\u003e)\n    plt.legend([\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Class 1\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Class 2\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Class 3\u0026#x27;\u003c/span\u003e])\n    plt.title(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Projected data along first two dimensions\u0026#x27;\u003c/span\u003e)\n\n    plt.subplot(\u003cspan class=\"hljs-number\"\u003e132\u003c/span\u003e)\n    plt.imshow(abs_diff)\n    plt.colorbar()\n    plt.title(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Visualization of absolute differences\u0026#x27;\u003c/span\u003e)\n\n    plt.subplot(\u003cspan class=\"hljs-number\"\u003e133\u003c/span\u003e)\n    ax = plt.hist(abs_diff.flatten())\n    plt.title(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Histogram of absolute differences\u0026#x27;\u003c/span\u003e)\n\n    fig.subplots_adjust(wspace=\u003cspan class=\"hljs-number\"\u003e.3\u003c/span\u003e) \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"reutersdatasetgaussianrandomprojection\"\u003eReuters Dataset: Gaussian Random Projection\u003c/h4\u003e\n\u003cp\u003eLet's apply Gaussian Random Projection to the Reuters dataset. The code below runs a \u003ccode\u003efor\u003c/code\u003e loop for different \u003ccode\u003eeps\u003c/code\u003e values. If the minimum safe dimensions returned by \u003ccode\u003ejohnson_lindenstrauss_min_dim\u003c/code\u003e is less than the actual data dimensions, then it calls the \u003ccode\u003efit_transform()\u003c/code\u003e method of \u003ccode\u003eGaussianRandomProjection\u003c/code\u003e. The \u003ccode\u003ecreate_visualization()\u003c/code\u003e function is then called to create a visualization for that value of \u003ccode\u003eeps\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAt every iteration, the code also stores the mean absolute difference and the percentage reduction in dimensionality achieved by Gaussian Random Projection:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003ereduction_dim_gauss = []\neps_arr_gauss = []\nmean_abs_diff_gauss = []\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e eps \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e np.arange(\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.999\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e):\n\n    min_dim = johnson_lindenstrauss_min_dim(n_samples=total_points, eps=eps)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e min_dim \u0026gt; X.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]:\n        \u003cspan class=\"hljs-keyword\"\u003econtinue\u003c/span\u003e\n    gauss_proj = GaussianRandomProjection(random_state=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, eps=eps)\n    X_transform = gauss_proj.fit_transform(X)\n    dist_raw = euclidean_distances(X)\n    dist_transform = euclidean_distances(X_transform)\n    abs_diff_gauss = \u003cspan class=\"hljs-built_in\"\u003eabs\u003c/span\u003e(dist_raw - dist_transform) \n\n    create_visualization(X_transform, y, abs_diff_gauss)\n    plt.suptitle(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;eps = \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-string\"\u003e\u0026#x27;{:.2f}\u0026#x27;\u003c/span\u003e.\u003cspan class=\"hljs-built_in\"\u003eformat\u003c/span\u003e(eps) + \u003cspan class=\"hljs-string\"\u003e\u0026#x27;, n_components = \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(X_transform.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]))\n    \n    reduction_dim_gauss.append(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e-X_transform.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]/X.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]*\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e)\n    eps_arr_gauss.append(eps)\n    mean_abs_diff_gauss.append(np.mean(abs_diff_gauss.flatten()))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-5.png\" alt=\"RCV1 dataset gaussian random projections\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-6.png\" alt=\"RCV1 dataset gaussian random projections\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-7.png\" alt=\"RCV1 dataset gaussian random projections\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-8.png\" alt=\"RCV1 dataset gaussian random projections\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-9.png\" alt=\"RCV1 dataset gaussian random projections\"\u003e\u003c/p\u003e\n\u003cp\u003eThe images of the absolute difference matrix and its corresponding histogram indicate that most of the values are close to zero. Hence, a large majority of the pair of points maintain their actual distance in the low dimensional space, retaining the original structure of data.\u003c/p\u003e\n\u003cp\u003eTo assess the quality of transformation, let's plot the mean absolute difference against \u003ccode\u003eeps\u003c/code\u003e. Also, the higher the value of \u003ccode\u003eeps\u003c/code\u003e, the greater the dimensionality reduction. Let's also plot the percentage reduction vs. \u003ccode\u003eeps\u003c/code\u003e in a second sub-plot:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig,ax = plt.subplots(nrows=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, ncols=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e))\nplt.subplot(\u003cspan class=\"hljs-number\"\u003e121\u003c/span\u003e)\nplt.plot(eps_arr_gauss, mean_abs_diff_gauss, marker=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;o\u0026#x27;\u003c/span\u003e, c=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;g\u0026#x27;\u003c/span\u003e)\nplt.xlabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;eps\u0026#x27;\u003c/span\u003e)\nplt.ylabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Mean absolute difference\u0026#x27;\u003c/span\u003e)\n\nplt.subplot(\u003cspan class=\"hljs-number\"\u003e122\u003c/span\u003e)\nplt.plot(eps_arr_gauss, reduction_dim_gauss, marker = \u003cspan class=\"hljs-string\"\u003e\u0026#x27;o\u0026#x27;\u003c/span\u003e, c=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;m\u0026#x27;\u003c/span\u003e)\nplt.xlabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;eps\u0026#x27;\u003c/span\u003e)\nplt.ylabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Percentage reduction in dimensionality\u0026#x27;\u003c/span\u003e)\n\nfig.subplots_adjust(wspace=\u003cspan class=\"hljs-number\"\u003e.4\u003c/span\u003e) \nplt.suptitle(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Assessing the Quality of Gaussian Random Projections\u0026#x27;\u003c/span\u003e)\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-10.png\" alt=\"RCV1 random projections reduction quality\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can see that using Gaussian Random Projection we can reduce the dimensionality of data to \u003cstrong\u003emore than 99%\u003c/strong\u003e! Though, this \u003cem\u003edoes\u003c/em\u003e come at the cost of a higher distortion of data.\u003c/p\u003e\n\u003ch4 id=\"reutersdatasetsparserandomprojection\"\u003eReuters Dataset: Sparse Random Projection\u003c/h4\u003e\n\u003cp\u003eWe can do a similar comparison with sparse Random Projection:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003ereduction_dim_sparse = []\neps_arr_sparse = []\nmean_abs_diff_sparse = []\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e eps \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e np.arange(\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.999\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e):\n\n    min_dim = johnson_lindenstrauss_min_dim(n_samples=total_points, eps=eps)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e min_dim \u0026gt; X.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]:\n        \u003cspan class=\"hljs-keyword\"\u003econtinue\u003c/span\u003e\n    sparse_proj = SparseRandomProjection(random_state=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, eps=eps, dense_output=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n    X_transform = sparse_proj.fit_transform(X)\n    dist_raw = euclidean_distances(X)\n    dist_transform = euclidean_distances(X_transform)\n    abs_diff_sparse = \u003cspan class=\"hljs-built_in\"\u003eabs\u003c/span\u003e(dist_raw - dist_transform) \n\n    create_visualization(X_transform, y, abs_diff_sparse)\n    plt.suptitle(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;eps = \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-string\"\u003e\u0026#x27;{:.2f}\u0026#x27;\u003c/span\u003e.\u003cspan class=\"hljs-built_in\"\u003eformat\u003c/span\u003e(eps) + \u003cspan class=\"hljs-string\"\u003e\u0026#x27;, n_components = \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(X_transform.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]))\n    \n    reduction_dim_sparse.append(\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e-X_transform.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]/X.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]*\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e)\n    eps_arr_sparse.append(eps)\n    mean_abs_diff_sparse.append(np.mean(abs_diff_sparse.flatten()))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-11.png\" alt=\"RCV1 dataset sparse random projections\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-12.png\" alt=\"RCV1 dataset sparse random projections\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-13.png\" alt=\"RCV1 dataset sparse random projections\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-14.png\" alt=\"RCV1 dataset sparse random projections\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-15.png\" alt=\"RCV1 dataset sparse random projections\"\u003e\u003c/p\u003e\n\u003cp\u003eIn the case of Random Projection, the absolute difference matrix appears similar to the one of Gaussian projection. The projected data on the first two dimensions, however, has a more interesting pattern, with many points mapped on the coordinate axis.\u003c/p\u003e\n\u003cp\u003eLet's also plot the mean absolute difference and percentage reduction in dimensionality for various values of the \u003ccode\u003eeps\u003c/code\u003e parameter:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig,ax = plt.subplots(nrows=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, ncols=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e))\nplt.subplot(\u003cspan class=\"hljs-number\"\u003e121\u003c/span\u003e)\nplt.plot(eps_arr_sparse, mean_abs_diff_sparse, marker=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;o\u0026#x27;\u003c/span\u003e, c=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;g\u0026#x27;\u003c/span\u003e)\nplt.xlabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;eps\u0026#x27;\u003c/span\u003e)\nplt.ylabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Mean absolute difference\u0026#x27;\u003c/span\u003e)\n\nplt.subplot(\u003cspan class=\"hljs-number\"\u003e122\u003c/span\u003e)\nplt.plot(eps_arr_sparse, reduction_dim_sparse, marker = \u003cspan class=\"hljs-string\"\u003e\u0026#x27;o\u0026#x27;\u003c/span\u003e, c=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;m\u0026#x27;\u003c/span\u003e)\nplt.xlabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;eps\u0026#x27;\u003c/span\u003e)\nplt.ylabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Percentage reduction in dimensionality\u0026#x27;\u003c/span\u003e)\n\nfig.subplots_adjust(wspace=\u003cspan class=\"hljs-number\"\u003e.4\u003c/span\u003e) \nplt.suptitle(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Assessing the Quality of Sparse Random Projections\u0026#x27;\u003c/span\u003e)\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/random-projection-theory-and-implementation-in-python-with-scikit-learn-17.png\" alt=\"sparse random projections reduction quality\"\u003e\u003c/p\u003e\n\u003cp\u003eThe trend of the two graphs is similar to that of a Gaussian Projection. However, the mean absolute difference for Gaussian Projection is lower than that of Random Projection.\u003c/p\u003e\n\u003ch3 id=\"conclusions\"\u003eConclusions\u003c/h3\u003e\n\u003cp\u003eIn this guide, we discussed the details of two main types of Random Projections, i.e., Gaussian and sparse Random Projection.\u003c/p\u003e\n\u003cp\u003eWe presented the details of the \u003cstrong\u003e\u003cem\u003eJohnson-Lindenstrauss lemma\u003c/em\u003e\u003c/strong\u003e, the mathematical basis for these methods. We then showed how this method can be used to transform data using Python's \u003ccode\u003esklearn\u003c/code\u003e library.\u003c/p\u003e\n\u003cp\u003eWe also illustrated the two methods on a real-life \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf\"\u003eReuters Corpus Volume I Dataset\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe encourage the reader to try out this method in supervised classification or regression tasks at the pre-processing stage when dealing with very high-dimensional datasets.\u003c/p\u003e\n","parent_id":null,"type":"article","status":"published","visibility":"public","img_feature":null,"is_featured":false,"locale":"en","custom_excerpt":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"comment_id":"605b4d5b2c5e1a08f1ec9cf7","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In this guide, we'll be taking a look at the theory and implementation behind Random Projections in Python - Gaussian and Sparse Random Projections, as well as a practical hands-on tutorial using a real-life dataset.","read_time_min":16,"published_by":16,"published_at":1630405800000,"created_by":null,"updated_by":null,"created_at":1616596315000,"updated_at":1634839863974,"contributors":[{"id":16,"name":"David Landup","slug":"david","email":"thealduinmaster@gmail.com","password_hash":"$2a$10$W/oMJdUBSTeG3trWAHa1xO0pQruxuLgD/6hS7VuxPafcmAxeBXmVi","role_id":2,"img_profile":"//s3.stackabuse.com/media/users/865cd7d217ea11c9d9555c4f666e2d73.jpg","img_cover":null,"bio_md":"Entrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs. \n\nGreat passion for accessible education and promotion of reason, science, humanism, and progress.","bio_html":"\u003cp\u003eEntrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs.\u003c/p\u003e\n\u003cp\u003eGreat passion for accessible education and promotion of reason, science, humanism, and progress.\u003c/p\u003e\n","website":"https://www.upwork.com/freelancers/~017664e499a2766871","location":"Serbia","facebook":"","twitter":"","github":null,"status":"active","locale":null,"last_seen_at":1622229427000,"created_by":null,"updated_by":null,"created_at":1534532687000,"updated_at":1640861394795,"role":"editor","secret_token":"2a2be92558fae38f89cfbd0c6a4ba90c","is_email_confirmed":false,"_pivot_content_id":905,"_pivot_user_id":16,"_pivot_role":"editor","_pivot_sort_order":1},{"id":69,"name":"Mehreen Saeed","slug":"mehreen","email":"courses.at.fast@gmail.com","password_hash":"$2a$10$SyXn0J3GubHzykx/cHqQSOwdP9JM6UHU3DA5R5mmNZvsTZ.zza97C","role_id":4,"img_profile":"//s3.stackabuse.com/media/users/89430606bd6f853f01889aa0bd28a437.png","img_cover":"//s3.stackabuse.com/media/users/7356adc6f29d109e0cfcaa44a662b2a1.JPG","bio_md":"I am an educator and I love mathematics and data science!","bio_html":"\u003cp\u003eI am an educator and I love mathematics and data science!\u003c/p\u003e\n","website":null,"location":"Pakistan","facebook":null,"twitter":null,"github":null,"status":"active","locale":null,"last_seen_at":1617038327000,"created_by":null,"updated_by":null,"created_at":1599395951000,"updated_at":1629840517435,"role":"author","secret_token":"7c90f1f392be6c321d7e2109f9642055","is_email_confirmed":false,"_pivot_content_id":905,"_pivot_user_id":69,"_pivot_role":"author","_pivot_sort_order":0}],"tags":[{"id":9,"name":"python","slug":"python","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1431358631000,"updated_at":1431358631000,"_pivot_content_id":905,"_pivot_tag_id":9,"_pivot_sort_order":0},{"id":57,"name":"artificial intelligence","slug":"artificial-intelligence","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1442517465000,"updated_at":1442517465000,"_pivot_content_id":905,"_pivot_tag_id":57,"_pivot_sort_order":3},{"id":75,"name":"machine learning","slug":"machine-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507305534000,"updated_at":1507305534000,"_pivot_content_id":905,"_pivot_tag_id":75,"_pivot_sort_order":2},{"id":76,"name":"scikit-learn","slug":"scikit-learn","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507308143000,"updated_at":1507308143000,"_pivot_content_id":905,"_pivot_tag_id":76,"_pivot_sort_order":5},{"id":106,"name":"data science","slug":"data-science","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1536865458000,"updated_at":1536865458000,"_pivot_content_id":905,"_pivot_tag_id":106,"_pivot_sort_order":1},{"id":140,"name":"numpy","slug":"numpy","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1564339082000,"updated_at":1564339082000,"_pivot_content_id":905,"_pivot_tag_id":140,"_pivot_sort_order":4}],"_pivot_tag_id":75,"_pivot_content_id":905},{"id":878,"old_id":"602fd8d22c5e1a08f1ec8550","uuid":"f0f43f62-0046-4b63-a27e-74a5cbd690ec","title":"Self-Organizing Maps: Theory and Implementation in Python with NumPy","slug":"self-organizing-maps-theory-and-implementation-in-python-with-numpy","body_md":"### Introduction\n\nIn this guide, we'll be taking a look at an unsupervised learning model, known as a **_Self-Organizing Map (SOM)_**, as well as its implementation in Python. We'll be using an **RGB Color** example to train the SOM and demonstrate its performance and typical usage.\n\n### Self-Organizing Maps: A General Introduction\n\nA _Self-Organizing Map_ was first introduced by \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://link.springer.com/article/10.1007/BF00337288\"\u003eTeuvo Kohonen in 1982\u003c/a\u003e and is also sometimes known as a **_Kohonen map_**. It is a special type of an _artificial neural network_, which builds a map of the training data. The map is generally a 2D rectangular grid of weights but can be extended to a 3D or higher dimensional model. Other grid structures like hexagonal grids are also possible.\n\nAn SOM is mainly used for data visualization and provides a quick visual summary of the training instances. In a 2D rectangular grid, each cell is represented by a weight vector. For a trained SOM, each cell weight represents a summary of a few training examples. Cells in the close vicinity of each other have similar weights, and like examples can be mapped to cells in a small neighborhood of each other. \n\nThe figure below is a rough illustration of the structure of the SOM:\n\n![self-organizing map illustration](https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-0.png)\n\nAn SOM is trained using **competitive learning**.\n\n\u003e **Competitive Learning** is a form of unsupervised learning, where constituent elements compete to produce a satisfying result, and only one gets to win the competition.\n\nWhen a training example is input into the grid, the **_Best Matching Unit (BMU)_** is determined (competition winner). The BMU is the cell whose weights are closest to the training example. \n\nNext, the BMU's weights and weights of the cells neighboring the BMU, are adapted to move closer to the input training instance. While there are other valid variants of training an SOM, we present the most popular and widely used implementation of the SOM in this guide.\n\nAs we'll be using some Python routines to demonstrate the functions used to train an SOM, let's import a few of the libraries we'll be using:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n### The Algorithm Behind Training Self-Organizing Maps\n\nThe basic algorithm for training an SOM is given below:\n\n- Initialize all grid weights of the SOM\n- Repeat until convergence or maximum epochs are reached\n  - Shuffle the training examples\n  - For each training instance \\\\(x\\\\)\n    - Find the best matching unit BMU\n    - Update the weight vector of BMU and its neighboring cells\n\nThe three steps for initialization, finding the BMU, and updating the weights are explained  in the following sections. Let's begin!\n\n#### Initializing the SOM GRID\n\nAll the SOM grid weights can be initialized randomly. The SOM grid weights can also be initialized by randomly chosen examples from the training dataset.\n\n\u003e Which one should you choose?\n\nSOMs are sensitive to the initial weight of the map, so this choice affects the overall model. According to \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://arxiv.org/ftp/arxiv/papers/1210/1210.5873.pdf\"\u003ea case study\u003c/a\u003e performed by Ayodeji and Evgeny of University of Leicester and Siberian Federal University:\n\n\u003e By comparing the proportion of final SOM map of **RI (Random Initialization)** which outperformed **PCI (Principal Component Initialization)** under the same conditions, it was observed that RI performed quite well for non-linear data sets. \n\u003e\n\u003e However for quasi-linear datasets, the result remains inconclusive. In general, we can conclude that the hypothesis about advantages of the PCI is definitely wrong for essentially nonlinear datasets.\n\nRandom initialization outperforms non-random initialization for non-linear datasets. For quasi-linear datasets, it's not quite clear what approach wins consistently. Given these results - we'll stick to **random initialization**.\n\n#### Finding the Best Matching Unit (BMU)\n\nAs mentioned earlier, the best matching unit is the cell of the SOM grid that is closest to the training example \\\\(x\\\\). One method of finding this unit is to compute the \u003ca target=\"_blank\" href=\"https://stackabuse.com/calculating-euclidean-distance-with-numpy/\"\u003e**Euclidean distance**\u003c/a\u003e of \\\\(x\\\\) from the weight of each cell of the grid. \n\n\u003e The cell with the minimum distance can be chosen as the BMU. \n\nAn important point to note is that Euclidean distance is not the only possible method of selecting the BMU. An alternative distance measure or a similarity metric can also be used to determine the BMU, and choosing this mainly depends on the data and model you're building specifically.\n\n#### Updating the Weight Vector of BMU and Neighboring Cells\n\nA training example \\\\(x\\\\) effects various cells of the SOM grid by pulling the weights of these cells towards it. The maximum change occurs in the BMU and the influence of \\\\(x\\\\) diminishes as we move away from the BMU in the SOM grid. For a cell with coordinates \\\\((i,j)\\\\), its weight \\\\(w_{ij}\\\\) is updated at epoch \\\\(t+1\\\\) as:\n\n$$\nw_{ij}^{(t+1)} \\leftarrow w_{ij}^{(t)} + \\Delta w_{ij}^{(t)}\n$$\n\nWhere \\\\(\\Delta w_{ij}^{(t)}\\\\) is the change to be added to \\\\(w_{ij}^{(t)}\\\\). It can be computed as:\n\n$$\n\\Delta w_{ij}^{(t)} = \\eta^{(t)} f_{i,j}(g,h,\\sigma_t) (x-w_{ij}^{(t)})\n$$\n\nFor this expression:\n\n- \\\\(t\\\\) is the epoch number\n- \\\\((g,h)\\\\) are the coordinates of BMU\n- \\\\(\\eta\\\\) is the learning rate\n- \\\\(\\sigma_t\\\\) is the radius\n- \\\\(f_{ij}(g,h,\\sigma_t)\\\\) is the neighborhood distance function\n\nIn the following sections, we'll present the details of this weight training expression.\n\n#### The Learning Rate\n\nThe learning rate \\\\(\\eta\\\\) is a constant in the range [0,1] and determines the step size of the weight vector towards the input training example. For \\\\(\\eta=0\\\\), there is no change in the weight, and when \\\\(\\eta=1\\\\) the weight vector \\\\(w_{ij}\\\\) take the value of \\\\(x\\\\). \n\n\\\\(\\eta\\\\) is kept high at the start and decayed as the epochs proceed. One strategy for reducing the learning rate during the training phase is to use exponential decay:\n\n$$\n\\eta^{(t)} = \\eta ^0 e^{-t*\\lambda}\n$$\n\nWhere \\\\(\\lambda\u003c0\\\\) is the decay rate.\n\nTo understand how the learning rate changes with the decay rate, let's plot the learning rate against various epochs when the initial learning rate is set to one:\n\n```python\nepochs = np.arange(0, 50)\nlr_decay = [0.001, 0.1, 0.5, 0.99]\nfig,ax = plt.subplots(nrows=1, ncols=4, figsize=(15,4))\nplt_ind = np.arange(4) + 141\nfor decay, ind in zip(lr_decay, plt_ind):\n    plt.subplot(ind)\n    learn_rate = np.exp(-epochs * decay)\n    plt.plot(epochs, learn_rate, c='cyan')\n    plt.title('decay rate: ' + str(decay))\n    plt.xlabel('epochs $t$')\n    plt.ylabel('$\\eta^(t)$')\nfig.subplots_adjust(hspace=0.5, wspace=0.3)\nplt.show()\n```\n\n![learning rates for self organizing maps](https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-1.png)\n\n#### The Neighborhood Distance Function\n\nThe neighborhood distance function is given by:\n\n$$\nf_{ij}(g,h,\\sigma_t) = e^\\frac{-d((i,j),(g,h))^2}{2\\sigma_t^2}\n$$\n\nwhere \\\\(d((i,j),(g,h))\\\\) is the distance of coordinates \\\\((i,j)\\\\) of a cell from the BMU's coordinates \\\\((g,h)\\\\), and \\\\(\\sigma_t\\\\) is the radius at epoch \\\\(t\\\\). Normally Euclidean distance is used to compute the distance, however, any other distance or similarity metric can be used.\n\nAs the distance of BMU with itself is zero, the weight change of the BMU reduces to:\n\n$$\n\\Delta w_{gh} = \\eta (x-w_{gh})\n$$\n\nFor a unit \\\\((i,j)\\\\) having a large distance from the BMU, the neighborhood distance function reduces to a near-zero value, leading to a very small magnitude of \\\\(\\Delta w_{ij}\\\\). Hence, such units are unaffected by the training example \\\\(x\\\\). One training example, therefore, only affects the BMU and the cells in the close vicinity of the BMU. As we move away from the BMU the change in weights becomes less and less until it is negligible. \n\nThe radius determines the influence region of a training example \\\\(x\\\\). A high radius value affects a larger number of cells and a smaller radius affects only the BMU. A common strategy is to start with a large radius and reduce it as the epochs proceed, i.e.:\n\n$$\n\\sigma_t = \\sigma_0 e^{-t*\\beta}\n$$\n\nHere \\\\(\\beta\u003c0\\\\) is the decay rate. The decay rate corresponding to radius has the same effect on the radius as the decay rate corresponding to the learning rate. To gain a deeper insight into the behavior of the neighborhood function, let's plot it against the distance for different values of the radius. A point to note in these graphs is that the distance function approaches a near-zero value as the distance exceeds 10 for \\\\(\\sigma^2 \\leq 10\\\\). \n\nWe'll use this fact later to make training more efficient in the implementation part:\n\n```python\ndistance = np.arange(0, 30)\nsigma_sq = [0.1, 1, 10, 100]\nfig,ax = plt.subplots(nrows=1, ncols=4, figsize=(15,4))\nplt_ind = np.arange(4) + 141\nfor s, ind in zip(sigma_sq, plt_ind):\n    plt.subplot(ind)\n    f = np.exp(-distance ** 2 / 2 / s)\n    plt.plot(distance, f, c='cyan')\n    plt.title('$\\sigma^2$ = ' + str(s))\n    plt.xlabel('Distance')\n    plt.ylabel('Neighborhood function $f$')\nfig.subplots_adjust(hspace=0.5, wspace=0.3)\nplt.show()\n```\n\n![decay rate for self organizing maps](https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-2.png)\n\n\n### Implementing a Self-Organizing Map in Python Using NumPy\n\nAs there is no built-in routine for an SOM in the de-facto standard machine learning library, **_Scikit-Learn_**, we'll do a quick implementation manually using **_NumPy_**. The unsupervised machine learning model is pretty straightforward and easy to implement. \n\nWe'll implement the SOM as a 2D `mxn` grid, hence requiring a 3D `NumPy` array. The third dimension is required for storing the weights in each cell:\n\n\n```python\n# Return the (g,h) index of the BMU in the grid\ndef find_BMU(SOM,x):\n    distSq = (np.square(SOM - x)).sum(axis=2)\n    return np.unravel_index(np.argmin(distSq, axis=None), distSq.shape)\n    \n# Update the weights of the SOM cells when given a single training example\n# and the model parameters along with BMU coordinates as a tuple\ndef update_weights(SOM, train_ex, learn_rate, radius_sq, \n                   BMU_coord, step=3):\n    g, h = BMU_coord\n    #if radius is close to zero then only BMU is changed\n    if radius_sq \u003c 1e-3:\n        SOM[g,h,:] += learn_rate * (train_ex - SOM[g,h,:])\n        return SOM\n    # Change all cells in a small neighborhood of BMU\n    for i in range(max(0, g-step), min(SOM.shape[0], g+step)):\n        for j in range(max(0, h-step), min(SOM.shape[1], h+step)):\n            dist_sq = np.square(i - g) + np.square(j - h)\n            dist_func = np.exp(-dist_sq / 2 / radius_sq)\n            SOM[i,j,:] += learn_rate * dist_func * (train_ex - SOM[i,j,:])   \n    return SOM    \n\n# Main routine for training an SOM. It requires an initialized SOM grid\n# or a partially trained grid as parameter\ndef train_SOM(SOM, train_data, learn_rate = .1, radius_sq = 1, \n             lr_decay = .1, radius_decay = .1, epochs = 10):    \n    learn_rate_0 = learn_rate\n    radius_0 = radius_sq\n    for epoch in np.arange(0, epochs):\n        rand.shuffle(train_data)      \n        for train_ex in train_data:\n            g, h = find_BMU(SOM, train_ex)\n            SOM = update_weights(SOM, train_ex, \n                                 learn_rate, radius_sq, (g,h))\n        # Update learning rate and radius\n        learn_rate = learn_rate_0 * np.exp(-epoch * lr_decay)\n        radius_sq = radius_0 * np.exp(-epoch * radius_decay)            \n    return SOM\n```\n\nLet's break down the key functions used to implement a Self-Organizing Map:\n\n`find_BMU()` returns the grid cell coordinates of the best matching unit when given the `SOM` grid and a training example `x`. It computes the square of the Euclidean distance between each cell weight and `x`, and returns `(g,h)`, i.e., the cell coordinates with the minimum distance.\n\nThe `update_weights()` function requires an SOM grid, a training example `x`, the parameters `learn_rate` and `radius_sq`, the coordinates of the best matching unit, and a `step` parameter. Theoretically, all cells of the SOM are updated on the next training example. However, we showed earlier that the change is negligible for cells that are far away from the BMU. Hence, we can make the code more efficient by changing only the cells in a small vicinity of the BMU. The `step` parameter specifies the maximum number of cells on the left, right, above, and below to change when updating the weights. \n\n\nFinallt, the `train_SOM()` function implements the main training procedure of an SOM. It requires an initialized or partially trained `SOM` grid and `train_data` as parameters. The advantage is to be able to train the SOM from a previous trained stage. Additionally `learn_rate` and `radius_sq` parameters are required along with their corresponding decay rates `lr_decay` and `radius_decay`. The `epochs` parameter is set to 10 by default but can be changed if needed.\n\n### Running the Self-Organizing Map on a Practical Example\n\nOne of the commonly cited examples for training an SOM is that of random colors. We can train an SOM grid and easily visualize how various similar colors get arranged in neighboring cells. \n\n\u003e Cells far away from each other have different colors. \n\nLet's run the `train_SOM()` function on a training data matrix filled with random RGB colors. \n\nThe code below initializes a training data matrix and an SOM grid with random RGB colors. It also displays the training data and the randomly initialized _SOM grid_. Note, the training matrix is a 3000x3 matrix, however, we have reshaped it to 50x60x3 matrix for visualization:\n\n```python\n# Dimensions of the SOM grid\nm = 10\nn = 10\n# Number of training examples\nn_x = 3000\nrand = np.random.RandomState(0)\n# Initialize the training data\ntrain_data = rand.randint(0, 255, (n_x, 3))\n# Initialize the SOM randomly\nSOM = rand.randint(0, 255, (m, n, 3)).astype(float)\n# Display both the training matrix and the SOM grid\nfig, ax = plt.subplots(\n    nrows=1, ncols=2, figsize=(12, 3.5), \n    subplot_kw=dict(xticks=[], yticks=[]))\nax[0].imshow(train_data.reshape(50, 60, 3))\nax[0].title.set_text('Training Data')\nax[1].imshow(SOM.astype(int))\nax[1].title.set_text('Randomly Initialized SOM Grid')\n```\n\n![randomly initialized self organizing map](https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-3.png)\n\nLet's now train the SOM and check up on it every 5 epochs as a quick overview of its progress:\n\n```python\nfig, ax = plt.subplots(\n    nrows=1, ncols=4, figsize=(15, 3.5), \n    subplot_kw=dict(xticks=[], yticks=[]))\ntotal_epochs = 0\nfor epochs, i in zip([1, 4, 5, 10], range(0,4)):\n    total_epochs += epochs\n    SOM = train_SOM(SOM, train_data, epochs=epochs)\n    ax[i].imshow(SOM.astype(int))\n    ax[i].title.set_text('Epochs = ' + str(total_epochs))\n```\n\n![self organizing map training and results](https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-4.png)\n\nThe example above is very interesting as it shows how the grid automatically arranges the RGB colors so that various shades of the same color are close together in the SOM grid. The arrangement takes place as early as the first epoch, but it's not ideal. We can see that the SOM converges in around 10 epochs and there are fewer changes in the subsequent epochs.\n\n#### Effect of Learning Rate and Radius\n\nTo see how the learning rate varies for different learning rates and radii, we can run the SOM for 10 epochs when starting from the same initial grid. The code below trains the SOM for three different values of the learning rate and three different radii. \n\nThe SOM is rendered after 5 epochs for each simulation:\n\n```python\nfig, ax = plt.subplots(\n    nrows=3, ncols=3, figsize=(15, 15), \n    subplot_kw=dict(xticks=[], yticks=[]))\n\n# Initialize the SOM randomly to the same state\n\nfor learn_rate, i in zip([0.001, 0.5, 0.99], [0, 1, 2]):\n    for radius_sq, j in zip([0.01, 1, 10], [0, 1, 2]):\n        rand = np.random.RandomState(0)\n        SOM = rand.randint(0, 255, (m, n, 3)).astype(float)        \n        SOM = train_SOM(SOM, train_data, epochs = 5,\n                        learn_rate = learn_rate, \n                        radius_sq = radius_sq)\n        ax[i][j].imshow(SOM.astype(int))\n        ax[i][j].title.set_text('$\\eta$ = ' + str(learn_rate) + \n                                ', $\\sigma^2$ = ' + str(radius_sq))\n```\n\n![effects and tuning self organizing map hyperparameters](https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-5.png)\n\nThe example above shows that for radius values close to zero (first column), the SOM only changes the individual cells but not the neighboring cells. Hence, a proper map is not created regardless of the learning rate. A similar case is also encountered for smaller learning rates (first row, second column). As with any other machine learning algorithm, a good balance of parameters is required for ideal training. \n\n### Conclusions\n\nIn this guide, we discussed the theoretical model of an SOM and its detailed implementation. We demonstrated the SOM on RGB colors and showed how different shades of the same color organized themselves on a 2D grid. \n\nWhile the SOMs are no longer very popular in the machine learning community, they remain a good model for data summary and visualization. \n","body_html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eIn this guide, we'll be taking a look at an unsupervised learning model, known as a \u003cstrong\u003e\u003cem\u003eSelf-Organizing Map (SOM)\u003c/em\u003e\u003c/strong\u003e, as well as its implementation in Python. We'll be using an \u003cstrong\u003eRGB Color\u003c/strong\u003e example to train the SOM and demonstrate its performance and typical usage.\u003c/p\u003e\n\u003ch3 id=\"selforganizingmapsageneralintroduction\"\u003eSelf-Organizing Maps: A General Introduction\u003c/h3\u003e\n\u003cp\u003eA \u003cem\u003eSelf-Organizing Map\u003c/em\u003e was first introduced by \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://link.springer.com/article/10.1007/BF00337288\"\u003eTeuvo Kohonen in 1982\u003c/a\u003e and is also sometimes known as a \u003cstrong\u003e\u003cem\u003eKohonen map\u003c/em\u003e\u003c/strong\u003e. It is a special type of an \u003cem\u003eartificial neural network\u003c/em\u003e, which builds a map of the training data. The map is generally a 2D rectangular grid of weights but can be extended to a 3D or higher dimensional model. Other grid structures like hexagonal grids are also possible.\u003c/p\u003e\n\u003cp\u003eAn SOM is mainly used for data visualization and provides a quick visual summary of the training instances. In a 2D rectangular grid, each cell is represented by a weight vector. For a trained SOM, each cell weight represents a summary of a few training examples. Cells in the close vicinity of each other have similar weights, and like examples can be mapped to cells in a small neighborhood of each other.\u003c/p\u003e\n\u003cp\u003eThe figure below is a rough illustration of the structure of the SOM:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-0.png\" alt=\"self-organizing map illustration\"\u003e\u003c/p\u003e\n\u003cp\u003eAn SOM is trained using \u003cstrong\u003ecompetitive learning\u003c/strong\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eCompetitive Learning\u003c/strong\u003e is a form of unsupervised learning, where constituent elements compete to produce a satisfying result, and only one gets to win the competition.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhen a training example is input into the grid, the \u003cstrong\u003e\u003cem\u003eBest Matching Unit (BMU)\u003c/em\u003e\u003c/strong\u003e is determined (competition winner). The BMU is the cell whose weights are closest to the training example.\u003c/p\u003e\n\u003cp\u003eNext, the BMU's weights and weights of the cells neighboring the BMU, are adapted to move closer to the input training instance. While there are other valid variants of training an SOM, we present the most popular and widely used implementation of the SOM in this guide.\u003c/p\u003e\n\u003cp\u003eAs we'll be using some Python routines to demonstrate the functions used to train an SOM, let's import a few of the libraries we'll be using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"thealgorithmbehindtrainingselforganizingmaps\"\u003eThe Algorithm Behind Training Self-Organizing Maps\u003c/h3\u003e\n\u003cp\u003eThe basic algorithm for training an SOM is given below:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInitialize all grid weights of the SOM\u003c/li\u003e\n\u003cli\u003eRepeat until convergence or maximum epochs are reached\n\u003cul\u003e\n\u003cli\u003eShuffle the training examples\u003c/li\u003e\n\u003cli\u003eFor each training instance \\(x\\)\n\u003cul\u003e\n\u003cli\u003eFind the best matching unit BMU\u003c/li\u003e\n\u003cli\u003eUpdate the weight vector of BMU and its neighboring cells\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe three steps for initialization, finding the BMU, and updating the weights are explained  in the following sections. Let's begin!\u003c/p\u003e\n\u003ch4 id=\"initializingthesomgrid\"\u003eInitializing the SOM GRID\u003c/h4\u003e\n\u003cp\u003eAll the SOM grid weights can be initialized randomly. The SOM grid weights can also be initialized by randomly chosen examples from the training dataset.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWhich one should you choose?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSOMs are sensitive to the initial weight of the map, so this choice affects the overall model. According to \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://arxiv.org/ftp/arxiv/papers/1210/1210.5873.pdf\"\u003ea case study\u003c/a\u003e performed by Ayodeji and Evgeny of University of Leicester and Siberian Federal University:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBy comparing the proportion of final SOM map of \u003cstrong\u003eRI (Random Initialization)\u003c/strong\u003e which outperformed \u003cstrong\u003ePCI (Principal Component Initialization)\u003c/strong\u003e under the same conditions, it was observed that RI performed quite well for non-linear data sets.\u003c/p\u003e\n\u003cp\u003eHowever for quasi-linear datasets, the result remains inconclusive. In general, we can conclude that the hypothesis about advantages of the PCI is definitely wrong for essentially nonlinear datasets.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eRandom initialization outperforms non-random initialization for non-linear datasets. For quasi-linear datasets, it's not quite clear what approach wins consistently. Given these results - we'll stick to \u003cstrong\u003erandom initialization\u003c/strong\u003e.\u003c/p\u003e\n\u003ch4 id=\"findingthebestmatchingunitbmu\"\u003eFinding the Best Matching Unit (BMU)\u003c/h4\u003e\n\u003cp\u003eAs mentioned earlier, the best matching unit is the cell of the SOM grid that is closest to the training example \\(x\\). One method of finding this unit is to compute the \u003ca target=\"_blank\" href=\"https://stackabuse.com/calculating-euclidean-distance-with-numpy/\"\u003e\u003cstrong\u003eEuclidean distance\u003c/strong\u003e\u003c/a\u003e of \\(x\\) from the weight of each cell of the grid.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe cell with the minimum distance can be chosen as the BMU.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eAn important point to note is that Euclidean distance is not the only possible method of selecting the BMU. An alternative distance measure or a similarity metric can also be used to determine the BMU, and choosing this mainly depends on the data and model you're building specifically.\u003c/p\u003e\n\u003ch4 id=\"updatingtheweightvectorofbmuandneighboringcells\"\u003eUpdating the Weight Vector of BMU and Neighboring Cells\u003c/h4\u003e\n\u003cp\u003eA training example \\(x\\) effects various cells of the SOM grid by pulling the weights of these cells towards it. The maximum change occurs in the BMU and the influence of \\(x\\) diminishes as we move away from the BMU in the SOM grid. For a cell with coordinates \\((i,j)\\), its weight \\(w_{ij}\\) is updated at epoch \\(t+1\\) as:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\nw_{ij}^{(t+1)} \\leftarrow w_{ij}^{(t)} + \\Delta w_{ij}^{(t)}\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cp\u003eWhere \\(\\Delta w_{ij}^{(t)}\\) is the change to be added to \\(w_{ij}^{(t)}\\). It can be computed as:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\n\\Delta w_{ij}^{(t)} = \\eta^{(t)} f_{i,j}(g,h,\\sigma_t) (x-w_{ij}^{(t)})\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cp\u003eFor this expression:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\\(t\\) is the epoch number\u003c/li\u003e\n\u003cli\u003e\\((g,h)\\) are the coordinates of BMU\u003c/li\u003e\n\u003cli\u003e\\(\\eta\\) is the learning rate\u003c/li\u003e\n\u003cli\u003e\\(\\sigma_t\\) is the radius\u003c/li\u003e\n\u003cli\u003e\\(f_{ij}(g,h,\\sigma_t)\\) is the neighborhood distance function\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn the following sections, we'll present the details of this weight training expression.\u003c/p\u003e\n\u003ch4 id=\"thelearningrate\"\u003eThe Learning Rate\u003c/h4\u003e\n\u003cp\u003eThe learning rate \\(\\eta\\) is a constant in the range [0,1] and determines the step size of the weight vector towards the input training example. For \\(\\eta=0\\), there is no change in the weight, and when \\(\\eta=1\\) the weight vector \\(w_{ij}\\) take the value of \\(x\\).\u003c/p\u003e\n\u003cp\u003e\\(\\eta\\) is kept high at the start and decayed as the epochs proceed. One strategy for reducing the learning rate during the training phase is to use exponential decay:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\n\\eta^{(t)} = \\eta ^0 e^{-t*\\lambda}\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cp\u003eWhere \\(\\lambda\u0026lt;0\\) is the decay rate.\u003c/p\u003e\n\u003cp\u003eTo understand how the learning rate changes with the decay rate, let's plot the learning rate against various epochs when the initial learning rate is set to one:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eepochs = np.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e)\nlr_decay = [\u003cspan class=\"hljs-number\"\u003e0.001\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.99\u003c/span\u003e]\nfig,ax = plt.subplots(nrows=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, ncols=\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\nplt_ind = np.arange(\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e) + \u003cspan class=\"hljs-number\"\u003e141\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e decay, ind \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(lr_decay, plt_ind):\n    plt.subplot(ind)\n    learn_rate = np.exp(-epochs * decay)\n    plt.plot(epochs, learn_rate, c=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;cyan\u0026#x27;\u003c/span\u003e)\n    plt.title(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;decay rate: \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(decay))\n    plt.xlabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;epochs $t$\u0026#x27;\u003c/span\u003e)\n    plt.ylabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;$\\eta^(t)$\u0026#x27;\u003c/span\u003e)\nfig.subplots_adjust(hspace=\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e, wspace=\u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e)\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-1.png\" alt=\"learning rates for self organizing maps\"\u003e\u003c/p\u003e\n\u003ch4 id=\"theneighborhooddistancefunction\"\u003eThe Neighborhood Distance Function\u003c/h4\u003e\n\u003cp\u003eThe neighborhood distance function is given by:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\nf_{ij}(g,h,\\sigma_t) = e^\\frac{-d((i,j),(g,h))^2}{2\\sigma_t^2}\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cp\u003ewhere \\(d((i,j),(g,h))\\) is the distance of coordinates \\((i,j)\\) of a cell from the BMU's coordinates \\((g,h)\\), and \\(\\sigma_t\\) is the radius at epoch \\(t\\). Normally Euclidean distance is used to compute the distance, however, any other distance or similarity metric can be used.\u003c/p\u003e\n\u003cp\u003eAs the distance of BMU with itself is zero, the weight change of the BMU reduces to:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\n\\Delta w_{gh} = \\eta (x-w_{gh})\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cp\u003eFor a unit \\((i,j)\\) having a large distance from the BMU, the neighborhood distance function reduces to a near-zero value, leading to a very small magnitude of \\(\\Delta w_{ij}\\). Hence, such units are unaffected by the training example \\(x\\). One training example, therefore, only affects the BMU and the cells in the close vicinity of the BMU. As we move away from the BMU the change in weights becomes less and less until it is negligible.\u003c/p\u003e\n\u003cp\u003eThe radius determines the influence region of a training example \\(x\\). A high radius value affects a larger number of cells and a smaller radius affects only the BMU. A common strategy is to start with a large radius and reduce it as the epochs proceed, i.e.:\u003c/p\u003e\n\u003cp\u003e$$\u003cbr\u003e\n\\sigma_t = \\sigma_0 e^{-t*\\beta}\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cp\u003eHere \\(\\beta\u0026lt;0\\) is the decay rate. The decay rate corresponding to radius has the same effect on the radius as the decay rate corresponding to the learning rate. To gain a deeper insight into the behavior of the neighborhood function, let's plot it against the distance for different values of the radius. A point to note in these graphs is that the distance function approaches a near-zero value as the distance exceeds 10 for \\(\\sigma^2 \\leq 10\\).\u003c/p\u003e\n\u003cp\u003eWe'll use this fact later to make training more efficient in the implementation part:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003edistance = np.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e)\nsigma_sq = [\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e]\nfig,ax = plt.subplots(nrows=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, ncols=\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\nplt_ind = np.arange(\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e) + \u003cspan class=\"hljs-number\"\u003e141\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e s, ind \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(sigma_sq, plt_ind):\n    plt.subplot(ind)\n    f = np.exp(-distance ** \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e / \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e / s)\n    plt.plot(distance, f, c=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;cyan\u0026#x27;\u003c/span\u003e)\n    plt.title(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;$\\sigma^2$ = \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(s))\n    plt.xlabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Distance\u0026#x27;\u003c/span\u003e)\n    plt.ylabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Neighborhood function $f$\u0026#x27;\u003c/span\u003e)\nfig.subplots_adjust(hspace=\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e, wspace=\u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e)\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-2.png\" alt=\"decay rate for self organizing maps\"\u003e\u003c/p\u003e\n\u003ch3 id=\"implementingaselforganizingmapinpythonusingnumpy\"\u003eImplementing a Self-Organizing Map in Python Using NumPy\u003c/h3\u003e\n\u003cp\u003eAs there is no built-in routine for an SOM in the de-facto standard machine learning library, \u003cstrong\u003e\u003cem\u003eScikit-Learn\u003c/em\u003e\u003c/strong\u003e, we'll do a quick implementation manually using \u003cstrong\u003e\u003cem\u003eNumPy\u003c/em\u003e\u003c/strong\u003e. The unsupervised machine learning model is pretty straightforward and easy to implement.\u003c/p\u003e\n\u003cp\u003eWe'll implement the SOM as a 2D \u003ccode\u003emxn\u003c/code\u003e grid, hence requiring a 3D \u003ccode\u003eNumPy\u003c/code\u003e array. The third dimension is required for storing the weights in each cell:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Return the (g,h) index of the BMU in the grid\u003c/span\u003e\n\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003efind_BMU\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eSOM,x\u003c/span\u003e):\u003c/span\u003e\n    distSq = (np.square(SOM - x)).\u003cspan class=\"hljs-built_in\"\u003esum\u003c/span\u003e(axis=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e np.unravel_index(np.argmin(distSq, axis=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e), distSq.shape)\n    \n\u003cspan class=\"hljs-comment\"\u003e# Update the weights of the SOM cells when given a single training example\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# and the model parameters along with BMU coordinates as a tuple\u003c/span\u003e\n\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003eupdate_weights\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eSOM, train_ex, learn_rate, radius_sq, \n                   BMU_coord, step=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e\n    g, h = BMU_coord\n    \u003cspan class=\"hljs-comment\"\u003e#if radius is close to zero then only BMU is changed\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e radius_sq \u0026lt; \u003cspan class=\"hljs-number\"\u003e1e-3\u003c/span\u003e:\n        SOM[g,h,:] += learn_rate * (train_ex - SOM[g,h,:])\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e SOM\n    \u003cspan class=\"hljs-comment\"\u003e# Change all cells in a small neighborhood of BMU\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, g-step), \u003cspan class=\"hljs-built_in\"\u003emin\u003c/span\u003e(SOM.shape[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], g+step)):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e j \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003emax\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, h-step), \u003cspan class=\"hljs-built_in\"\u003emin\u003c/span\u003e(SOM.shape[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], h+step)):\n            dist_sq = np.square(i - g) + np.square(j - h)\n            dist_func = np.exp(-dist_sq / \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e / radius_sq)\n            SOM[i,j,:] += learn_rate * dist_func * (train_ex - SOM[i,j,:])   \n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e SOM    \n\n\u003cspan class=\"hljs-comment\"\u003e# Main routine for training an SOM. It requires an initialized SOM grid\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# or a partially trained grid as parameter\u003c/span\u003e\n\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003etrain_SOM\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eSOM, train_data, learn_rate = \u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e, radius_sq = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \n             lr_decay = \u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e, radius_decay = \u003cspan class=\"hljs-number\"\u003e.1\u003c/span\u003e, epochs = \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e\u003c/span\u003e):\u003c/span\u003e    \n    learn_rate_0 = learn_rate\n    radius_0 = radius_sq\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e epoch \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e np.arange(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, epochs):\n        rand.shuffle(train_data)      \n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e train_ex \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e train_data:\n            g, h = find_BMU(SOM, train_ex)\n            SOM = update_weights(SOM, train_ex, \n                                 learn_rate, radius_sq, (g,h))\n        \u003cspan class=\"hljs-comment\"\u003e# Update learning rate and radius\u003c/span\u003e\n        learn_rate = learn_rate_0 * np.exp(-epoch * lr_decay)\n        radius_sq = radius_0 * np.exp(-epoch * radius_decay)            \n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e SOM\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet's break down the key functions used to implement a Self-Organizing Map:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003efind_BMU()\u003c/code\u003e returns the grid cell coordinates of the best matching unit when given the \u003ccode\u003eSOM\u003c/code\u003e grid and a training example \u003ccode\u003ex\u003c/code\u003e. It computes the square of the Euclidean distance between each cell weight and \u003ccode\u003ex\u003c/code\u003e, and returns \u003ccode\u003e(g,h)\u003c/code\u003e, i.e., the cell coordinates with the minimum distance.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eupdate_weights()\u003c/code\u003e function requires an SOM grid, a training example \u003ccode\u003ex\u003c/code\u003e, the parameters \u003ccode\u003elearn_rate\u003c/code\u003e and \u003ccode\u003eradius_sq\u003c/code\u003e, the coordinates of the best matching unit, and a \u003ccode\u003estep\u003c/code\u003e parameter. Theoretically, all cells of the SOM are updated on the next training example. However, we showed earlier that the change is negligible for cells that are far away from the BMU. Hence, we can make the code more efficient by changing only the cells in a small vicinity of the BMU. The \u003ccode\u003estep\u003c/code\u003e parameter specifies the maximum number of cells on the left, right, above, and below to change when updating the weights.\u003c/p\u003e\n\u003cp\u003eFinallt, the \u003ccode\u003etrain_SOM()\u003c/code\u003e function implements the main training procedure of an SOM. It requires an initialized or partially trained \u003ccode\u003eSOM\u003c/code\u003e grid and \u003ccode\u003etrain_data\u003c/code\u003e as parameters. The advantage is to be able to train the SOM from a previous trained stage. Additionally \u003ccode\u003elearn_rate\u003c/code\u003e and \u003ccode\u003eradius_sq\u003c/code\u003e parameters are required along with their corresponding decay rates \u003ccode\u003elr_decay\u003c/code\u003e and \u003ccode\u003eradius_decay\u003c/code\u003e. The \u003ccode\u003eepochs\u003c/code\u003e parameter is set to 10 by default but can be changed if needed.\u003c/p\u003e\n\u003ch3 id=\"runningtheselforganizingmaponapracticalexample\"\u003eRunning the Self-Organizing Map on a Practical Example\u003c/h3\u003e\n\u003cp\u003eOne of the commonly cited examples for training an SOM is that of random colors. We can train an SOM grid and easily visualize how various similar colors get arranged in neighboring cells.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCells far away from each other have different colors.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLet's run the \u003ccode\u003etrain_SOM()\u003c/code\u003e function on a training data matrix filled with random RGB colors.\u003c/p\u003e\n\u003cp\u003eThe code below initializes a training data matrix and an SOM grid with random RGB colors. It also displays the training data and the randomly initialized \u003cem\u003eSOM grid\u003c/em\u003e. Note, the training matrix is a 3000x3 matrix, however, we have reshaped it to 50x60x3 matrix for visualization:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Dimensions of the SOM grid\u003c/span\u003e\nm = \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e\nn = \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# Number of training examples\u003c/span\u003e\nn_x = \u003cspan class=\"hljs-number\"\u003e3000\u003c/span\u003e\nrand = np.random.RandomState(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# Initialize the training data\u003c/span\u003e\ntrain_data = rand.randint(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e, (n_x, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e))\n\u003cspan class=\"hljs-comment\"\u003e# Initialize the SOM randomly\u003c/span\u003e\nSOM = rand.randint(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e, (m, n, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)).astype(\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# Display both the training matrix and the SOM grid\u003c/span\u003e\nfig, ax = plt.subplots(\n    nrows=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, ncols=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3.5\u003c/span\u003e), \n    subplot_kw=\u003cspan class=\"hljs-built_in\"\u003edict\u003c/span\u003e(xticks=[], yticks=[]))\nax[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].imshow(train_data.reshape(\u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e60\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e))\nax[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].title.set_text(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Training Data\u0026#x27;\u003c/span\u003e)\nax[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e].imshow(SOM.astype(\u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e))\nax[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e].title.set_text(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Randomly Initialized SOM Grid\u0026#x27;\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-3.png\" alt=\"randomly initialized self organizing map\"\u003e\u003c/p\u003e\n\u003cp\u003eLet's now train the SOM and check up on it every 5 epochs as a quick overview of its progress:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig, ax = plt.subplots(\n    nrows=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, ncols=\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e3.5\u003c/span\u003e), \n    subplot_kw=\u003cspan class=\"hljs-built_in\"\u003edict\u003c/span\u003e(xticks=[], yticks=[]))\ntotal_epochs = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e epochs, i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e)):\n    total_epochs += epochs\n    SOM = train_SOM(SOM, train_data, epochs=epochs)\n    ax[i].imshow(SOM.astype(\u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e))\n    ax[i].title.set_text(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Epochs = \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(total_epochs))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-4.png\" alt=\"self organizing map training and results\"\u003e\u003c/p\u003e\n\u003cp\u003eThe example above is very interesting as it shows how the grid automatically arranges the RGB colors so that various shades of the same color are close together in the SOM grid. The arrangement takes place as early as the first epoch, but it's not ideal. We can see that the SOM converges in around 10 epochs and there are fewer changes in the subsequent epochs.\u003c/p\u003e\n\u003ch4 id=\"effectoflearningrateandradius\"\u003eEffect of Learning Rate and Radius\u003c/h4\u003e\n\u003cp\u003eTo see how the learning rate varies for different learning rates and radii, we can run the SOM for 10 epochs when starting from the same initial grid. The code below trains the SOM for three different values of the learning rate and three different radii.\u003c/p\u003e\n\u003cp\u003eThe SOM is rendered after 5 epochs for each simulation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig, ax = plt.subplots(\n    nrows=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, ncols=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e), \n    subplot_kw=\u003cspan class=\"hljs-built_in\"\u003edict\u003c/span\u003e(xticks=[], yticks=[]))\n\n\u003cspan class=\"hljs-comment\"\u003e# Initialize the SOM randomly to the same state\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e learn_rate, i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e0.001\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0.99\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]):\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e radius_sq, j \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e0.01\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e]):\n        rand = np.random.RandomState(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n        SOM = rand.randint(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e255\u003c/span\u003e, (m, n, \u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)).astype(\u003cspan class=\"hljs-built_in\"\u003efloat\u003c/span\u003e)        \n        SOM = train_SOM(SOM, train_data, epochs = \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e,\n                        learn_rate = learn_rate, \n                        radius_sq = radius_sq)\n        ax[i][j].imshow(SOM.astype(\u003cspan class=\"hljs-built_in\"\u003eint\u003c/span\u003e))\n        ax[i][j].title.set_text(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;$\\eta$ = \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(learn_rate) + \n                                \u003cspan class=\"hljs-string\"\u003e\u0026#x27;, $\\sigma^2$ = \u0026#x27;\u003c/span\u003e + \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(radius_sq))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-5.png\" alt=\"effects and tuning self organizing map hyperparameters\"\u003e\u003c/p\u003e\n\u003cp\u003eThe example above shows that for radius values close to zero (first column), the SOM only changes the individual cells but not the neighboring cells. Hence, a proper map is not created regardless of the learning rate. A similar case is also encountered for smaller learning rates (first row, second column). As with any other machine learning algorithm, a good balance of parameters is required for ideal training.\u003c/p\u003e\n\u003ch3 id=\"conclusions\"\u003eConclusions\u003c/h3\u003e\n\u003cp\u003eIn this guide, we discussed the theoretical model of an SOM and its detailed implementation. We demonstrated the SOM on RGB colors and showed how different shades of the same color organized themselves on a 2D grid.\u003c/p\u003e\n\u003cp\u003eWhile the SOMs are no longer very popular in the machine learning community, they remain a good model for data summary and visualization.\u003c/p\u003e\n","parent_id":null,"type":"article","status":"published","visibility":"public","img_feature":null,"is_featured":false,"locale":"en","custom_excerpt":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"comment_id":"602fd8d22c5e1a08f1ec8550","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In this guide, we'll cover Self-Organizing Maps in detail, as well as implement a SOM in Python with Numpy and experiment with the hyperparameters to get to know how they affect the model.","read_time_min":11,"published_by":16,"published_at":1629887400000,"created_by":null,"updated_by":null,"created_at":1613748434000,"updated_at":1631996786989,"contributors":[{"id":16,"name":"David Landup","slug":"david","email":"thealduinmaster@gmail.com","password_hash":"$2a$10$W/oMJdUBSTeG3trWAHa1xO0pQruxuLgD/6hS7VuxPafcmAxeBXmVi","role_id":2,"img_profile":"//s3.stackabuse.com/media/users/865cd7d217ea11c9d9555c4f666e2d73.jpg","img_cover":null,"bio_md":"Entrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs. \n\nGreat passion for accessible education and promotion of reason, science, humanism, and progress.","bio_html":"\u003cp\u003eEntrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs.\u003c/p\u003e\n\u003cp\u003eGreat passion for accessible education and promotion of reason, science, humanism, and progress.\u003c/p\u003e\n","website":"https://www.upwork.com/freelancers/~017664e499a2766871","location":"Serbia","facebook":"","twitter":"","github":null,"status":"active","locale":null,"last_seen_at":1622229427000,"created_by":null,"updated_by":null,"created_at":1534532687000,"updated_at":1640861394795,"role":"editor","secret_token":"2a2be92558fae38f89cfbd0c6a4ba90c","is_email_confirmed":false,"_pivot_content_id":878,"_pivot_user_id":16,"_pivot_role":"editor","_pivot_sort_order":1},{"id":69,"name":"Mehreen Saeed","slug":"mehreen","email":"courses.at.fast@gmail.com","password_hash":"$2a$10$SyXn0J3GubHzykx/cHqQSOwdP9JM6UHU3DA5R5mmNZvsTZ.zza97C","role_id":4,"img_profile":"//s3.stackabuse.com/media/users/89430606bd6f853f01889aa0bd28a437.png","img_cover":"//s3.stackabuse.com/media/users/7356adc6f29d109e0cfcaa44a662b2a1.JPG","bio_md":"I am an educator and I love mathematics and data science!","bio_html":"\u003cp\u003eI am an educator and I love mathematics and data science!\u003c/p\u003e\n","website":null,"location":"Pakistan","facebook":null,"twitter":null,"github":null,"status":"active","locale":null,"last_seen_at":1617038327000,"created_by":null,"updated_by":null,"created_at":1599395951000,"updated_at":1629840517435,"role":"author","secret_token":"7c90f1f392be6c321d7e2109f9642055","is_email_confirmed":false,"_pivot_content_id":878,"_pivot_user_id":69,"_pivot_role":"author","_pivot_sort_order":0}],"tags":[{"id":9,"name":"python","slug":"python","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1431358631000,"updated_at":1431358631000,"_pivot_content_id":878,"_pivot_tag_id":9,"_pivot_sort_order":0},{"id":57,"name":"artificial intelligence","slug":"artificial-intelligence","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1442517465000,"updated_at":1442517465000,"_pivot_content_id":878,"_pivot_tag_id":57,"_pivot_sort_order":3},{"id":75,"name":"machine learning","slug":"machine-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507305534000,"updated_at":1507305534000,"_pivot_content_id":878,"_pivot_tag_id":75,"_pivot_sort_order":2},{"id":106,"name":"data science","slug":"data-science","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1536865458000,"updated_at":1536865458000,"_pivot_content_id":878,"_pivot_tag_id":106,"_pivot_sort_order":1},{"id":140,"name":"numpy","slug":"numpy","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1564339082000,"updated_at":1564339082000,"_pivot_content_id":878,"_pivot_tag_id":140,"_pivot_sort_order":4}],"_pivot_tag_id":75,"_pivot_content_id":878},{"id":893,"old_id":"604515cf2c5e1a08f1ec8f8d","uuid":"13384552-0f72-4b9b-9fef-10d44356d557","title":"Guide to Multidimensional Scaling in Python with Scikit-Learn","slug":"guide-to-multidimensional-scaling-in-python-with-scikit-learn","body_md":"### Introduction\n\n\u003e In this guide, we'll dive into a **dimensionality reduction**, **data embedding** and **data visualization technique** known as **Multidimensional Scaling (MDS)**.\n\nWe'll be utilizing Scikit-Learn to perform Multidimensional Scaling, as it has a wonderfully simple and powerful API. Throughout the guide, we'll be using the \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://scikit-learn.org/0.19/datasets/olivetti_faces.html\"\u003eOlivetti faces dataset from AT\u0026T\u003c/a\u003e to illustrate the embedding of data in a lower-dimensional space.\n\nBy the end of the guide, you'll have a firm grasp on Multidimensional Scaling, as well as its hyperparameters and how they impact the technique.\n\n### What is Multidimensional Scaling?\n\n\u003e _MDS is a non-linear technique for embedding data in a lower-dimensional space_. \n\nIt maps points residing in a higher-dimensional space to a lower-dimensional space while preserving the distances between those points as much as possible. Because of this, the pairwise distances between points in the lower-dimensional space are matched closely to their actual distances.\n\nThe following figure is an example of a possible mapping of points from 3D to 2D and 1D space. The pairwise distances of the three points in 3D space are exactly preserved in the 2D space but not in the 1D space. If we run MDS, it would ensure a minimal difference between the actual pairwise distances and the pairwise distances of the mapped points:\n\n![illustration of multidimensional scaling](https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-0.png)\n\nMDS can be used as a preprocessing step for dimensionality reduction in classification and regression problems. \n\n\u003e Other than Multidimensional Scaling, you can also use other Dimensionality Reduction techniques, such as **Principal Component Analysis (PCA)** or **Singular Value Decomposition (SVD)**. If you'd like to read about both of them, as well has how to use them to your advantage, read our \u003ca target=\"_blank\" href=\"https://stackabuse.com/dimensionality-reduction-in-python-with-scikit-learn/\"\u003eGuide to Dimensionality Reduction in Python with Scikit-Learn\u003c/a\u003e!\n\nMDS is not only an effective technique for dimensionality reduction but also for data visualization. It maintains the same clusters and patterns of high-dimensional data in the lower-dimensional space so you can boil down, say, a 5-dimensional dataset to a 3-dimensional dataset which you can interpret much more easily and naturally. \n\nNormally the distance measure used in MDS is the \u003ca target=\"_blank\" href=\"https://stackabuse.com/calculating-euclidean-distance-with-numpy/\"\u003e**Euclidean distance**\u003c/a\u003e, however, any other suitable dissimilarity metric can be used when applying MDS.\n\nThere are two main ways to implement MDS:\n\n- **Metric MDS / Classical MDS**: This version of MDS aims to preserve the pairwise distance/dissimilarity measure as much as possible.\n- **Non-Metric MDS**: This method is applicable when only the ranks of a dissimilarity metric are known. MDS then maps the objects so that the ranks are preserved as much as possible.\n\n### Performing Multidimensional Scaling in Python with Scikit-Learn\n\nThe Scikit-Learn library's `sklearn.manifold` module implements manifold learning and data embedding techniques. We'll be using the `MDS` class of this module. The embeddings are determined using the **_stress minimization using majorization (SMACOF)_** algorithm. Some of the important parameters for setting up the `MDS` object are (this is not an exhaustive list):\n\n- `n_components`: Number of dimensions to map the points to. The default value is 2.\n- `metric`: A Boolean variable with a default value of `True` for metric MDS and `False` for its non-metric version.\n- `dissimilarity`: The default value is `euclidean`, which specifies Euclidean pairwise distances. The other possible value is `precomputed`. Using `precomputed` requires the computation of the pairwise distance matrix and using this matrix as an input to the `fit()` or `fit_transform()` function. \n\nThe four attributes associated with an `MDS` object are:\n\n- `embedding_`: Location of points in the new space.\n- `stress_`: Goodness-of-fit statistic used in MDS.\n- `dissimilarity_matrix_`: The matrix of pairwise distances/dissimilarity.\n- `n_iter_`: Number of iterations pertaining to the best goodness-of-fit measure.\n\nLike all other classes for dimensionality reduction in `scikit-learn`, the `MDS` class also implements the `fit()` and `fit_transform()` methods.\n\n#### A Simple Illustration\n\nIn this section, we show how to apply MDS using a very simple example. We'll add the import section first:\n\n```python\nfrom sklearn.manifold import MDS\nfrom matplotlib import pyplot as plt\nimport sklearn.datasets as dt\nimport seaborn as sns         \nimport numpy as np\nfrom sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n```\n\nThe code below sets up an `MDS` object and calls its method `fit_transform()`. This method returns the embedded points in 2D space. Let's print the resulting mapping:\n\n```python\nX = np.array([[0, 0, 0], [0, 0, 1], [1, 1, 1], [0, 1, 0], [0, 1, 1]])\nmds = MDS(random_state=0)\nX_transform = mds.fit_transform(X)\nprint(X_transform)\n```\n\n```plaintext\n[[ 0.72521687  0.52943352]\n [ 0.61640884 -0.48411805]\n [-0.9113603  -0.47905115]\n [-0.2190564   0.71505714]\n [-0.21120901 -0.28132146]]\n```\n\nSince the embeddings are created based on the stress minimization algorithm, we can also take a look at the `stress` variable:\n\n```python\nstress = mds.stress_\nprint(stress)\n```\n\nThis results in:\n\n```plaintext\n0.18216844548575467\n```\n\nAnother method of applying MDS is by constructing a distance matrix and applying MDS directly to this matrix as shown in the code below. This method is useful when a distance measure other than Euclidean distance is required. The code below computes the pairwise **_Manhattan distances_** (also called the city block distance or L1 distance) and transforms the data via MDS. \n\nNote the `dissimilarity` argument has been set to `precomputed`:\n\n```python\ndist_manhattan = manhattan_distances(X)\nmds = MDS(dissimilarity='precomputed', random_state=0)\n# Get the embeddings\nX_transform_L1 = mds.fit_transform(dist_manhattan)\n```\n\nThis results in:\n\n```plaintext\n[[ 0.9847767   0.84738596]\n [ 0.81047787 -0.37601578]\n [-1.104849   -1.06040621]\n [-0.29311254  0.87364759]\n [-0.39729303 -0.28461157]]\n```\n\nThough, this doesn't help us gain a good intuition as to what just happened. Humans aren't that good at crunching numbers. To gain a better understanding of the entire process, let's plot the original points and their embeddings created by preserving Euclidean distances. An original point and its corresponding embedded point are both shown in the same color:\n\n```python\ncolors = ['r', 'g', 'b', 'c', 'm']\nsize = [64, 64, 64, 64, 64]\nfig = plt.figure(2, (10,4))\nax = fig.add_subplot(121, projection='3d')\nplt.scatter(X[:,0], X[:,1], zs=X[:,2], s=size, c=colors)\nplt.title('Original Points')\n\nax = fig.add_subplot(122)\nplt.scatter(X_transform[:,0], X_transform[:,1], s=size, c=colors)\nplt.title('Embedding in 2D')\nfig.subplots_adjust(wspace=.4, hspace=0.5)\nplt.show()\n```\n\n![mapping 3d to 2d with multidimensional scaling](https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-1.png)\n\nThe plot on the right keeps the relative distances generally intact - purple, green and blue are close together, and their relative position to each other is approximately the same when compared to cyan and red.\n\n### Practical Multidimensional Scaling On *Olivetti Faces Dataset From AT\u0026T*\n\nAs a practical illustration of MDS, we'll use the \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://scikit-learn.org/0.19/datasets/olivetti_faces.html\"\u003eOlivetti faces dataset from AT\u0026T\u003c/a\u003e to show the embeddings in a space with dimensions as low as 2D. The dataset has 10 64x64 bitmap images per person, each image acquired with varying facial expressions or lighting conditions.\n\n\u003e MDS preserves the patterns in data so that different face images of **the same person are close to each other in the 2D space and far away from another person's mapped face**. \n\nTo avoid clutter, we'll take only the faces of 4 distinct people and apply MDS to them.\n\nBefore fetching the dataset and applying MDS, let's write a small function, `mapData()`, that takes the input arguments, i.e., the pairwise distance matrix `dist_matrix`, raw data matrix `X`, the class variable `y`, the Boolean variable `metric` and `title` for the graph. \n\nThe function applies MDS to the distance matrix and displays the transformed points in 2D space, with the same colored points indicating the mapped image of the same person. In a second figure, it also displays the image of each face on the graph where it is mapped in the lower-dimensional space. \n\nWe'll demonstrate MDS with different distance measures along with non-metric MDS:\n\n```python\ndef mapData(dist_matrix, X, y, metric, title):\n    mds = MDS(metric=metric, dissimilarity='precomputed', random_state=0)\n    # Get the embeddings\n    pts = mds.fit_transform(dist_matrix)\n    # Plot the embedding, colored according to the class of the points\n    fig = plt.figure(2, (15,6))\n    ax = fig.add_subplot(1,2,1)    \n    ax = sns.scatterplot(x=pts[:, 0], y=pts[:, 1],\n                         hue=y, palette=['r', 'g', 'b', 'c'])\n\n    # Add the second plot\n    ax = fig.add_subplot(1,2,2)\n    # Plot the points again\n    plt.scatter(pts[:, 0], pts[:, 1])\n    \n    # Annotate each point by its corresponding face image\n    for x, ind in zip(X, range(pts.shape[0])):\n        im = x.reshape(64,64)\n        imagebox = OffsetImage(im, zoom=0.3, cmap=plt.cm.gray)\n        i = pts[ind, 0]\n        j = pts[ind, 1]\n        ab = AnnotationBbox(imagebox, (i, j), frameon=False)\n        ax.add_artist(ab)\n    plt.title(title)    \n    plt.show()\n```\n\nThe code below fetches the Olivetti faces dataset and extracts examples with labels \u003c 4:\n\n```python\nfaces = dt.fetch_olivetti_faces()\nX_faces = faces.data\ny_faces = faces.target\nind = y_faces \u003c 4\nX_faces = X_faces[ind,:]\ny_faces = y_faces[ind]\n```\n\nAnd without further ado, let's load the data in and run our `mapData()` function on it!\n\n#### Using the Euclidean Pairwise Distances\n\nThe mapping of the Olivetti faces dataset using Euclidean distances is shown below. Euclidean distance is the default distance for MDS because of how versatile and commonly used it is:\n\n```python\ndist_euclid = euclidean_distances(X_faces)\nmapData(dist_euclid, X_faces, y_faces, True, \n        'Metric MDS with Euclidean')\n```\n\n![euclidean multidimensional scaling](https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-2.png)\n\nWe can see a nice mapping of 64x64 images to a two-dimensional space, where the class of each image is well separated from the rest in most cases. It's worth taking a moment to appreciate the fact that images residing in a 64x64 dimension space can be reduced to a two dimensional space, and still retain their informational value.\n\n#### Using the Manhattan Pairwise Distances\n\nFor comparison, we can perform MDS on the same data using the Manhatten pairwise distances. The code below uses the Manhatten distance matrix as an input to `mapData()`:\n\n```python\ndist_L1 = manhattan_distances(X_faces)\nmapData(dist_L1, X_faces, y_faces, True, \n        'Metric MDS with Manhattan')\n```\n\n![mahnattan multidimensional scaling](https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-3.png)\n\nWe can see the mapping is quite similar to the one obtained via Euclidean distances. Each class is nicely separated in the lower-dimensional space, though they're offset **a bit** differently on the plot.\n\n#### Performing Non-Metric Multidimensional Scaling\n\nAs a final example, we'll show non-metric MDS on the same dataset using Euclidean distances and see how it compares with the corresponding metric version:\n\n```python\nmapData(dist_euclid, X_faces, y_faces, False, \n        'Non-metric MDS with Euclidean')\n```\n\n![non-metric multidimensional scaling](https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-4.png)\n\nThere are quite a lot of hiccups here. We can see that this version of MDS _does not_ perform so well on the Olivetti faces dataset. \n\n\u003e This is mainly because of the quantitative nature of data. \n\nNon-metric MDS maintains the ranked distances between objects rather than the actual distances.\n\n### The n_components Parameter in MDS\n\nOne of the important hyper-parameters involved in MDS is the size of the lower-dimensional space in which the points are embedded. \n\n\u003e This would be very relevant when MDS is used as a preprocessing step for dimensionality reduction. \n\nThe question arises:\n\n\u003e Just how many dimensions do you pick, so that you reduce the dimensionality the most you can, without losing important information?\n\nA simple method to choose a value of this parameter is to run MDS on different values of `n_components` and plot the `stress_` value for each embedding. Given that the `stress_` value decreases with higher dimensions - you pick a point that has a fair tradeoff between `stress_` and `n_components`.\n\nThe code below runs MDS by varying the dimensions from 1-20 and plots the corresponding `stress_` attribute for each embedding:\n\n```python\nstress = []\n# Max value for n_components\nmax_range = 21\nfor dim in range(1, max_range):\n    # Set up the MDS object\n    mds = MDS(n_components=dim, dissimilarity='precomputed', random_state=0)\n    # Apply MDS\n    pts = mds.fit_transform(dist_euclid)\n    # Retrieve the stress value\n    stress.append(mds.stress_)\n# Plot stress vs. n_components    \nplt.plot(range(1, max_range), stress)\nplt.xticks(range(1, max_range, 2))\nplt.xlabel('n_components')\nplt.ylabel('stress')\nplt.show()\n```\n\n![finding the right number of compo](https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-5.png)\n\nWe can see that increasing the value of `n_components` decreases the stress value at the beginning and then the curve levels off. There's almost no difference between 18 and 19 dimensions, but there's a *huge* difference between 1 and 2 dimensions.\n\nThe elbow of the curve is a good choice for the optimal value of `n_components`. In this case the value can be taken at 4, which is an **amazing 0.09% reduction of features/attributes.**\n\n### Conclusions\n\nThis guide was an introduction to *Multidimensional Scaling* in Python, using Scikit-Learn. We've taken a look at how Multidimensional Scaling works, its hyperparameters, which vartiations exist and then applied it on a practical dataset.\n\nWe've used the Olivetti Faces dataset, from AT\u0026T and illustrated that images residing in a 64x64 dimensional space can be mapped to a *two-dimensional* space, and **still retain the individual patterns or clusters across images**.\n","body_html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn this guide, we'll dive into a \u003cstrong\u003edimensionality reduction\u003c/strong\u003e, \u003cstrong\u003edata embedding\u003c/strong\u003e and \u003cstrong\u003edata visualization technique\u003c/strong\u003e known as \u003cstrong\u003eMultidimensional Scaling (MDS)\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe'll be utilizing Scikit-Learn to perform Multidimensional Scaling, as it has a wonderfully simple and powerful API. Throughout the guide, we'll be using the \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://scikit-learn.org/0.19/datasets/olivetti_faces.html\"\u003eOlivetti faces dataset from AT\u0026amp;T\u003c/a\u003e to illustrate the embedding of data in a lower-dimensional space.\u003c/p\u003e\n\u003cp\u003eBy the end of the guide, you'll have a firm grasp on Multidimensional Scaling, as well as its hyperparameters and how they impact the technique.\u003c/p\u003e\n\u003ch3 id=\"whatismultidimensionalscaling\"\u003eWhat is Multidimensional Scaling?\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eMDS is a non-linear technique for embedding data in a lower-dimensional space\u003c/em\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIt maps points residing in a higher-dimensional space to a lower-dimensional space while preserving the distances between those points as much as possible. Because of this, the pairwise distances between points in the lower-dimensional space are matched closely to their actual distances.\u003c/p\u003e\n\u003cp\u003eThe following figure is an example of a possible mapping of points from 3D to 2D and 1D space. The pairwise distances of the three points in 3D space are exactly preserved in the 2D space but not in the 1D space. If we run MDS, it would ensure a minimal difference between the actual pairwise distances and the pairwise distances of the mapped points:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-0.png\" alt=\"illustration of multidimensional scaling\"\u003e\u003c/p\u003e\n\u003cp\u003eMDS can be used as a preprocessing step for dimensionality reduction in classification and regression problems.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOther than Multidimensional Scaling, you can also use other Dimensionality Reduction techniques, such as \u003cstrong\u003ePrincipal Component Analysis (PCA)\u003c/strong\u003e or \u003cstrong\u003eSingular Value Decomposition (SVD)\u003c/strong\u003e. If you'd like to read about both of them, as well has how to use them to your advantage, read our \u003ca target=\"_blank\" href=\"https://stackabuse.com/dimensionality-reduction-in-python-with-scikit-learn/\"\u003eGuide to Dimensionality Reduction in Python with Scikit-Learn\u003c/a\u003e!\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eMDS is not only an effective technique for dimensionality reduction but also for data visualization. It maintains the same clusters and patterns of high-dimensional data in the lower-dimensional space so you can boil down, say, a 5-dimensional dataset to a 3-dimensional dataset which you can interpret much more easily and naturally.\u003c/p\u003e\n\u003cp\u003eNormally the distance measure used in MDS is the \u003ca target=\"_blank\" href=\"https://stackabuse.com/calculating-euclidean-distance-with-numpy/\"\u003e\u003cstrong\u003eEuclidean distance\u003c/strong\u003e\u003c/a\u003e, however, any other suitable dissimilarity metric can be used when applying MDS.\u003c/p\u003e\n\u003cp\u003eThere are two main ways to implement MDS:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eMetric MDS / Classical MDS\u003c/strong\u003e: This version of MDS aims to preserve the pairwise distance/dissimilarity measure as much as possible.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eNon-Metric MDS\u003c/strong\u003e: This method is applicable when only the ranks of a dissimilarity metric are known. MDS then maps the objects so that the ranks are preserved as much as possible.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"performingmultidimensionalscalinginpythonwithscikitlearn\"\u003ePerforming Multidimensional Scaling in Python with Scikit-Learn\u003c/h3\u003e\n\u003cp\u003eThe Scikit-Learn library's \u003ccode\u003esklearn.manifold\u003c/code\u003e module implements manifold learning and data embedding techniques. We'll be using the \u003ccode\u003eMDS\u003c/code\u003e class of this module. The embeddings are determined using the \u003cstrong\u003e\u003cem\u003estress minimization using majorization (SMACOF)\u003c/em\u003e\u003c/strong\u003e algorithm. Some of the important parameters for setting up the \u003ccode\u003eMDS\u003c/code\u003e object are (this is not an exhaustive list):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003en_components\u003c/code\u003e: Number of dimensions to map the points to. The default value is 2.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emetric\u003c/code\u003e: A Boolean variable with a default value of \u003ccode\u003eTrue\u003c/code\u003e for metric MDS and \u003ccode\u003eFalse\u003c/code\u003e for its non-metric version.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edissimilarity\u003c/code\u003e: The default value is \u003ccode\u003eeuclidean\u003c/code\u003e, which specifies Euclidean pairwise distances. The other possible value is \u003ccode\u003eprecomputed\u003c/code\u003e. Using \u003ccode\u003eprecomputed\u003c/code\u003e requires the computation of the pairwise distance matrix and using this matrix as an input to the \u003ccode\u003efit()\u003c/code\u003e or \u003ccode\u003efit_transform()\u003c/code\u003e function.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe four attributes associated with an \u003ccode\u003eMDS\u003c/code\u003e object are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eembedding_\u003c/code\u003e: Location of points in the new space.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003estress_\u003c/code\u003e: Goodness-of-fit statistic used in MDS.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edissimilarity_matrix_\u003c/code\u003e: The matrix of pairwise distances/dissimilarity.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003en_iter_\u003c/code\u003e: Number of iterations pertaining to the best goodness-of-fit measure.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLike all other classes for dimensionality reduction in \u003ccode\u003escikit-learn\u003c/code\u003e, the \u003ccode\u003eMDS\u003c/code\u003e class also implements the \u003ccode\u003efit()\u003c/code\u003e and \u003ccode\u003efit_transform()\u003c/code\u003e methods.\u003c/p\u003e\n\u003ch4 id=\"asimpleillustration\"\u003eA Simple Illustration\u003c/h4\u003e\n\u003cp\u003eIn this section, we show how to apply MDS using a very simple example. We'll add the import section first:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.manifold \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e MDS\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e matplotlib \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e sklearn.datasets \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e dt\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e seaborn \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e sns         \n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.metrics.pairwise \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e manhattan_distances, euclidean_distances\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e matplotlib.offsetbox \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OffsetImage, AnnotationBbox\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe code below sets up an \u003ccode\u003eMDS\u003c/code\u003e object and calls its method \u003ccode\u003efit_transform()\u003c/code\u003e. This method returns the embedded points in 2D space. Let's print the resulting mapping:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eX = np.array([[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], [\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]])\nmds = MDS(random_state=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\nX_transform = mds.fit_transform(X)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(X_transform)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e[[ 0.72521687  0.52943352]\n [ 0.61640884 -0.48411805]\n [-0.9113603  -0.47905115]\n [-0.2190564   0.71505714]\n [-0.21120901 -0.28132146]]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSince the embeddings are created based on the stress minimization algorithm, we can also take a look at the \u003ccode\u003estress\u003c/code\u003e variable:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003estress = mds.stress_\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(stress)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e0.18216844548575467\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnother method of applying MDS is by constructing a distance matrix and applying MDS directly to this matrix as shown in the code below. This method is useful when a distance measure other than Euclidean distance is required. The code below computes the pairwise \u003cstrong\u003e\u003cem\u003eManhattan distances\u003c/em\u003e\u003c/strong\u003e (also called the city block distance or L1 distance) and transforms the data via MDS.\u003c/p\u003e\n\u003cp\u003eNote the \u003ccode\u003edissimilarity\u003c/code\u003e argument has been set to \u003ccode\u003eprecomputed\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003edist_manhattan = manhattan_distances(X)\nmds = MDS(dissimilarity=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;precomputed\u0026#x27;\u003c/span\u003e, random_state=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# Get the embeddings\u003c/span\u003e\nX_transform_L1 = mds.fit_transform(dist_manhattan)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e[[ 0.9847767   0.84738596]\n [ 0.81047787 -0.37601578]\n [-1.104849   -1.06040621]\n [-0.29311254  0.87364759]\n [-0.39729303 -0.28461157]]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThough, this doesn't help us gain a good intuition as to what just happened. Humans aren't that good at crunching numbers. To gain a better understanding of the entire process, let's plot the original points and their embeddings created by preserving Euclidean distances. An original point and its corresponding embedded point are both shown in the same color:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003ecolors = [\u003cspan class=\"hljs-string\"\u003e\u0026#x27;r\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;g\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;b\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;c\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;m\u0026#x27;\u003c/span\u003e]\nsize = [\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e]\nfig = plt.figure(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, (\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\nax = fig.add_subplot(\u003cspan class=\"hljs-number\"\u003e121\u003c/span\u003e, projection=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;3d\u0026#x27;\u003c/span\u003e)\nplt.scatter(X[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], X[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], zs=X[:,\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e], s=size, c=colors)\nplt.title(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Original Points\u0026#x27;\u003c/span\u003e)\n\nax = fig.add_subplot(\u003cspan class=\"hljs-number\"\u003e122\u003c/span\u003e)\nplt.scatter(X_transform[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], X_transform[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], s=size, c=colors)\nplt.title(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Embedding in 2D\u0026#x27;\u003c/span\u003e)\nfig.subplots_adjust(wspace=\u003cspan class=\"hljs-number\"\u003e.4\u003c/span\u003e, hspace=\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e)\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-1.png\" alt=\"mapping 3d to 2d with multidimensional scaling\"\u003e\u003c/p\u003e\n\u003cp\u003eThe plot on the right keeps the relative distances generally intact - purple, green and blue are close together, and their relative position to each other is approximately the same when compared to cyan and red.\u003c/p\u003e\n\u003ch3 id=\"practicalmultidimensionalscalingonolivettifacesdatasetfromatt\"\u003ePractical Multidimensional Scaling On \u003cem\u003eOlivetti Faces Dataset From AT\u0026amp;T\u003c/em\u003e\u003c/h3\u003e\n\u003cp\u003eAs a practical illustration of MDS, we'll use the \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://scikit-learn.org/0.19/datasets/olivetti_faces.html\"\u003eOlivetti faces dataset from AT\u0026amp;T\u003c/a\u003e to show the embeddings in a space with dimensions as low as 2D. The dataset has 10 64x64 bitmap images per person, each image acquired with varying facial expressions or lighting conditions.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMDS preserves the patterns in data so that different face images of \u003cstrong\u003ethe same person are close to each other in the 2D space and far away from another person's mapped face\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eTo avoid clutter, we'll take only the faces of 4 distinct people and apply MDS to them.\u003c/p\u003e\n\u003cp\u003eBefore fetching the dataset and applying MDS, let's write a small function, \u003ccode\u003emapData()\u003c/code\u003e, that takes the input arguments, i.e., the pairwise distance matrix \u003ccode\u003edist_matrix\u003c/code\u003e, raw data matrix \u003ccode\u003eX\u003c/code\u003e, the class variable \u003ccode\u003ey\u003c/code\u003e, the Boolean variable \u003ccode\u003emetric\u003c/code\u003e and \u003ccode\u003etitle\u003c/code\u003e for the graph.\u003c/p\u003e\n\u003cp\u003eThe function applies MDS to the distance matrix and displays the transformed points in 2D space, with the same colored points indicating the mapped image of the same person. In a second figure, it also displays the image of each face on the graph where it is mapped in the lower-dimensional space.\u003c/p\u003e\n\u003cp\u003eWe'll demonstrate MDS with different distance measures along with non-metric MDS:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-function\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title\"\u003emapData\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003edist_matrix, X, y, metric, title\u003c/span\u003e):\u003c/span\u003e\n    mds = MDS(metric=metric, dissimilarity=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;precomputed\u0026#x27;\u003c/span\u003e, random_state=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n    \u003cspan class=\"hljs-comment\"\u003e# Get the embeddings\u003c/span\u003e\n    pts = mds.fit_transform(dist_matrix)\n    \u003cspan class=\"hljs-comment\"\u003e# Plot the embedding, colored according to the class of the points\u003c/span\u003e\n    fig = plt.figure(\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, (\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e6\u003c/span\u003e))\n    ax = fig.add_subplot(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)    \n    ax = sns.scatterplot(x=pts[:, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], y=pts[:, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e],\n                         hue=y, palette=[\u003cspan class=\"hljs-string\"\u003e\u0026#x27;r\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;g\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;b\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;c\u0026#x27;\u003c/span\u003e])\n\n    \u003cspan class=\"hljs-comment\"\u003e# Add the second plot\u003c/span\u003e\n    ax = fig.add_subplot(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e)\n    \u003cspan class=\"hljs-comment\"\u003e# Plot the points again\u003c/span\u003e\n    plt.scatter(pts[:, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], pts[:, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n    \n    \u003cspan class=\"hljs-comment\"\u003e# Annotate each point by its corresponding face image\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e x, ind \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(X, \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(pts.shape[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])):\n        im = x.reshape(\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e64\u003c/span\u003e)\n        imagebox = OffsetImage(im, zoom=\u003cspan class=\"hljs-number\"\u003e0.3\u003c/span\u003e, cmap=plt.cm.gray)\n        i = pts[ind, \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e]\n        j = pts[ind, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n        ab = AnnotationBbox(imagebox, (i, j), frameon=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\n        ax.add_artist(ab)\n    plt.title(title)    \n    plt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe code below fetches the Olivetti faces dataset and extracts examples with labels \u0026lt; 4:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efaces = dt.fetch_olivetti_faces()\nX_faces = faces.data\ny_faces = faces.target\nind = y_faces \u0026lt; \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e\nX_faces = X_faces[ind,:]\ny_faces = y_faces[ind]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd without further ado, let's load the data in and run our \u003ccode\u003emapData()\u003c/code\u003e function on it!\u003c/p\u003e\n\u003ch4 id=\"usingtheeuclideanpairwisedistances\"\u003eUsing the Euclidean Pairwise Distances\u003c/h4\u003e\n\u003cp\u003eThe mapping of the Olivetti faces dataset using Euclidean distances is shown below. Euclidean distance is the default distance for MDS because of how versatile and commonly used it is:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003edist_euclid = euclidean_distances(X_faces)\nmapData(dist_euclid, X_faces, y_faces, \u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, \n        \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Metric MDS with Euclidean\u0026#x27;\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-2.png\" alt=\"euclidean multidimensional scaling\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can see a nice mapping of 64x64 images to a two-dimensional space, where the class of each image is well separated from the rest in most cases. It's worth taking a moment to appreciate the fact that images residing in a 64x64 dimension space can be reduced to a two dimensional space, and still retain their informational value.\u003c/p\u003e\n\u003ch4 id=\"usingthemanhattanpairwisedistances\"\u003eUsing the Manhattan Pairwise Distances\u003c/h4\u003e\n\u003cp\u003eFor comparison, we can perform MDS on the same data using the Manhatten pairwise distances. The code below uses the Manhatten distance matrix as an input to \u003ccode\u003emapData()\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003edist_L1 = manhattan_distances(X_faces)\nmapData(dist_L1, X_faces, y_faces, \u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, \n        \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Metric MDS with Manhattan\u0026#x27;\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-3.png\" alt=\"mahnattan multidimensional scaling\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can see the mapping is quite similar to the one obtained via Euclidean distances. Each class is nicely separated in the lower-dimensional space, though they're offset \u003cstrong\u003ea bit\u003c/strong\u003e differently on the plot.\u003c/p\u003e\n\u003ch4 id=\"performingnonmetricmultidimensionalscaling\"\u003ePerforming Non-Metric Multidimensional Scaling\u003c/h4\u003e\n\u003cp\u003eAs a final example, we'll show non-metric MDS on the same dataset using Euclidean distances and see how it compares with the corresponding metric version:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003emapData(dist_euclid, X_faces, y_faces, \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e, \n        \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Non-metric MDS with Euclidean\u0026#x27;\u003c/span\u003e)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-4.png\" alt=\"non-metric multidimensional scaling\"\u003e\u003c/p\u003e\n\u003cp\u003eThere are quite a lot of hiccups here. We can see that this version of MDS \u003cem\u003edoes not\u003c/em\u003e perform so well on the Olivetti faces dataset.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis is mainly because of the quantitative nature of data.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNon-metric MDS maintains the ranked distances between objects rather than the actual distances.\u003c/p\u003e\n\u003ch3 id=\"then_componentsparameterinmds\"\u003eThe n_components Parameter in MDS\u003c/h3\u003e\n\u003cp\u003eOne of the important hyper-parameters involved in MDS is the size of the lower-dimensional space in which the points are embedded.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis would be very relevant when MDS is used as a preprocessing step for dimensionality reduction.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe question arises:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eJust how many dimensions do you pick, so that you reduce the dimensionality the most you can, without losing important information?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eA simple method to choose a value of this parameter is to run MDS on different values of \u003ccode\u003en_components\u003c/code\u003e and plot the \u003ccode\u003estress_\u003c/code\u003e value for each embedding. Given that the \u003ccode\u003estress_\u003c/code\u003e value decreases with higher dimensions - you pick a point that has a fair tradeoff between \u003ccode\u003estress_\u003c/code\u003e and \u003ccode\u003en_components\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe code below runs MDS by varying the dimensions from 1-20 and plots the corresponding \u003ccode\u003estress_\u003c/code\u003e attribute for each embedding:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003estress = []\n\u003cspan class=\"hljs-comment\"\u003e# Max value for n_components\u003c/span\u003e\nmax_range = \u003cspan class=\"hljs-number\"\u003e21\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e dim \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, max_range):\n    \u003cspan class=\"hljs-comment\"\u003e# Set up the MDS object\u003c/span\u003e\n    mds = MDS(n_components=dim, dissimilarity=\u003cspan class=\"hljs-string\"\u003e\u0026#x27;precomputed\u0026#x27;\u003c/span\u003e, random_state=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\n    \u003cspan class=\"hljs-comment\"\u003e# Apply MDS\u003c/span\u003e\n    pts = mds.fit_transform(dist_euclid)\n    \u003cspan class=\"hljs-comment\"\u003e# Retrieve the stress value\u003c/span\u003e\n    stress.append(mds.stress_)\n\u003cspan class=\"hljs-comment\"\u003e# Plot stress vs. n_components    \u003c/span\u003e\nplt.plot(\u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, max_range), stress)\nplt.xticks(\u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, max_range, \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e))\nplt.xlabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;n_components\u0026#x27;\u003c/span\u003e)\nplt.ylabel(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;stress\u0026#x27;\u003c/span\u003e)\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/guide-to-multidimensional-scaling-in-python-with-scikit-learn-5.png\" alt=\"finding the right number of compo\"\u003e\u003c/p\u003e\n\u003cp\u003eWe can see that increasing the value of \u003ccode\u003en_components\u003c/code\u003e decreases the stress value at the beginning and then the curve levels off. There's almost no difference between 18 and 19 dimensions, but there's a \u003cem\u003ehuge\u003c/em\u003e difference between 1 and 2 dimensions.\u003c/p\u003e\n\u003cp\u003eThe elbow of the curve is a good choice for the optimal value of \u003ccode\u003en_components\u003c/code\u003e. In this case the value can be taken at 4, which is an \u003cstrong\u003eamazing 0.09% reduction of features/attributes.\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"conclusions\"\u003eConclusions\u003c/h3\u003e\n\u003cp\u003eThis guide was an introduction to \u003cem\u003eMultidimensional Scaling\u003c/em\u003e in Python, using Scikit-Learn. We've taken a look at how Multidimensional Scaling works, its hyperparameters, which vartiations exist and then applied it on a practical dataset.\u003c/p\u003e\n\u003cp\u003eWe've used the Olivetti Faces dataset, from AT\u0026amp;T and illustrated that images residing in a 64x64 dimensional space can be mapped to a \u003cem\u003etwo-dimensional\u003c/em\u003e space, and \u003cstrong\u003estill retain the individual patterns or clusters across images\u003c/strong\u003e.\u003c/p\u003e\n","parent_id":null,"type":"article","status":"published","visibility":"public","img_feature":null,"is_featured":false,"locale":"en","custom_excerpt":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"comment_id":"604515cf2c5e1a08f1ec8f8d","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In this guide, we'll take a look at Multidimensional Scaling in Python with Scikit-Learn, with practical applications to the Olivetta Faces dataset.","read_time_min":8,"published_by":16,"published_at":1629801000000,"created_by":null,"updated_by":null,"created_at":1615140303000,"updated_at":1631997076778,"contributors":[{"id":16,"name":"David Landup","slug":"david","email":"thealduinmaster@gmail.com","password_hash":"$2a$10$W/oMJdUBSTeG3trWAHa1xO0pQruxuLgD/6hS7VuxPafcmAxeBXmVi","role_id":2,"img_profile":"//s3.stackabuse.com/media/users/865cd7d217ea11c9d9555c4f666e2d73.jpg","img_cover":null,"bio_md":"Entrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs. \n\nGreat passion for accessible education and promotion of reason, science, humanism, and progress.","bio_html":"\u003cp\u003eEntrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs.\u003c/p\u003e\n\u003cp\u003eGreat passion for accessible education and promotion of reason, science, humanism, and progress.\u003c/p\u003e\n","website":"https://www.upwork.com/freelancers/~017664e499a2766871","location":"Serbia","facebook":"","twitter":"","github":null,"status":"active","locale":null,"last_seen_at":1622229427000,"created_by":null,"updated_by":null,"created_at":1534532687000,"updated_at":1640861394795,"role":"editor","secret_token":"2a2be92558fae38f89cfbd0c6a4ba90c","is_email_confirmed":false,"_pivot_content_id":893,"_pivot_user_id":16,"_pivot_role":"editor","_pivot_sort_order":1},{"id":69,"name":"Mehreen Saeed","slug":"mehreen","email":"courses.at.fast@gmail.com","password_hash":"$2a$10$SyXn0J3GubHzykx/cHqQSOwdP9JM6UHU3DA5R5mmNZvsTZ.zza97C","role_id":4,"img_profile":"//s3.stackabuse.com/media/users/89430606bd6f853f01889aa0bd28a437.png","img_cover":"//s3.stackabuse.com/media/users/7356adc6f29d109e0cfcaa44a662b2a1.JPG","bio_md":"I am an educator and I love mathematics and data science!","bio_html":"\u003cp\u003eI am an educator and I love mathematics and data science!\u003c/p\u003e\n","website":null,"location":"Pakistan","facebook":null,"twitter":null,"github":null,"status":"active","locale":null,"last_seen_at":1617038327000,"created_by":null,"updated_by":null,"created_at":1599395951000,"updated_at":1629840517435,"role":"author","secret_token":"7c90f1f392be6c321d7e2109f9642055","is_email_confirmed":false,"_pivot_content_id":893,"_pivot_user_id":69,"_pivot_role":"author","_pivot_sort_order":0}],"tags":[{"id":9,"name":"python","slug":"python","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1431358631000,"updated_at":1431358631000,"_pivot_content_id":893,"_pivot_tag_id":9,"_pivot_sort_order":0},{"id":57,"name":"artificial intelligence","slug":"artificial-intelligence","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1442517465000,"updated_at":1442517465000,"_pivot_content_id":893,"_pivot_tag_id":57,"_pivot_sort_order":5},{"id":75,"name":"machine learning","slug":"machine-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507305534000,"updated_at":1507305534000,"_pivot_content_id":893,"_pivot_tag_id":75,"_pivot_sort_order":4},{"id":76,"name":"scikit-learn","slug":"scikit-learn","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507308143000,"updated_at":1507308143000,"_pivot_content_id":893,"_pivot_tag_id":76,"_pivot_sort_order":1},{"id":106,"name":"data science","slug":"data-science","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1536865458000,"updated_at":1536865458000,"_pivot_content_id":893,"_pivot_tag_id":106,"_pivot_sort_order":2},{"id":213,"name":"data visualization","slug":"data-visualization","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1603629084000,"updated_at":1603629084000,"_pivot_content_id":893,"_pivot_tag_id":213,"_pivot_sort_order":3}],"_pivot_tag_id":75,"_pivot_content_id":893},{"id":1007,"old_id":null,"uuid":"32e67dd8-8c5b-444b-9658-4406dd1ebb82","title":"Feature Scaling Data with Scikit-Learn for Machine Learning in Python","slug":"feature-scaling-data-with-scikit-learn-for-machine-learning-in-python","body_md":"### Introduction\n\nPreprocessing data is an often overlooked key step in Machine Learning. In fact - it's *as important* as the shiny model you want to fit with it. \n\n\u003e **Garbage in - garbage out.**\n\nYou can have the *best* model crafted for any sort of problem - if you feed it garbage, it'll spew out garbage. It's worth noting that _\"garbage\"_ doesn't refer to random data. It's a harsh label we attach to any data that doesn't allow the model to do its best - some more so than other. That being said - the same data can be bad for one model, but great for another. *Generally*, various Machine Learning models don't generalize as well on data with high scale variance, so you'll typically want to iron it out before feeding it into a model.\n\n\u003e **_Normalization_** and **_Standardization_** are two techniques commonly used during *Data Preprocessing* to adjust the features to a common scale.\n\nIn this guide, we'll dive into what Feature Scaling is and scale the features of a dataset to a more fitting scale. Then, we'll train a `SGDRegressor` model on the original and scaled data to check whether it had much effect on this specific dataset.\n\n### What is Feature Scaling - Normalization and Standardization\n\n**_Scaling_** or **_Feature Scaling_** is the process of changinng the scale of certain features to a common one. This is typically achieved through **normalization** and **standardization** (scaling techniques).\n\n- **_Normalization_** is the process of scaling data into a range of [0, 1]. It's more useful and common for regression tasks.\n\n$$\nx' = \\frac{x-x_{min}}{x_{max} - x_{min}}\n$$\n\n- **_Standardization_** is the process of scaling data so that they have a *mean value of 0* and a *standard deviation of 1*. It's more useful and common for classification tasks.\n\n$$\nx' = \\frac{x-\\mu}{\\sigma}\n$$\n\nA normal distribution with these values is called a *standard normal distribution*. \n\nIt's worth noting that standardizing data doesn't guarantee that it'll be within the [0, 1] range. It most likely won't be - which can be a problem for certain algorithms that expect this range.\n\nTo perform standardization, Scikit-Learn provides us with the `StandardScaler` class.\n\nNormalization is also known as *Min-Max Scaling* and Scikit-Learn provides the `MinMaxScaler` for this purpose. On the other hand, it also provides a `Normalizer`, which can make things a bit confusing.\n\n**Note:** The `Normalizer` class **doesn't perform** the same scaling as `MinMaxScaler`. `Normalizer` works on *rows*, not features, and it scales them independently.\n\n### When to Perform Feature Scaling?\n\n\u003e Feature Scaling doesn't *guarantee* better model performance for **_all_** models.\n\nFor instance, Feature Scaling doesn't do much if the scale doesn't matter. For \u003ca target=\"_blank\" href=\"https://stackabuse.com/k-means-clustering-with-scikit-learn\"\u003e**_K-Means Clustering_**\u003c/a\u003e, the \u003ca target=\"_blank\" href=\"\"\u003e_Euclidean distance_\u003c/a\u003e is important, so Feature Scaling makes a huge impact. It also makes a huge impact for any algorithms that rely on gradients, such as linear models that are fitted by minimizing loss with \u003ca target=\"_blank\" href=\"\"\u003eGradient Descent\u003c/a\u003e.\n\n\u003ca target=\"_blank\" href=\"https://stackabuse.com/implementing-pca-in-python-with-scikit-learn\"\u003e**_Principal Component Analysis (PCA)_**\u003c/a\u003e also suffers from data that isn't scaled properly.\n\nIn the case of Scikit-Learn - you won't see any tangible difference with a \u003ca target=\"_blank\" href=\"https://stackabuse.com/linear-regression-in-python-with-scikit-learn\"\u003e`LinearRegression`\u003c/a\u003e, but will see a substantial difference with a `SGDRegressor`, because a `SGDRegressor`, which is also a linear model, depends on **_Stochastic Gradient Descent_** to fit the parameters.\n\nA \u003ca target=\"_blank\" href=\"https://stackabuse.com/decision-trees-in-python-with-scikit-learn\"\u003e**_tree-based model_**\u003c/a\u003e won't suffer from unscaled data, because scale doesn't affect them at all, but if you perform \u003ca target=\"_blank\" href=\"https://stackabuse.com/gradient-boosting-classifiers-in-python-with-scikit-learn\"\u003e**_Gradient Boosting on Classifiers_**\u003c/a\u003e, the scale *does* affect learning.\n\n### Importing Data and Exploratory Data Analysis\n\nWe'll be working with the \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://www.kaggle.com/prevek18/ames-housing-dataset\"\u003eAmes Housing Dataset\u003c/a\u003e which contains 79 features regarding houses sold in Ames, Iowa, as well as their sale price. This is a great dataset for basic and advanced regression training, since there are a lot of features to tweak and fiddle with, which ultimately usually affect the sales price in some way or the other.\n\n\u003e If you'd like to read our series of articles on Deep Learning with Keras, which produces a Deep Learning model to predict these prices more accurarely, read our \u003ca target=\"_blank\" href=\"https://stackabuse.com/hands-on-house-price-prediction-deep-learning-in-python-with-keras\"\u003eDeep Learning in Python with Keras\u003c/a\u003e series.\n\nLet's import the data and take a look at some of the features we'll be using:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the Dataset\ndf = pd.read_csv('AmesHousing.csv')\n# Single out a couple of predictor variables and labels ('SalePrice' is our target label set)\nx = df[['Gr Liv Area', 'Overall Qual']].values\ny = df['SalePrice'].values\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 4))\n\nax[0].scatter(x[:,0], y)\nax[1].scatter(x[:,1], y)\n\nplt.show()\n```\n\nThere's a clear strong positive correlation between the *\"Gr Liv Area\"* feature and the *\"SalePrice\"* feature - with only a couple of outliers. There's also a strong positive correlation between the *\"Overall Qual\"* feature and the *\"SalePrice\"*:\n\n![](https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-1.png)\n\nThough these are on a much different scale - the *\"Gr Liv Area\"* spans up to ~5000 (measured in square feet), while the *\"Overall Qual\"* feature spans up to 10 (discrete categories of quality). If we were to plot these two on the same axes, we wouldn't be able to tell much about the *\"Overall Qual\"* feature:\n\n```python\nfig, ax = plt.subplots(figsize=(12, 4))\n\nax.scatter(x[:,0], y)\nax.scatter(x[:,1], y)\n```\n\n![](https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-2.png)\n\nAdditionally, if we were to plot their distributions, we wouldn't have much luck either:\n\n```python\nfig, ax = plt.subplots(figsize=(12, 4))\n\nax.hist(x[:,0])\nax.hist(x[:,1])\n```\n\n![](https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-3.png)\n\nThe scale of these features is so different that we can't really make much out by plotting them together. *This* is where feature scaling kicks in.\n\n### StandardScaler\n\nThe `StandardScaler` class is used to transform the data by *standardizing* it. Let's import it and *scale* the data via its `fit_transform()` method:\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Import StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\nfig, ax = plt.subplots(figsize=(12, 4))\n\nscaler = StandardScaler()\nx_std = scaler.fit_transform(x)\n\nax.hist(x_std[:,0])\nax.hist(x_std[:,1])\n```\n\n**Note:** We're using `fit_transform()` on the entirety of the dataset here to demonstrate the usage of the `StandardScaler` class and visualize its effects. When building a model or pipeline, like we will shortly - you shouldn't `fit_transform()` the entirety of the dataset, but rather, just `fit()` the training data, and `transform()` the testing data.\n\nRunning this piece of code will calculate the **μ** and **σ** parameters - this process is known as *fitting the data*, and then *transform it* so that these values correspond to _1_ and _0_ respectively.\n\nWhen we plot the distributions of these features now, we'll be greeted with a much more manageable plot:\n\n![](https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-4.png)\n\nIf we were to plot these through Scatter Plots yet again, we'd perhaps more clearly see the effects of the standarization:\n\n```python\nfig, ax = plt.subplots(figsize=(12, 4))\n\nscaler = StandardScaler()\nx_std = scaler.fit_transform(x)\n\nax.scatter(x_std[:,0], y)\nax.scatter(x_std[:,1], y)\n```\n\n![](https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-5.png)\n\n### MinMaxScaler\n\nTo *normalize* features, we use the `MinMaxScaler` class. It works in much the same way as `StandardScaler`, but uses a fundementally different approach to scaling the data:\n\n```python\nfig, ax = plt.subplots(figsize=(12, 4))\n\nscaler = MinMaxScaler()\nx_minmax = scaler.fit_transform(x)\n\nax.hist(x_minmax [:,0])\nax.hist(x_minmax [:,1])\n```\n\nThey are *normalized* in the range of *[0, 1]*. If we were to plot the distributions again, we'd be greeted with:\n\n![](https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-6.png)\n\nThe *skewness* of the distribution is preserved, unlike with *standardization* which makes them overlap much more. Though, if we were to plot the data through Scatter Plots again:\n\n```python\nfig, ax = plt.subplots(figsize=(12, 4))\n\nscaler = MinMaxScaler()\nx_minmax = scaler.fit_transform(x)\n\nax.scatter(x_minmax [:,0], y)\nax.scatter(x_minmax [:,1], y)\n```\n\nWe'd be able to see the strong positive correlation between both of these with the *\"SalePrice\"* with the feature, but the *\"Overall Qual\"* feature awkwardly overextends to the right, because the outliers of the *\"Gr Liv Area\"* feature forced the majority of its distribution to trail on the left-hand side.\n\n![](https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-7.png)\n\n### Effects of Outliers\n\nBoth *normalization* and *standardization* are sensitive to outliers - it's enough for the dataset to have a *single* outlier that's way out there to make things look really weird. Let's add a synthetic entry to the *\"Gr Liv Area\"* feature to see how it affects the scaling process:\n\n```python\nfig, ax = plt.subplots(figsize=(12, 4))\n\nscaler = MinMaxScaler()\nx_minmax = scaler.fit_transform(x)\n\nax.scatter(x_minmax [:,0], y)\n```\n\n![](https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-8.png)\n\nThe single outlier, on the far right of the plot has really affected the new distribution. *All* of the data, except for the outlier is located in the first two quartiles:\n\n```python\nfig, ax = plt.subplots(figsize=(12, 4))\n\nscaler = MinMaxScaler()\nx_minmax = scaler.fit_transform(x)\n\nax.hist(x_minmax [:,0])\n```\n\n![](https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-9.png)\n\n### Feature Scaling Through Scikit-Learn Pipelines\n\nFinally, let's go ahead and train a model with and without scaling features beforehand. When working on Machine Learning projects - we typically have a *pipeline* for the data before it arrives at the model we're fitting.\n\nWe'll be using the `Pipeline` class which lets us minimize and, to a degree, automate this process, even though we have just two steps - scaling the data, and fitting a model:\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nimport sklearn.metrics as metrics\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import Data\ndf = pd.read_csv('AmesHousing.csv')\nx = df[['Gr Liv Area', 'Overall Qual']].values\ny = df['SalePrice'].values\n\n# Split into a training and testing set\nX_train, X_test, Y_train, Y_test = train_test_split(x, y)\n\n# Define the pipeline for scaling and model fitting\npipeline = Pipeline([\n    (\"MinMax Scaling\", MinMaxScaler()),\n    (\"SGD Regression\", SGDRegressor())\n])\n\n# Scale the data and fit the model\npipeline.fit(X_train, Y_train)\n\n# Evaluate the model\nY_pred = pipeline.predict(X_test)\nprint('Mean Absolute Error: ', mean_absolute_error(Y_pred, Y_test))\nprint('Score', pipeline.score(X_test, Y_test))\n```\n\nThis results in:\n\n```plaintext\nMean Absolute Error:  27614.031131858766\nScore 0.7536086980531018\n```\n\nThe mean absolute error is _~27000_, and the accuracy score is _~75%_. This means that on average, our model misses the price by _$27000_, which doesn't sound that bad, although, it could be improved beyond this.\n\nMost notably, the type of model we used is a bit too rigid and we haven't fed many features in so these two are most definitely the places that can be improved.\n\nThough - let's not lose focus of what we're interested in. How does this model perform *without* Feature Scaling? Let's modify the pipeline to skip the scaling step:\n\n```python\npipeline = Pipeline([\n    (\"SGD Regression\", SGDRegressor())\n])\n```\n\nWhat happens might surprise you:\n\n```plaintext\nMean Absolute Error:  1260383513716205.8\nScore -2.772781517117743e+20\n```\n\nWe've gone from **_~75%_** accuracy to **_~-3%_** accuracy just by skipping to scale our features. *Any* learning algorithm that depends on the scale of features will typically see major benefits from Feature Scaling. Those that don't, won't see much of a difference. \n\nFor instance, if we train a `LinearRegression` on this same data, with and without scaling, we'll see unremarkable results on the behalf of the scaling, and decent results on behalf of the model itself:\n\n```python\npipeline1 = Pipeline([\n    (\"Linear Regression\", LinearRegression())\n])\n\npipeline2 = Pipeline([\n    (\"Scaling\", StandardScaler()),\n    (\"Linear Regression\", LinearRegression())\n])\n\npipeline1.fit(X_train, Y_train)\npipeline2.fit(X_train, Y_train)\n\nY_pred1 = pipeline1.predict(X_test)\nY_pred2 = pipeline2.predict(X_test)\n\nprint('Pipeline 1 Mean Absolute Error: ', mean_absolute_error(Y_pred1, Y_test))\nprint('Pipeline 1 Score', pipeline1.score(X_test, Y_test))\n\nprint('Pipeline 2 Mean Absolute Error: ', mean_absolute_error(Y_pred2, Y_test))\nprint('Pipeline 2 Score', pipeline2.score(X_test, Y_test))\n```\n\n```plaintext\nPipeline 1 Mean Absolute Error:  27706.61376199076\nPipeline 1 Score 0.7641840816646945\n\nPipeline 2 Mean Absolute Error:  27706.613761990764\nPipeline 2 Score 0.7641840816646945\n```\n\n### Conclusion\n\nFeature Scaling is the process of scaling the values of features to a more managable scale. You'll typically perform it before feeding these features into algorithms that are affected by scale, during the preprocessing phase.\n\nIn this guide, we've taken a look at what Feature Scaling is and how to perform it in Python with Scikit-Learn, using `StandardScaler` to perform standardization and `MinMaxScaler` to perform normalization. We've also taken a look at how outliers affect these processes and the difference between a scale-sensitive model being trained with and without Feature Scaling.","body_html":"\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003ePreprocessing data is an often overlooked key step in Machine Learning. In fact - it's \u003cem\u003eas important\u003c/em\u003e as the shiny model you want to fit with it.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eGarbage in - garbage out.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eYou can have the \u003cem\u003ebest\u003c/em\u003e model crafted for any sort of problem - if you feed it garbage, it'll spew out garbage. It's worth noting that \u003cem\u003e\u0026quot;garbage\u0026quot;\u003c/em\u003e doesn't refer to random data. It's a harsh label we attach to any data that doesn't allow the model to do its best - some more so than other. That being said - the same data can be bad for one model, but great for another. \u003cem\u003eGenerally\u003c/em\u003e, various Machine Learning models don't generalize as well on data with high scale variance, so you'll typically want to iron it out before feeding it into a model.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eNormalization\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eStandardization\u003c/em\u003e\u003c/strong\u003e are two techniques commonly used during \u003cem\u003eData Preprocessing\u003c/em\u003e to adjust the features to a common scale.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn this guide, we'll dive into what Feature Scaling is and scale the features of a dataset to a more fitting scale. Then, we'll train a \u003ccode\u003eSGDRegressor\u003c/code\u003e model on the original and scaled data to check whether it had much effect on this specific dataset.\u003c/p\u003e\n\u003ch3 id=\"whatisfeaturescalingnormalizationandstandardization\"\u003eWhat is Feature Scaling - Normalization and Standardization\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eScaling\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eFeature Scaling\u003c/em\u003e\u003c/strong\u003e is the process of changinng the scale of certain features to a common one. This is typically achieved through \u003cstrong\u003enormalization\u003c/strong\u003e and \u003cstrong\u003estandardization\u003c/strong\u003e (scaling techniques).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eNormalization\u003c/em\u003e\u003c/strong\u003e is the process of scaling data into a range of [0, 1]. It's more useful and common for regression tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\u003cbr\u003e\nx' = \\frac{x-x_{min}}{x_{max} - x_{min}}\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eStandardization\u003c/em\u003e\u003c/strong\u003e is the process of scaling data so that they have a \u003cem\u003emean value of 0\u003c/em\u003e and a \u003cem\u003estandard deviation of 1\u003c/em\u003e. It's more useful and common for classification tasks.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e$$\u003cbr\u003e\nx' = \\frac{x-\\mu}{\\sigma}\u003cbr\u003e\n$$\u003c/p\u003e\n\u003cp\u003eA normal distribution with these values is called a \u003cem\u003estandard normal distribution\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eIt's worth noting that standardizing data doesn't guarantee that it'll be within the [0, 1] range. It most likely won't be - which can be a problem for certain algorithms that expect this range.\u003c/p\u003e\n\u003cp\u003eTo perform standardization, Scikit-Learn provides us with the \u003ccode\u003eStandardScaler\u003c/code\u003e class.\u003c/p\u003e\n\u003cp\u003eNormalization is also known as \u003cem\u003eMin-Max Scaling\u003c/em\u003e and Scikit-Learn provides the \u003ccode\u003eMinMaxScaler\u003c/code\u003e for this purpose. On the other hand, it also provides a \u003ccode\u003eNormalizer\u003c/code\u003e, which can make things a bit confusing.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e The \u003ccode\u003eNormalizer\u003c/code\u003e class \u003cstrong\u003edoesn't perform\u003c/strong\u003e the same scaling as \u003ccode\u003eMinMaxScaler\u003c/code\u003e. \u003ccode\u003eNormalizer\u003c/code\u003e works on \u003cem\u003erows\u003c/em\u003e, not features, and it scales them independently.\u003c/p\u003e\n\u003ch3 id=\"whentoperformfeaturescaling\"\u003eWhen to Perform Feature Scaling?\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003eFeature Scaling doesn't \u003cem\u003eguarantee\u003c/em\u003e better model performance for \u003cstrong\u003e\u003cem\u003eall\u003c/em\u003e\u003c/strong\u003e models.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFor instance, Feature Scaling doesn't do much if the scale doesn't matter. For \u003ca target=\"_blank\" href=\"https://stackabuse.com/k-means-clustering-with-scikit-learn\"\u003e\u003cstrong\u003e\u003cem\u003eK-Means Clustering\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e, the \u003ca target=\"_blank\" href=\"\"\u003e\u003cem\u003eEuclidean distance\u003c/em\u003e\u003c/a\u003e is important, so Feature Scaling makes a huge impact. It also makes a huge impact for any algorithms that rely on gradients, such as linear models that are fitted by minimizing loss with \u003ca target=\"_blank\" href=\"\"\u003eGradient Descent\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" href=\"https://stackabuse.com/implementing-pca-in-python-with-scikit-learn\"\u003e\u003cstrong\u003e\u003cem\u003ePrincipal Component Analysis (PCA)\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e also suffers from data that isn't scaled properly.\u003c/p\u003e\n\u003cp\u003eIn the case of Scikit-Learn - you won't see any tangible difference with a \u003ca target=\"_blank\" href=\"https://stackabuse.com/linear-regression-in-python-with-scikit-learn\"\u003e\u003ccode\u003eLinearRegression\u003c/code\u003e\u003c/a\u003e, but will see a substantial difference with a \u003ccode\u003eSGDRegressor\u003c/code\u003e, because a \u003ccode\u003eSGDRegressor\u003c/code\u003e, which is also a linear model, depends on \u003cstrong\u003e\u003cem\u003eStochastic Gradient Descent\u003c/em\u003e\u003c/strong\u003e to fit the parameters.\u003c/p\u003e\n\u003cp\u003eA \u003ca target=\"_blank\" href=\"https://stackabuse.com/decision-trees-in-python-with-scikit-learn\"\u003e\u003cstrong\u003e\u003cem\u003etree-based model\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e won't suffer from unscaled data, because scale doesn't affect them at all, but if you perform \u003ca target=\"_blank\" href=\"https://stackabuse.com/gradient-boosting-classifiers-in-python-with-scikit-learn\"\u003e\u003cstrong\u003e\u003cem\u003eGradient Boosting on Classifiers\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e, the scale \u003cem\u003edoes\u003c/em\u003e affect learning.\u003c/p\u003e\n\u003ch3 id=\"importingdataandexploratorydataanalysis\"\u003eImporting Data and Exploratory Data Analysis\u003c/h3\u003e\n\u003cp\u003eWe'll be working with the \u003ca rel=\"nofollow noopener\" target=\"_blank\" href=\"https://www.kaggle.com/prevek18/ames-housing-dataset\"\u003eAmes Housing Dataset\u003c/a\u003e which contains 79 features regarding houses sold in Ames, Iowa, as well as their sale price. This is a great dataset for basic and advanced regression training, since there are a lot of features to tweak and fiddle with, which ultimately usually affect the sales price in some way or the other.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIf you'd like to read our series of articles on Deep Learning with Keras, which produces a Deep Learning model to predict these prices more accurarely, read our \u003ca target=\"_blank\" href=\"https://stackabuse.com/hands-on-house-price-prediction-deep-learning-in-python-with-keras\"\u003eDeep Learning in Python with Keras\u003c/a\u003e series.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eLet's import the data and take a look at some of the features we'll be using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\n\u003cspan class=\"hljs-comment\"\u003e# Load the Dataset\u003c/span\u003e\ndf = pd.read_csv(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;AmesHousing.csv\u0026#x27;\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# Single out a couple of predictor variables and labels (\u0026#x27;SalePrice\u0026#x27; is our target label set)\u003c/span\u003e\nx = df[[\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Gr Liv Area\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Overall Qual\u0026#x27;\u003c/span\u003e]].values\ny = df[\u003cspan class=\"hljs-string\"\u003e\u0026#x27;SalePrice\u0026#x27;\u003c/span\u003e].values\n\nfig, ax = plt.subplots(ncols=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e, figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\nax[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].scatter(x[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], y)\nax[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e].scatter(x[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], y)\n\nplt.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThere's a clear strong positive correlation between the \u003cem\u003e\u0026quot;Gr Liv Area\u0026quot;\u003c/em\u003e feature and the \u003cem\u003e\u0026quot;SalePrice\u0026quot;\u003c/em\u003e feature - with only a couple of outliers. There's also a strong positive correlation between the \u003cem\u003e\u0026quot;Overall Qual\u0026quot;\u003c/em\u003e feature and the \u003cem\u003e\u0026quot;SalePrice\u0026quot;\u003c/em\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-1.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThough these are on a much different scale - the \u003cem\u003e\u0026quot;Gr Liv Area\u0026quot;\u003c/em\u003e spans up to ~5000 (measured in square feet), while the \u003cem\u003e\u0026quot;Overall Qual\u0026quot;\u003c/em\u003e feature spans up to 10 (discrete categories of quality). If we were to plot these two on the same axes, we wouldn't be able to tell much about the \u003cem\u003e\u0026quot;Overall Qual\u0026quot;\u003c/em\u003e feature:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig, ax = plt.subplots(figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\nax.scatter(x[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], y)\nax.scatter(x[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], y)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-2.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eAdditionally, if we were to plot their distributions, we wouldn't have much luck either:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig, ax = plt.subplots(figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\nax.hist(x[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\nax.hist(x[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-3.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe scale of these features is so different that we can't really make much out by plotting them together. \u003cem\u003eThis\u003c/em\u003e is where feature scaling kicks in.\u003c/p\u003e\n\u003ch3 id=\"standardscaler\"\u003eStandardScaler\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eStandardScaler\u003c/code\u003e class is used to transform the data by \u003cem\u003estandardizing\u003c/em\u003e it. Let's import it and \u003cem\u003escale\u003c/em\u003e the data via its \u003ccode\u003efit_transform()\u003c/code\u003e method:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\u003cspan class=\"hljs-comment\"\u003e# Import StandardScaler\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.preprocessing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e StandardScaler\n\nfig, ax = plt.subplots(figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\nscaler = StandardScaler()\nx_std = scaler.fit_transform(x)\n\nax.hist(x_std[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\nax.hist(x_std[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e We're using \u003ccode\u003efit_transform()\u003c/code\u003e on the entirety of the dataset here to demonstrate the usage of the \u003ccode\u003eStandardScaler\u003c/code\u003e class and visualize its effects. When building a model or pipeline, like we will shortly - you shouldn't \u003ccode\u003efit_transform()\u003c/code\u003e the entirety of the dataset, but rather, just \u003ccode\u003efit()\u003c/code\u003e the training data, and \u003ccode\u003etransform()\u003c/code\u003e the testing data.\u003c/p\u003e\n\u003cp\u003eRunning this piece of code will calculate the \u003cstrong\u003eμ\u003c/strong\u003e and \u003cstrong\u003eσ\u003c/strong\u003e parameters - this process is known as \u003cem\u003efitting the data\u003c/em\u003e, and then \u003cem\u003etransform it\u003c/em\u003e so that these values correspond to \u003cem\u003e1\u003c/em\u003e and \u003cem\u003e0\u003c/em\u003e respectively.\u003c/p\u003e\n\u003cp\u003eWhen we plot the distributions of these features now, we'll be greeted with a much more manageable plot:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-4.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eIf we were to plot these through Scatter Plots yet again, we'd perhaps more clearly see the effects of the standarization:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig, ax = plt.subplots(figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\nscaler = StandardScaler()\nx_std = scaler.fit_transform(x)\n\nax.scatter(x_std[:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], y)\nax.scatter(x_std[:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], y)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-5.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3 id=\"minmaxscaler\"\u003eMinMaxScaler\u003c/h3\u003e\n\u003cp\u003eTo \u003cem\u003enormalize\u003c/em\u003e features, we use the \u003ccode\u003eMinMaxScaler\u003c/code\u003e class. It works in much the same way as \u003ccode\u003eStandardScaler\u003c/code\u003e, but uses a fundementally different approach to scaling the data:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig, ax = plt.subplots(figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\nscaler = MinMaxScaler()\nx_minmax = scaler.fit_transform(x)\n\nax.hist(x_minmax [:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\nax.hist(x_minmax [:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThey are \u003cem\u003enormalized\u003c/em\u003e in the range of \u003cem\u003e[0, 1]\u003c/em\u003e. If we were to plot the distributions again, we'd be greeted with:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-6.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003eskewness\u003c/em\u003e of the distribution is preserved, unlike with \u003cem\u003estandardization\u003c/em\u003e which makes them overlap much more. Though, if we were to plot the data through Scatter Plots again:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig, ax = plt.subplots(figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\nscaler = MinMaxScaler()\nx_minmax = scaler.fit_transform(x)\n\nax.scatter(x_minmax [:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], y)\nax.scatter(x_minmax [:,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e], y)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe'd be able to see the strong positive correlation between both of these with the \u003cem\u003e\u0026quot;SalePrice\u0026quot;\u003c/em\u003e with the feature, but the \u003cem\u003e\u0026quot;Overall Qual\u0026quot;\u003c/em\u003e feature awkwardly overextends to the right, because the outliers of the \u003cem\u003e\u0026quot;Gr Liv Area\u0026quot;\u003c/em\u003e feature forced the majority of its distribution to trail on the left-hand side.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-7.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3 id=\"effectsofoutliers\"\u003eEffects of Outliers\u003c/h3\u003e\n\u003cp\u003eBoth \u003cem\u003enormalization\u003c/em\u003e and \u003cem\u003estandardization\u003c/em\u003e are sensitive to outliers - it's enough for the dataset to have a \u003cem\u003esingle\u003c/em\u003e outlier that's way out there to make things look really weird. Let's add a synthetic entry to the \u003cem\u003e\u0026quot;Gr Liv Area\u0026quot;\u003c/em\u003e feature to see how it affects the scaling process:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig, ax = plt.subplots(figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\nscaler = MinMaxScaler()\nx_minmax = scaler.fit_transform(x)\n\nax.scatter(x_minmax [:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e], y)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-8.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThe single outlier, on the far right of the plot has really affected the new distribution. \u003cem\u003eAll\u003c/em\u003e of the data, except for the outlier is located in the first two quartiles:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003efig, ax = plt.subplots(figsize=(\u003cspan class=\"hljs-number\"\u003e12\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\nscaler = MinMaxScaler()\nx_minmax = scaler.fit_transform(x)\n\nax.hist(x_minmax [:,\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cimg src=\"https://s3.stackabuse.com/media/articles/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python-9.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3 id=\"featurescalingthroughscikitlearnpipelines\"\u003eFeature Scaling Through Scikit-Learn Pipelines\u003c/h3\u003e\n\u003cp\u003eFinally, let's go ahead and train a model with and without scaling features beforehand. When working on Machine Learning projects - we typically have a \u003cem\u003epipeline\u003c/em\u003e for the data before it arrives at the model we're fitting.\u003c/p\u003e\n\u003cp\u003eWe'll be using the \u003ccode\u003ePipeline\u003c/code\u003e class which lets us minimize and, to a degree, automate this process, even though we have just two steps - scaling the data, and fitting a model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.model_selection \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e train_test_split\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.pipeline \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Pipeline\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.linear_model \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SGDRegressor\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.preprocessing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e StandardScaler\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.preprocessing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e MinMaxScaler\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e sklearn.metrics \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e mean_absolute_error\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e sklearn.metrics \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e metrics\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pandas \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e pd\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e numpy \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e np\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e plt\n\n\u003cspan class=\"hljs-comment\"\u003e# Import Data\u003c/span\u003e\ndf = pd.read_csv(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;AmesHousing.csv\u0026#x27;\u003c/span\u003e)\nx = df[[\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Gr Liv Area\u0026#x27;\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\u0026#x27;Overall Qual\u0026#x27;\u003c/span\u003e]].values\ny = df[\u003cspan class=\"hljs-string\"\u003e\u0026#x27;SalePrice\u0026#x27;\u003c/span\u003e].values\n\n\u003cspan class=\"hljs-comment\"\u003e# Split into a training and testing set\u003c/span\u003e\nX_train, X_test, Y_train, Y_test = train_test_split(x, y)\n\n\u003cspan class=\"hljs-comment\"\u003e# Define the pipeline for scaling and model fitting\u003c/span\u003e\npipeline = Pipeline([\n    (\u003cspan class=\"hljs-string\"\u003e\u0026quot;MinMax Scaling\u0026quot;\u003c/span\u003e, MinMaxScaler()),\n    (\u003cspan class=\"hljs-string\"\u003e\u0026quot;SGD Regression\u0026quot;\u003c/span\u003e, SGDRegressor())\n])\n\n\u003cspan class=\"hljs-comment\"\u003e# Scale the data and fit the model\u003c/span\u003e\npipeline.fit(X_train, Y_train)\n\n\u003cspan class=\"hljs-comment\"\u003e# Evaluate the model\u003c/span\u003e\nY_pred = pipeline.predict(X_test)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Mean Absolute Error: \u0026#x27;\u003c/span\u003e, mean_absolute_error(Y_pred, Y_test))\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Score\u0026#x27;\u003c/span\u003e, pipeline.score(X_test, Y_test))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis results in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eMean Absolute Error:  27614.031131858766\nScore 0.7536086980531018\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe mean absolute error is \u003cem\u003e~27000\u003c/em\u003e, and the accuracy score is \u003cem\u003e~75%\u003c/em\u003e. This means that on average, our model misses the price by \u003cem\u003e$27000\u003c/em\u003e, which doesn't sound that bad, although, it could be improved beyond this.\u003c/p\u003e\n\u003cp\u003eMost notably, the type of model we used is a bit too rigid and we haven't fed many features in so these two are most definitely the places that can be improved.\u003c/p\u003e\n\u003cp\u003eThough - let's not lose focus of what we're interested in. How does this model perform \u003cem\u003ewithout\u003c/em\u003e Feature Scaling? Let's modify the pipeline to skip the scaling step:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003epipeline = Pipeline([\n    (\u003cspan class=\"hljs-string\"\u003e\u0026quot;SGD Regression\u0026quot;\u003c/span\u003e, SGDRegressor())\n])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhat happens might surprise you:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003eMean Absolute Error:  1260383513716205.8\nScore -2.772781517117743e+20\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe've gone from \u003cstrong\u003e\u003cem\u003e~75%\u003c/em\u003e\u003c/strong\u003e accuracy to \u003cstrong\u003e\u003cem\u003e~-3%\u003c/em\u003e\u003c/strong\u003e accuracy just by skipping to scale our features. \u003cem\u003eAny\u003c/em\u003e learning algorithm that depends on the scale of features will typically see major benefits from Feature Scaling. Those that don't, won't see much of a difference.\u003c/p\u003e\n\u003cp\u003eFor instance, if we train a \u003ccode\u003eLinearRegression\u003c/code\u003e on this same data, with and without scaling, we'll see unremarkable results on the behalf of the scaling, and decent results on behalf of the model itself:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003epipeline1 = Pipeline([\n    (\u003cspan class=\"hljs-string\"\u003e\u0026quot;Linear Regression\u0026quot;\u003c/span\u003e, LinearRegression())\n])\n\npipeline2 = Pipeline([\n    (\u003cspan class=\"hljs-string\"\u003e\u0026quot;Scaling\u0026quot;\u003c/span\u003e, StandardScaler()),\n    (\u003cspan class=\"hljs-string\"\u003e\u0026quot;Linear Regression\u0026quot;\u003c/span\u003e, LinearRegression())\n])\n\npipeline1.fit(X_train, Y_train)\npipeline2.fit(X_train, Y_train)\n\nY_pred1 = pipeline1.predict(X_test)\nY_pred2 = pipeline2.predict(X_test)\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Pipeline 1 Mean Absolute Error: \u0026#x27;\u003c/span\u003e, mean_absolute_error(Y_pred1, Y_test))\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Pipeline 1 Score\u0026#x27;\u003c/span\u003e, pipeline1.score(X_test, Y_test))\n\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Pipeline 2 Mean Absolute Error: \u0026#x27;\u003c/span\u003e, mean_absolute_error(Y_pred2, Y_test))\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\u0026#x27;Pipeline 2 Score\u0026#x27;\u003c/span\u003e, pipeline2.score(X_test, Y_test))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs\"\u003ePipeline 1 Mean Absolute Error:  27706.61376199076\nPipeline 1 Score 0.7641840816646945\n\nPipeline 2 Mean Absolute Error:  27706.613761990764\nPipeline 2 Score 0.7641840816646945\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eFeature Scaling is the process of scaling the values of features to a more managable scale. You'll typically perform it before feeding these features into algorithms that are affected by scale, during the preprocessing phase.\u003c/p\u003e\n\u003cp\u003eIn this guide, we've taken a look at what Feature Scaling is and how to perform it in Python with Scikit-Learn, using \u003ccode\u003eStandardScaler\u003c/code\u003e to perform standardization and \u003ccode\u003eMinMaxScaler\u003c/code\u003e to perform normalization. We've also taken a look at how outliers affect these processes and the difference between a scale-sensitive model being trained with and without Feature Scaling.\u003c/p\u003e\n","parent_id":null,"type":"article","status":"published","visibility":"public","img_feature":null,"is_featured":false,"locale":"en","custom_excerpt":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"comment_id":null,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"In this guide, we'll take a look at how and why to perform Feature Scaling for Machine Learning projects, using Python's ScikitLearn library.","read_time_min":null,"published_by":null,"published_at":1626093540000,"created_by":16,"updated_by":null,"created_at":1624564628804,"updated_at":1632340430933,"contributors":[{"id":16,"name":"David Landup","slug":"david","email":"thealduinmaster@gmail.com","password_hash":"$2a$10$W/oMJdUBSTeG3trWAHa1xO0pQruxuLgD/6hS7VuxPafcmAxeBXmVi","role_id":2,"img_profile":"//s3.stackabuse.com/media/users/865cd7d217ea11c9d9555c4f666e2d73.jpg","img_cover":null,"bio_md":"Entrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs. \n\nGreat passion for accessible education and promotion of reason, science, humanism, and progress.","bio_html":"\u003cp\u003eEntrepreneur, Software and Machine Learning Engineer, with a deep fascination towards the application of Computation and Deep Learning in Life Sciences (Bioinformatics, Drug Discovery, Genomics), Neuroscience (Computational Neuroscience), robotics and BCIs.\u003c/p\u003e\n\u003cp\u003eGreat passion for accessible education and promotion of reason, science, humanism, and progress.\u003c/p\u003e\n","website":"https://www.upwork.com/freelancers/~017664e499a2766871","location":"Serbia","facebook":"","twitter":"","github":null,"status":"active","locale":null,"last_seen_at":1622229427000,"created_by":null,"updated_by":null,"created_at":1534532687000,"updated_at":1640861394795,"role":"editor","secret_token":"2a2be92558fae38f89cfbd0c6a4ba90c","is_email_confirmed":false,"_pivot_content_id":1007,"_pivot_user_id":16,"_pivot_role":"editor","_pivot_sort_order":0}],"tags":[{"id":9,"name":"python","slug":"python","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1431358631000,"updated_at":1431358631000,"_pivot_content_id":1007,"_pivot_tag_id":9,"_pivot_sort_order":0},{"id":75,"name":"machine learning","slug":"machine-learning","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1507305534000,"updated_at":1507305534000,"_pivot_content_id":1007,"_pivot_tag_id":75,"_pivot_sort_order":1},{"id":106,"name":"data science","slug":"data-science","img_cover":null,"description_md":null,"description_html":"","visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"inject_header":null,"inject_footer":null,"canonical_url":null,"color":null,"created_by":null,"updated_by":null,"created_at":1536865458000,"updated_at":1536865458000,"_pivot_content_id":1007,"_pivot_tag_id":106,"_pivot_sort_order":2}],"_pivot_tag_id":75,"_pivot_content_id":1007}],"content_count":"94"},"page":1,"error":null},"__N_SSP":true},"page":"/tag/[...slug]","query":{"slug":["machine-learning"]},"buildId":"1637467473890","isFallback":false,"gssp":true,"customServer":true}</script><script nomodule="" src="./machine learning - Stack Abuse_files/polyfills-a76f4360280b7a18f57e.js.download"></script><script src="./machine learning - Stack Abuse_files/webpack-be80a870933e223259e9.js.download" async=""></script><script src="./machine learning - Stack Abuse_files/framework-0974038618461ef712e9.js.download" async=""></script><script src="./machine learning - Stack Abuse_files/main-518b48f042719025b17a.js.download" async=""></script><script src="./machine learning - Stack Abuse_files/_app-663f6941febb9816e224.js.download" async=""></script><script src="./machine learning - Stack Abuse_files/3-9d8c849ccdc38cf8a43a.js.download" async=""></script><script src="./machine learning - Stack Abuse_files/566-b745348a44c8b90e96f7.js.download" async=""></script><script src="./machine learning - Stack Abuse_files/221-fd1b1d9fc7fde140cafc.js.download" async=""></script><script src="./machine learning - Stack Abuse_files/455-3962e458e87e7d9e0442.js.download" async=""></script><script src="./machine learning - Stack Abuse_files/[...slug]-cdcef7f320901bfdcbca.js.download" async=""></script><script src="./machine learning - Stack Abuse_files/_buildManifest.js.download" async=""></script><script src="./machine learning - Stack Abuse_files/_ssgManifest.js.download" async=""></script><next-route-announcer><p aria-live="assertive" id="__next-route-announcer__" role="alert" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; white-space: nowrap; overflow-wrap: normal;"></p></next-route-announcer><script src="./machine learning - Stack Abuse_files/543-2e1c9b1100c9aa9ba3f6.js.download"></script><script src="./machine learning - Stack Abuse_files/638-a94f01bea17e0c9ed083.js.download"></script><script src="./machine learning - Stack Abuse_files/677-fbfef5f954127ac1a02a.js.download"></script><script src="./machine learning - Stack Abuse_files/566-2b1d89f30e5af269bcf2.js.download"></script><script src="./machine learning - Stack Abuse_files/456-c4e0b33b2f8e38f3f57f.js.download"></script><script src="./machine learning - Stack Abuse_files/988-736ed4d52c6ef55892cf.js.download"></script><script src="./machine learning - Stack Abuse_files/917-3229522ef7d6ecbe92c5.js.download"></script><script src="./machine learning - Stack Abuse_files/[slug]-e8f5b4417775ebd0d6be.js.download"></script><script src="./machine learning - Stack Abuse_files/index-0fb1cfa76a5a39e8a613.js.download"></script><script src="./machine learning - Stack Abuse_files/[...slug]-2322209993883dd7542d.js.download"></script></body></html>
